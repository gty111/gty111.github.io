{"posts":[{"title":"NVVM (NVCC &amp; LLVM)","text":"NVIDIA’s CUDA Compiler (NVCC) is based on the widely used LLVM open source compiler infrastructure. Developers can create or extend programming languages with support for GPU acceleration using the NVIDIA Compiler SDK. Some links about NVVM. What is cuda llvm compiler NVVM IR docs NVVM IR is a compiler IR (intermediate representation) based on the LLVM IR. The NVVM IR is designed to represent GPU compute kernels (for example, CUDA kernels). High-level language front-ends, like the CUDA C compiler front-end, can generate NVVM IR. NVVM IR samples LLVM NVPTX backend CUDA Compilation Trajectory libNVVM API libdevice User’s Guide The libdevice library is an LLVM bitcode library that implements common functions for GPU kernels.","link":"/2023/04/22/NVVM-IR/"},{"title":"Tips for Linux","text":"This passage is to log some tips about Linux. How to change default shell : chsh kown_hosts : log the public key of the host you have visited How to use ssh /etc/passwd ls compress : tar -cf *.tar path-to-file(dir) uncompress : tar -xf *.tar make -n : only print inst not execute view system INFO : cat /proc/version (kernel) or cat /etc/issue (system) view cpu INFO : cat /proc/cpuinfo -nostdlib compilation flags -E : Only run the preprocessor -S : Only run preprocess and compilation steps (=&gt; .s .ll) -c : Only run preprocess, compile, and assemble steps (=&gt; .o)","link":"/2023/03/30/Tips-for-Linux/"},{"title":"我的一生一芯","text":"一生一芯是一个开放社区性质的公益教学项目，主要是为了解决中国芯片设计人才缺失的问题，主办方是国科大和计算所，要求学生设计出一块自己的CPU并成功流片。 初识一生一芯记得还是在大四阶段接触到的一生一芯计划，当时是在西电保研群某个大佬发的链接，于是就报名参加了。大四应该是整个大学最轻松的时候（指心情方面），保研阶段结束，于是就开始刷各种实验或写毕设。 深入一生一芯前期最终要的就是要完成两个实验，数字电路基础实验(dlco)和计算机系统基础实验(PA)。 数字电路基础实验dlco主要是通过nvboard(一个虚拟FPGA开发板)进行实验，而实验语言便是verilog。我之前从来没有接触过硬件相关知识，所以会习惯用软件编程的思想写硬件程序(verilog)，这也是我最终退出一生一芯的主要原因。在完成dlco实验过程中，其中最令我印象深刻的是字符输入界面，我在上面投入了很多时间，最终也实现了几乎所有的扩展要求，当打开界面，在键盘上敲下各个按键时字母符号出现在屏幕上时，感觉十分有成就感。 计算机系统基础实验PA实验是继CSAPP LAB后第二个令我震惊的实验，感觉我本科实验真的好lj。简单地介绍下PA是什么，它要求学生实现一个经过简化但功能完备的x86/mips32/riscv32(64)模拟器，最终可以运行galgame等游戏。PA可以说是由浅入深，让学生从零基础逐渐深入到计算机底层的各个领域。PA实验给我带来最大的帮助就是了解了riscv指令集和模拟器究竟是什么。 CPU编写一生一芯顾名思义肯定是要写CPU啦，许多人一听到写CPU就会感觉“哇，这也太难了吧”，其实你可以把它想得很简单，如果用软件编程的思想来看是一个流程：写程序（verilog或chisel）=&gt; 逻辑综合(可以理解为软件的编译) =&gt; 剩下全部丢给后端去做了。所以芯片设计对于我的理解就是写verilog，我想“C++我都不怕，怎么可能怕verilog呢?”，于是开始了漫长的verilog编写，我甚至迭代了十几个版本（每个版本都通过了benchmark测试正确性），最后发现自己写的verilog还是lj。这时候，我才发现我缺少了最重要的一环，硬件设计思想，于是我默默地退出了。 后记尽管我的一生一芯计划并没有走到最后，但是我在参加的过程中还是收货了很多知识，这对我之后的学习起了很大的帮助。我对于一生一芯计划评价还是很高的，它可以系统地训练或培养学生进行芯片设计，从中你可以学习到很多体系结构的知识，希望一生一芯可以一直走下去吧，也希望中国的芯片巨头早日出现(对标苹果,intel,Nvidia,AMD)。","link":"/2023/03/23/YSYX/"},{"title":"How to build this web","text":"This web is building by Hexo and Icarus. Install Node.js for linux reference this for windows through nvs or nvm for mac through Homebrew or MacPorts Install Hexo (require Node.js)1npm install -g hexo-cli Create your site123456hexo init &lt;folder&gt; cd &lt;folder&gt;npm installnpm install -S hexo-theme-icarus hexo-renderer-inferno # install icarushexo config theme icarus # use theme icarushexo server # start server at localhost Tips about Hexo or Icarus Add read more to your blogs : just add &lt;!-- more --&gt; in your md at proper position. To search icons to use, visit here Reference hexo-tutorial getting-started-with-icarus","link":"/2023/03/20/build_web/"},{"title":"ebooks and tutorials","text":"Some links about ebooks and tutorials. RAPIDS(GPU Accelerated Data Science)ROCMCUDACohere(LLM)Qiskit(Quantum computing)The Architecture of Open Source ApplicationsHow to write a LLVM backendLinux kernel doc","link":"/2023/05/01/ebooks/"},{"title":"Optimize GEMM step by step","text":"一步步优化GEMM系列，每次引入一个优化概念并对比性能变化 点击每个标题链接跳转到对应github仓库 总体思路首先我们构建了一个初级的GEMM kernel，它使用CUDA mma.sync指令来使用GPU tensor core单元，并对比了和cutlass算子的性能 上图展示了GEMM MMA的计算流程，蓝色部分代表1个block要计算的部分，蓝色部分下的每个小方块代表每个warp的计算部分，右侧青色部分代表每个warp的计算部分，青色部分下的每个小方块代表tensor core支持的分块大小，在调用tensor core之前，加载一个绿色方块和红色方块进入共享内存，之后每个warp独立同步地调用mma.sync 来计算每个分块的结果，其中 M'、N' 、K' 代表tensor core单元支持计算的GEMM维度。 baseline性能: 3.44% (相比cutlass) 使用向量化(vector)vector分支主要介绍向量化load/store， 优化后性能: 4.74% 向量化在不同层级的表现 cu level1*reinterpret_cast&lt;float4*&gt;(&amp;A[tileIdx]) = *reinterpret_cast&lt;float4*&gt;(&amp;arg.A[rowA_0*arg.problem_size.k()+colA_0]); ptx level12ld.global.v4.u32 {%r161, %r162, %r163, %r164}, [%rd5];st.shared.v4.u32 [%r16], {%r161, %r162, %r163, %r164}; SASS level12LDG.E.128 R8, [R10.64] ;STS.128 [R17], R8 ; 结果123456[ problem size] (8192,8192,8192)[ cutlassMMA] Runtime: 15.816908(ms) Gflops: 69514.954246[ MMA_base] Runtime: 460.150970(ms) Gflops: 2389.458457[ MMA_base==ref] PASS[ MMA_vec] Runtime: 333.618652(ms) Gflops: 3295.713894[ MMA_vec==ref] PASS 避免bank冲突并且合并访存(bfco)bfco分支主要介绍如何通过解决shared memory bank conflict 和 memory coalesce (访存合并) 来优化性能 优化后性能: 5.00% Shared memory bank参考cuda programming guide 要注意连续的bank存储连续的字(32-bits)，这里字的大小为32 bits，总共有32个bank 要想解决bank conflict问题，要将一个warp内线程读取的shared memory尽量分散到不同的bank里 memory coalesce（访存合并）访存合并用一句话来简单概括就是一个warp内线程读取的global memory尽量是连续的且128字节对齐 为什么是128字节对齐而不是其他数字？我的理解是cache line的大小是128字节，这样一个warp内的访存可以合并成以cache line为基本单位的memory transaction 代码分析为了解决bank conflict 和 memory coalesce，对代码做的主要修改为变量 tileidx 1234// in function loadtileCint tileIdx = threadIdx.x*64 + i*4; // baseint tileIdx = threadIdx.x*64 + (i+threadIdx.x/1)%16*4; // bank freeint tileIdx = threadIdx.x*4 + i*blockDim.x*4; // memory coalesce + bank free 结果12345678[ problem size] (8192,8192,8192)[ cutlassMMA] Runtime: 15.788442(ms) Gflops: 69640.288231[ MMA_base] Runtime: 333.625763(ms) Gflops: 3295.643652[ MMA_base==ref] PASS[ MMA_bf] Runtime: 326.514526(ms) Gflops: 3367.420249[ MMA_bf==ref] PASS[ MMA_bf_co] Runtime: 315.669495(ms) Gflops: 3483.110172[ MMA_bf_co==ref] PASS 使用异步拷贝(ldgsts)ldgsts 分支主要来介绍使用Ampere引入的异步拷贝来优化性能 优化后性能: 5.36% 异步拷贝CUDA 11 includes a new asynchronous copy (async copy) API to take advantage of the A100GPU’s hardware-accelerated direct-copy-to-shared functionality. Async copy performs anasynchronous (non-blocking) direct memory transfer from global memory to shared memory,bypassing the SM threads and combining the functions of separate “load from global memoryinto a register”, and “write to shared memory from a register” operations into a single, efficientoperation. Async copy eliminates the need for intermediate staging of data through the register file (RF),reducing register file bandwidth. It also efficiently uses memory bandwidth and reduces powerconsumption. As the name implies, async copy works asynchronously, allowing othercomputations to occur during global-to-shared memory copies. Async copy is able to notify theprogram of copy completion via the GPU’s new barrier feature. Bypassing L1 and the register file can significantly accelerate memory copy performance,especially for multiple successive async-copy operations that copy large amounts of data fromglobal to shared memory. Two variants of the async copy instruction are available for different usage scenarios. BYPASS,which bypasses L1 cache and the register file as described above, and ACCESS which savesdata to L1 for subsequent accesses and reuse. cu level12345asm volatile(&quot;cp.async.cg.shared.global [%0], [%1], %2;\\n&quot; :: &quot;r&quot;((uint32_t)__cvta_generic_to_shared(&amp;C[tileIdx])), &quot;l&quot;(&amp;arg.C[rowC_0*arg.problem_size.n()+colC_0]), &quot;n&quot;(16) ); ptx level1cp.async.cg.shared.global [%r158], [%rd18], 16; SASS level1LDGSTS.E.BYPASS.128 [R7+0x1800], [R4.64] ; 结果123456[ problem size] (8192,8192,8192)[ cutlassMMA] Runtime: 15.938765(ms) Gflops: 68983.491336[ MMA_base] Runtime: 315.683228(ms) Gflops: 3482.958649[ MMA_base==ref] PASS[ MMA_ldgsts] Runtime: 297.315948(ms) Gflops: 3698.125289[ MMA_ldgsts==ref] PASS 使用寄存器(reg)reg 分支介绍使用寄存器来优化性能 优化后性能: 35.39% CUDA中的寄存器寄存器的概念可能对于高级编程者来说是比较陌生的，因为在编程中一般并不会刻意地声明要使用寄存器来做什么操作，编译器会帮我们处理好这个问题，这就导致了在编写CUDA算子时往往会忽略掉寄存器的使用，可以通过ncu或编译时设置编译参数来查看kernel中每个线程使用了几个寄存器，比如在我们对比的cutlass的kernel中每个线程使用了230个寄存器，但是本例中baseline的kernel中每个线程只使用了32个寄存器，所以可以考虑将频繁使用的tileC(也就是图中的蓝色部分)从共享内存转移到寄存器中。 如何使用寄存器？其实很简单，在kernel中声明变量或数组就可以(不过如果一个线程使用太多寄存器会发生register spilling，可以在编译好程序后反汇编查看下有没有local memory) 在代码中添加了 1ElementOutput C_fragment[64]; 并修改好相关逻辑后，再次编译发现每个线程使用了156个线程 杂项之前方法优化效果不明显的原因应该是kernel性能瓶颈在别的地方，可以从这个寄存器优化后的版本尝试如果不采用向量化，不解决bank冲突或访存不合并，或者不采用异步拷贝，kernel的性能变化是怎样的？ 结果原来没有使用寄存器才是kernel性能差的主要原因… 123456[ problem size] (8192,8192,8192)[ cutlassMMA] Runtime: 16.149094(ms) Gflops: 68085.036418[ MMA_base] Runtime: 297.333862(ms) Gflops: 3697.902483[ MMA_base==ref] PASS[ MMA_tune] Runtime: 45.636402(ms) Gflops: 24092.863952[ MMA_tune==ref] PASS 使用数据预取(prefetch)prefetch 分支介绍使用数据预取来优化性能 优化后性能：39.36% 数据预取需要将缓冲区加倍，主要流程如下 假设计算mma_{i}依赖于数据data_{i}, load data_{i}代表开始加载数据data_{i}, 只有在synchronize后加载的数据才保证可见, 那么数据预取的伪代码如下 1234567load data_{1}for i=1:... synchronize mma_{i} load data_{i+1}end 这样可以让数据加载(data_{i+1})和计算(mma_{i})尽可能重叠起来 结果123456[ problem size] (8192,8192,8192)[ cutlassMMA] Runtime: 15.947670(ms) Gflops: 68944.969952[ MMA_base] Runtime: 45.381512(ms) Gflops: 24228.184273[ MMA_base==ref] PASS[ MMA_tune] Runtime: 40.519497(ms) Gflops: 27135.372140[ MMA_tune==ref] PASS 关于PTXAS有趣的发现(ptxas)ptxas 分支分享一个调优过程中发现的关于ptxas(ptx汇编器)有意思的东西 事情起因在优化kernel过程中，发现每个warp循环计算每个Ctile时，都需要从共享内存加载Atile和Btile一次，这样Atile和Btile会被重复加载4次，那其实这里数据是可以复用的，而不用多次加载，于是重新设计了下每个warp计算流程，将4x4tile划分为4个2x2的大tile，每次计算大tile前先把对应的Atile和Btile从共享内存加载到寄存器中，在计算下一个大tile时只需要重新加载一个 A/B tile即可(例如从左上角移动到右上角只需要重新加载Btile即可，Atile是可以复用的)，下图为优化前后的计算流程图，其中C tile为每个warp需要计算的矩阵C的部分，图上的数字代表数据块被加载的次数 然而就在我把代码写好，验证通过后，惊讶地发现两个kernel的运行时间很接近，通过ncu profile后发现运行的指令数竟然是一样的，反汇编后发现两个kernel的sass指令数竟然是相同的，然后仔细看了下，代码逻辑完全是一模一样的，只是部分寄存器命名不一样，这有点离谱，然后看了下两个kernel的ptx指令逻辑还是不一样的，难道CUDA的ptxas优化的这么离谱。 这里给出kernel的ptx和sass指令 两个kernel的ptx版本分别为 ptx_mma 和 ptx_mma_tune 两个kernel的sass版本分别为 sass_mma 和 sass_mma_tune 杂项优化了半天最后发现机器码都是一样的，确实感觉到了编译器的强大，关键是怎么知道代码哪些是已经被编译器优化好了呢。 另外意外发现了A100 80GB 相比A100 40GB 可以提升33%左右的性能，于是感觉很奇怪，这两个版本不就是显存大小不一样嘛，怎么运行速度差距这么大，于是发现A100 80GB显存带宽 2TB/s，而40BG版本显存带宽 1.5TB/s，这相当于显存带宽提升了33%，这难道全部转化成性能提升了吗？ 123456789101112131415A100 40GB[ problem size] (5120,4096,4096)[ cutlassMMA] Runtime: 2.370118(ms) Gflops: 72485.278929[ MMA_base] Runtime: 6.451385(ms) Gflops: 26629.735875[ MMA_base==ref] PASS[ MMA_tune] Runtime: 6.456460(ms) Gflops: 26608.804078[ MMA_tune==ref] PASSA100 80GB[ problem size] (5120,4096,4096)[ cutlassMMA] Runtime: 1.781453(ms) Gflops: 96437.410102[ MMA_base] Runtime: 4.881101(ms) Gflops: 35196.711561[ MMA_base==ref] PASS[ MMA_tune] Runtime: 4.883047(ms) Gflops: 35182.685107[ MMA_tune==ref] PASS 优化数据预取(prefetchx)prefetchx 分支和之前的prefetch分支类似，区别是增加了预取数据大小并利用了同步指令cp.async.waitgroup N 优化后性能：46.89% 数据预取在之前介绍过，本次优化主要将预取数据的大小增加了1倍，并且显式地调用同步指令cp.async.waitgroup N来确保数据已经拷贝完成，主要流程如下图片 结果 PS: 这次测试结果使用的是A100 80GB版本 123456[ problem size] (8192,8192,8192)[ cutlassMMA] Runtime: 12.857344(ms) Gflops: 85516.235366[ MMA_base] Runtime: 30.978357(ms) Gflops: 35492.896431[ MMA_base==ref] PASS[ MMA_tune] Runtime: 27.419851(ms) Gflops: 40099.109788[ MMA_tune==ref] PASS 调整线程块和warp计算的矩阵大小(shape) shape 分支调整了每个block和warp计算的矩阵C的大小 优化后性能：62.39% cutlass 中 ShapeMMAThreadBlock 代表每个线程块计算的矩阵C的大小，而 ShapeMMAWarp 代表每个warp计算的矩阵C的大小 之前手写的 MMA kernel 每个线程块计算 128x64，每个warp计算 64x32，调整后每个线程块计算 128x128，每个warp计算 64x64，这样可以增加数据复用 PS: 实测如果在kernel中申请长度为128的数组，编译器会将其分配到local memory， 所以为了避免这样的情况发生，需要将长度为128的数组分成两个长度为64的数组 结果 PS: 这次测试结果没有对比之前的kernel 1234[ problem size] (8192,8192,8192)[ cutlassMMA] Runtime: 12.744192(ms) Gflops: 86275.506296[ MMA_tune] Runtime: 20.427467(ms) Gflops: 53825.156547[ MMA_tune==ref] PASS 调整线程块分配到的计算位置(swizzle)swizzle 分支调整每个thread block分配到的计算位置来优化性能 优化后性能: 68.43% swizzle本次优化思路来自于 cutlass 中 ThreadBlockSwizzle，一开始接触可能比较难以理解这个概念，这个swizzle最核心的就是对于 blockIdx 进行如下变换 123blockIdx.x ==&gt; block_idx_x &gt;&gt; log_tileblockIdx.y ==&gt; (block_idx_y &lt;&lt; log_tile) + ((block_idx_x) &amp; ((1 &lt;&lt; (log_tile)) - 1))blockIdx.z ==&gt; blockIdx.z 看了上面的变换公式，你可能还是一头雾水，其实我们来用一张图来简单说明(log_tile=2), 假如我们启动了一个kernel，它的gridDim为(16,1,1)(即下图中左边的分布)，那么经过 swizzle 变换后得到下图中右边的分布，所以我们可以发现 swizzle 后的线程块在2D分布上满足局部性原理，那这样有什么好处呢，好处就是可以尽量提升L2缓存的命中率或L2缓存中的数据复用。 12340 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 | 0 4 8 12 | 1 5 9 13 | 2 6 10 14 | 3 7 11 15 结果1234[ problem size] (8192,8192,8192)[ cutlassMMA] Runtime: 12.671488(ms) Gflops: 86770.523274[ MMA_tune] Runtime: 18.476339(ms) Gflops: 59509.170487[ MMA_tune==ref] PASS 使用ldmatrix指令ldmatrix.sync 指令是 Warp-level matrix load instruction，它是 mma.sync 对应的load共享内存的指令 优化后性能: 73.65% 解决shared memory bank冲突之前没有注意到shared memory 存在 bank 冲突 ，所以通过调整 shared memory 布局来解决 bank 冲突问题 上图中 0,1,2,3 分别代表每个 warp 一次加载的 shared memory 部分, 加载的部分为 A 矩阵(行主序) 访存合并在解决 shared memory bank 冲突时，如果做如下调整会导致 global memory 访存无法合并 因为 N'=8 ， 所以一行正好是 32bytes，即一个sector，而如果一个sector被拆开放到 shared memory 中去会导致访存无法合并，而下图中的变换是可以合并访存的 Nsight ComputeNsight Compute(ncu) 是 NVIDIA 推出的对 kernel profile 工具，之前一直用的命令行方式，最近发现 ncu-ui 输出的信息更完善丰富，比如下图可以对访存进行细致的展示 结果1234[ problem size] (8192,8192,8192)[ cutlassMMA] Runtime: 12.476519(ms) Gflops: 88126.476648[ MMA_tune] Runtime: 16.939159(ms) Gflops: 64909.456381[ MMA_tune==ref] PASS","link":"/2023/06/20/gemm-optimize/"},{"title":"github初级使用指南","text":"什么是github generated by chatgpt GitHub是一个基于互联网的Git代码托管平台，它为开发者提供了一个在线上协作开发、版本控制和代码分享的平台。开发者可以将自己的代码托管在GitHub上，与他人协作开发项目，创建开源软件，分享自己的代码以及与其他开发者互相交流和学习。GitHub上的开源项目数量很多，涉及各种编程语言和开发领域，它已经成为了全球最大的开源社区之一。除此之外，GitHub也提供了一些其他的功能，例如问题跟踪、维基、团队协作等。 如何使用github我对github的理解就是学习开源项目、参与开源项目或托管(分享)自己代码。 基础知识 Star: 在github上star一个仓库可以理解为收藏，可以在你的stars中看到你收藏的所有仓库 Fork: 在github上fork一个仓库可以理解为将这个仓库拷贝一份到自己名下（因为仓库的所有权是在别人，如果想要修改该仓库需要Fork到自己的名下或者提pull resquests即PR） Follow: 在github上follow一个人可以理解为关注，并且你可以收到TA之后的动态推送(比如TA创建仓库、star仓库等)，推送可以在这里的following看到 README: github一般每个仓库会有一个文件叫README.md，它的意义是github仓库的说明书，默认情况下你打开一个github仓库会展示给你，你可以通过阅读README来了解一个仓库 ssh key: github的身份验证机制，它允许你通过ssh协议访问github仓库(命令行方式)，而不需要用户名和密码 private or public: github仓库主要分为private和public，别人看不到你的private仓库，只能看到你的public仓库 github常年处于半墙状态，所以使用可能有些不方便 托管代码当你创建了一个仓库后，你可以在网页上编辑或者在本地编辑好后上传至github。一般来说，我们会选择在本地编辑之后上传的方式，这种方式首先需要我们配置后github的ssh key，关于如何配置ssh key网上应该有很多教程了。 当你在github上新建仓库后，github会在新仓库给出命令提示，你可以按照github命令提示进行操作。 学习开源项目github上有很多好玩且优秀的开源项目，你可以通过搜索发现自己感兴趣的开源项目(吐槽下github的搜索功能感觉很一般)，或者通过follow领域的大牛来看TA们star的项目或创建维护的项目。","link":"/2023/03/29/github-use-guide/"},{"title":"learn-cutlass-0","text":"learn cutlass is a series of tutorials to learn cutlass by reading its examples or source code CUTLASS is a header-only template library. After reading that, you will be lost in templates. 00_basic_gemm12345678910111213141516171819// Defines cutlass::gemm::device::Gemm, the generic Gemm computation template class.#include &quot;cutlass/gemm/device/gemm.h&quot;using CutlassGemm = cutlass::gemm::device::Gemm&lt;A_TYPE,A_LAYOUT,B_TYPE,B_LAYOUT,C_TYPE,C_LAYOUT&gt; ;// where A_TYPE is Data-type of A matrix and A_LAYOUT is Layout of A matrixCutlassGemm gemm_operator;CutlassGemm::Arguments args({M , N, K}, {A_POINTER, lda}, {B_POINTER, ldb}, {C_POINTER, ldc}, {C_POINTER, ldc}, {alpha, beta});// where A_POINTER is pointer of A matrix and lda is the number of elements between consecutive rows or colmnscutlass::Status status = gemm_operator(args);// call gemm operation 01_cutlass_utilities1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495// CUTLASS includes needed for half-precision GEMM kernel#include &quot;cutlass/cutlass.h&quot;#include &quot;cutlass/core_io.h&quot;#include &quot;cutlass/layout/matrix.h&quot;#include &quot;cutlass/gemm/device/gemm.h&quot;//// CUTLASS utility includes//// Defines operator&lt;&lt;() to write TensorView objects to std::ostream#include &quot;cutlass/util/tensor_view_io.h&quot;// Defines cutlass::HostTensor&lt;&gt;#include &quot;cutlass/util/host_tensor.h&quot;// Defines cutlass::half_t#include &quot;cutlass/numeric_types.h&quot;// Defines device_memory::copy_device_to_device()#include &quot;cutlass/util/device_memory.h&quot;// Defines cutlass::reference::device::TensorFillRandomGaussian()#include &quot;cutlass/util/reference/device/tensor_fill.h&quot;// Defines cutlass::reference::host::TensorEquals()#include &quot;cutlass/util/reference/host/tensor_compare.h&quot;// Defines cutlass::reference::host::Gemm()#include &quot;cutlass/util/reference/host/gemm.h&quot;// another way to call gemm without using Argumentscutlass::Status status = gemm_op({ {M, N, K}, {A, lda}, {B, ldb}, {C, ldc}, {C, ldc}, {alpha, beta} });// define a tensor (M,N) in cutlass, where DTYPE is data typecutlass::HostTensor&lt;DTYPE,LAYOUT&gt; VAR(cutlass::MatrixCoord(M,N)) ;cutlass::HostTensor&lt;cutlass::half_t, cutlass::layout::ColumnMajor&gt; A(cutlass::MatrixCoord(M, K));// fill a tensor (RandomGaussian) where A.device_view() return TensorView of that tensor in cutlasscutlass::reference::device::TensorFillRandomGaussian( A.device_view(), seed, mean, stddev, bits_less_than_one );// copy data from device to device in cutlass where A.device_data() return pointer of that tensor// A.capacity() return the logical capacity based on extent and layout. May differ from size().cutlass::device_memory::copy_device_to_device( C_reference.device_data(), C_cutlass.device_data(), C_cutlass.capacity());// Copies data from device to hostA.sync_host();// Copies data from host to deviceA.sync_device();// Compute the reference result using the host-side GEMM reference implementation.// I think the only difference between TensorView and TensorRef is that TensorView is read-only // while TensorRef can return pointer of matrixcutlass::reference::host::Gemm&lt; cutlass::half_t, // ElementA cutlass::layout::ColumnMajor, // LayoutA cutlass::half_t, // ElementB cutlass::layout::ColumnMajor, // LayoutB cutlass::half_t, // ElementOutput cutlass::layout::ColumnMajor, // LayoutOutput cutlass::half_t, // ScalarType cutlass::half_t // ComputeType&gt; gemm_ref;gemm_ref( {M, N, K}, // problem size (type: cutlass::gemm::GemmCoord) alpha, // alpha (type: cutlass::half_t) A.host_ref(), // A (type: TensorRef&lt;half_t, ColumnMajor&gt;) B.host_ref(), // B (type: TensorRef&lt;half_t, ColumnMajor&gt;) beta, // beta (type: cutlass::half_t) C_reference.host_ref() // C (type: TensorRef&lt;half_t, ColumnMajor&gt;));// Compare reference to computed resultscutlass::reference::host::TensorEquals( C_reference.host_view(), C_cutlass.host_view()); 04_tile_iterator12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697// CUTLASS includes#include &quot;cutlass/transform/threadblock/predicated_tile_iterator.h&quot;#include &quot;cutlass/layout/pitch_linear.h&quot;#include &quot;cutlass/transform/pitch_linear_thread_map.h&quot;//// CUTLASS utility includes//// Defines operator&lt;&lt;() to write TensorView objects to std::ostream#include &quot;cutlass/util/tensor_view_io.h&quot;// Defines cutlass::HostTensor&lt;&gt;#include &quot;cutlass/util/host_tensor.h&quot;// Defines cutlass::reference::host::TensorFill() and// cutlass::reference::host::TensorFillBlockSequential()#include &quot;cutlass/util/reference/host/tensor_fill.h&quot;// For this example, we chose a &lt;64, 4&gt; tile shape. The PredicateTileIterator expects// PitchLinearShape and PitchLinear layout.using Shape = cutlass::layout::PitchLinearShape&lt;64, 4&gt;;using Layout = cutlass::layout::PitchLinear;using Element = int;int const kThreads = 32;// ThreadMaps define how threads are mapped to a given tile. The PitchLinearStripminedThreadMap// stripmines a pitch-linear tile among a given number of threads, first along the contiguous// dimension then along the strided dimension.using ThreadMap = cutlass::transform::PitchLinearStripminedThreadMap&lt;Shape, kThreads&gt;;// Define the PredicateTileIterator, using TileShape, Element, Layout, and ThreadMap typesusing Iterator = cutlass::transform::threadblock::PredicatedTileIterator&lt; Shape, Element, Layout, 1, ThreadMap&gt;;cutlass::Coord&lt;2&gt; copy_extent = cutlass::make_Coord(M, K);cutlass::Coord&lt;2&gt; alloc_extent = cutlass::make_Coord(M, K);// another way to define tensor// Allocate source and destination tensorscutlass::HostTensor&lt;Element, Layout&gt; src_tensor(alloc_extent);cutlass::HostTensor&lt;Element, Layout&gt; dst_tensor(alloc_extent);Element oob_value = Element(-1);// Initialize destination tensor with all -1scutlass::reference::host::TensorFill(dst_tensor.host_view(), oob_value);// Initialize source tensor with sequentially increasing valuescutlass::reference::host::BlockFillSequential(src_tensor.host_data(), src_tensor.capacity());dst_tensor.sync_device();src_tensor.sync_device();typename Iterator::Params dst_params(dst_tensor.layout());typename Iterator::Params src_params(src_tensor.layout());dim3 block(kThreads, 1);dim3 grid(1, 1);// Launch copy kernel to perform the copycopy&lt;Iterator&gt;&lt;&lt;&lt; grid, block &gt;&gt;&gt;( dst_params, dst_tensor.device_data(), src_params, src_tensor.device_data(), copy_extent);// copy functiontemplate &lt;typename Iterator&gt;__global__ void copy( typename Iterator::Params dst_params, typename Iterator::Element *dst_pointer, typename Iterator::Params src_params, typename Iterator::Element *src_pointer, cutlass::Coord&lt;2&gt; extent) { Iterator dst_iterator(dst_params, dst_pointer, extent, threadIdx.x); Iterator src_iterator(src_params, src_pointer, extent, threadIdx.x); // PredicatedTileIterator uses PitchLinear layout and therefore takes in a PitchLinearShape. // The contiguous dimension can be accessed via Iterator::Shape::kContiguous and the strided // dimension can be accessed via Iterator::Shape::kStrided int iterations = (extent[1] + Iterator::Shape::kStrided - 1) / Iterator::Shape::kStrided; typename Iterator::Fragment fragment; for(; iterations &gt; 0; --iterations) { src_iterator.load(fragment); dst_iterator.store(fragment); ++src_iterator; ++dst_iterator; }}","link":"/2023/03/20/learn-cutlass-0/"},{"title":"learn-cutlass-1","text":"In cutlass 3.0, it introduces a new library, Cute, to describe and manipulate tensors of threads and data. Different types of GEMM TYPE of GEMM BITS of DATA TYPE of DATA HGEMM 16 floating-point number SGEMM 32 floating-point number DGEMM 64 floating-point number IGEMM 8 or 16 or 32 or 64 integer RowMajorInterleaved ColumnMajorInterleaved 12#include &quot;cutlass/layout/matrix.h&quot;template&lt;int Interleave&gt; struct cutlass::layout::RowMajorInterleaved&lt;Interleave&gt;; RowMajorInterleaved is a layout which confused me. I didn’t know the meaning of Interleaved.So I create an example to figure it out. 123456789101112131415161718192021222324252627282930313233343536#include &lt;iostream&gt;#include &lt;cstdio&gt;// Defines cutlass::layout::RowMajorInterleave#include &quot;cutlass/layout/matrix.h&quot;// Defines cutlass::HostTensor&lt;&gt;#include &quot;cutlass/util/host_tensor.h&quot;// Defines cutlass::MatrixCoord#include &quot;cutlass/matrix_coord.h&quot;#define M 4#define N 4int main(){ cutlass::HostTensor&lt;int,cutlass::layout::RowMajorInterleaved&lt;2&gt; &gt; A(cutlass::MatrixCoord(M,N)); int num = 0; for(int i=0;i&lt;M;i++) for(int j=0;j&lt;N;j++){ A.at({i,j}) = ++num; } int *A_ = A.host_data(); for(int i=0;i&lt;A.capacity();i++){ printf(&quot;%3d &quot;,A_[i]); // if((i+1)%N==0)printf(&quot;\\n&quot;); } /** * output: * 1 5 2 6 3 7 4 8 9 13 10 14 11 15 12 16 * */} If tensor A is a simple RowMajor, the output should be this 11 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 In my opinion, Interleaved means it will iterate in shape(1) with size Interleave and then iterate in shape(0).Other things need to mind is Interleaved may cause padding of a matrix, like 1234567891011#define M 3#define N 3cutlass::HostTensor&lt;int,cutlass::layout::RowMajorInterleaved&lt;2&gt; &gt; A(cutlass::MatrixCoord(M,N));int num = 0;for(int i=0;i&lt;M;i++)for(int j=0;j&lt;N;j++){ A.at({i,j}) = ++num; }/** * the element in A should be * 1 4 2 5 3 6 7 0 8 0 9 0 typename in C++In cutlass, you will see typename everywhere. Obviously, you can use typename when building template. But it has other usage, such as The following is generated by chatgpt 12345template&lt;typename T&gt;void foo() { typename T::iterator it; // ...} In C++, the typename keyword is used to specify that a dependent name is a type. A dependent name is a name that depends on a template parameter, and the compiler cannot determine whether the name refers to a type or some other entity until the template is instantiated with concrete types. Consider the above example. Here, T::iterator is a dependent name because it depends on the template parameter T. Without the typename keyword, the compiler would not be able to determine whether T::iterator is a type or a member variable until T is instantiated with a concrete type. The typename keyword tells the compiler that T::iterator is a type. Note that typename is only needed when using a dependent name as a type. If the name does not depend on a template parameter, or if it is not used as a type, the typename keyword should not be used. ThreadblockSwizzle The following is generated by chatgpt In Cutlass, ThreadblockSwizzle is a feature that allows for different threadblock configurations to be used when performing matrix-multiplication operations. ThreadblockSwizzle can be used to optimize the performance of GEMM (General Matrix Multiply) operations on GPUs, by mapping the threadblocks to the data in a way that maximizes data reuse and minimizes thread divergence. ThreadblockSwizzle functions in Cutlass can be used to map the blockIdx (block index) of the CUDA grid to GEMM problems. These functions implement different strategies for organizing the threads within a threadblock, such as tiling, padding, and transposing, in order to improve the efficiency of memory access patterns and to maximize the utilization of GPU resources. Overall, ThreadblockSwizzle in Cutlass is a powerful feature that can significantly improve the performance of matrix-multiplication operations on GPUs, particularly for large-scale problems. Comparison between typedef and using The following is generated by chatgpt In C++, both typedef and using can be used to create aliases for types. Here are some key differences between the two: Syntax12typedef &lt;existing_type&gt; &lt;new_type_alias&gt;;using &lt;new_type_alias&gt; = &lt;existing_type&gt;; Scopetypedef: A type alias created using typedef has the same scope as the original type. This means that it is accessible anywhere that the original type is accessible. using: A type alias created using using has a narrower scope than the original type. It is only accessible within the block in which it is defined. Functionalitytypedef: typedef can only be used to create aliases for types. It cannot be used to create template aliases. using: using can be used to create both type aliases and template aliases. This makes it more versatile than typedef. Readabilitytypedef: typedef can only be used to create aliases for types. It cannot be used to create template aliases. using: using can be used to create both type aliases and template aliases. This makes it more versatile than typedef. 05_batched_gemmThis example demonstrates how to use cutlass to compute a batched strided gemm in two different ways: By specifying pointers to the first matrices of the batch and the stride between the consecutive matrices of the batch (this is called a strided batched gemm). By copying pointers to all matrices of the batch to the device memory (this is called an array gemm). In the example, it simply calls two APIs which is cutlass::gemm::device::GemmArray and cutlass::gemm::device::GemmBatched. So I think it is time to read the source of cutlass. GemmArrayLet’s take GemmArray as an example. 1234567891011121314151617181920212223242526272829// GemmArray is defined in following file#include &quot;cutlass/gemm/device/gemm_array.h&quot;// simplified defination of GemmArraytemlate&lt; typename ElementA_, typename LayoutA_, typename ElementB_, typename LayoutB_, typename ElementC_, typename LayoutC_ //...&gt;class GemmArray{ public: // ignore some detailed attribute and functions using GemmKernel = kernel::GemmArray&lt;typename DefaultGemmKernel::Mma, typename DefaultGemmKernel::Epilogue, ThreadblockSwizzle&gt;; Status run(cudaStream_t stream = nullptr) { // ignore some detailed codes cutlass::Kernel&lt;GemmKernel&gt;&lt;&lt;&lt;grid, block, smem_size, stream&gt;&gt;&gt;(params_); } // overload operator () for calling gemm_op(...) Status operator()(cudaStream_t stream = nullptr) { return run(stream); }}; See, it is not very complicated. The class GemmArray is just built with many templates(the context of a class) and overloads operator () to call cutlass::Kernel. Then the question is coming. What is cutlass:Kernal? 1234567891011121314151617#include &quot;cutlass/device_kernel.h&quot;/// Generic CUTLASS kernel template.template &lt;typename Operator&gt;__global__void Kernel(typename Operator::Params params) { // Dynamic shared memory base pointer extern __shared__ int SharedStorageBase[]; // Declare pointer to dynamic shared memory. typename Operator::SharedStorage *shared_storage = reinterpret_cast&lt;typename Operator::SharedStorage *&gt;(SharedStorageBase); Operator op; op(params, *shared_storage);}; It is just a kernel template. So the important is Opearator of cutlass::Kernal which stands for cutlass::gemm::kernel::GemmArray. 123456789101112131415#include &quot;cutlass/gemm/kernel/gemm_array.h&quot;template &lt; typename Mma_, ///! Threadblock-scoped matrix multiply-accumulate typename Epilogue_, ///! Epilogue typename ThreadblockSwizzle_ ///! Threadblock swizzling function&gt;struct GemmArray{ // ignore some detailed attribute and functions CUTLASS_DEVICE void operator()(Params const &amp;params, SharedStorage &amp;shared_storage) { // codes run on device }}; So operator() is the core of class/struct in cutlass. And all the others are the context of that class/struct.","link":"/2023/03/21/learn-cutlass-1/"},{"title":"learn-cutlass-2","text":"I always wonder why cutlass provides many kinds of implementions of GEMM instead of just only one. In my opinion, in different situations the best implementions of GEMM differs. So that is what differs cutlass from cublas. You can make your own custiomlized implemention of GEMM to provide the best performance. The annotation in cutlass: When the template variables are passed to instantiate CUTLASS GEMM kernel, it internally deduce the amount of threads needed per thread-block, amount of shared memory, storing data in bank-conflict free manner, and ton of other variables required to compose, initialize and launch a high performance GEMM kernel. This is the beauty of CUTLASS, it relieves developer from understanding and coding complicated hardware optimizations which can easily go wrong. CUTLASS divides a kernel into hierarchical composable sections. Which means, at each thread, warp and thread-block level, they compute on their own tile-size with higher level of tile sizes being composed from lower level ones. Multiple thread-tiles (tile size each thread computes) can be used to form warp-tiles (tile size each warp computes) and multiple warp tiles can be used to compute threadblock-tile (tile size computed by a threadblock). InstructionShapeWhen it is used in tensor core operations for specifying the basic GEMM (M,N,K) such as 1using InstructionShape = cutlass::gemm::GemmShape&lt;8, 8, 4&gt;; // TensorCore instruction shape Or it is used in SIMT operations 1234// SIMT (except dp4a)using InstructionShape = cutlass::gemm::GemmShape&lt;1, 1, 1&gt;;// SIMT dp4ausing InstructionShape = cutlass::gemm::GemmShape&lt;1, 1, 4&gt;; dp4a12dp4a.atype.btype d, a, b, c;.atype = .btype = { .u32, .s32 }; Four-way byte dot product which is accumulated in 32-bit result.Operand a and b are 32-bit inputs which hold 4 byte inputs in packed form for dot product.Operand c has type .u32 if both .atype and .btype are .u32 else operand c has type .s32. 12345678d = c;∕∕ Extract 4 bytes from a 32bit input and sign or zero extend∕∕ based on input type.Va = extractAndSignOrZeroExt_4(a, .atype);Vb = extractAndSignOrZeroExt_4(b, .btype);for (i = 0; i &lt; 4; ++i) {d += Va[i] * Vb[i];} EpilogueThe above code focuses only on the matrix multiply computation C = AB whose result is held in the registers of each thread within the threadblock. The mapping of logical elements in the output tile to each thread is chosen to maximize performance of the matrix multiply computation but does not result in efficient, coalesced loads and stores to global memory. The epilogue is a separate phase in which threads exchange data through shared memory then cooperatively access global memory using efficient striped access patterns. It is also the phase in which linear scaling and other elementwise operations may be conveniently computed using the matrix product results as inputs. CUTLASS defines several typical epilogue operations such as linear scaling and clamping, but other device-side function call operators may be used to perform custom operations. 06_splitK_gemmsplitK is partitioning a GEMM with its K dimension. \\(C = \\sum_{i=1}^{K}{A_i}*B_i\\) 12// Define cutlass::gemm::device::GemmSplitKParallel#include &quot;cutlass/gemm/device/gemm_splitk_parallel.h&quot; In cutlass, templates are arguments/context of a function. Not all the combinations of templates work. So you need to know which combination is correct. To know its more, try to build a splitK using arch sm80 instead of sm70. One possible solution is as follows: 1234567891011121314142c142&lt; using SmArch = cutlass::arch::Sm70;---&gt; using SmArch = cutlass::arch::Sm80;150c150&lt; using ShapeMMAOp = cutlass::gemm::GemmShape&lt;8, 8, 4&gt;; // &lt;- MMA Op tile M = 8, N = 8, K = 4---&gt; using ShapeMMAOp = cutlass::gemm::GemmShape&lt;16, 8, 16&gt;; // &lt;- MMA Op tile M = 16, N = 8, K = 16188,189c188,189&lt; if (props.major != 7) {&lt; std::cerr &lt;&lt; &quot;Volta Tensor Ops must be run on a machine with compute capability of 70, 72, or 75.&quot;---&gt; if (props.major != 8 &amp;&amp; props.minor != 0) {&gt; std::cerr &lt;&lt; &quot;Amphere Tensor Ops must be run on a machine with compute capability of 80.&quot;","link":"/2023/03/24/learn-cutlass-2/"},{"title":"learn-cutlass-3","text":"Warp-level GEMMs may be implemented either by TensorCores issuing mma.sync or wmma instructions, or by thread-level matrix computations issued to CUDA cores. Wmma is an API in CUDA C++ for using TensorCores and if you want to use TensorCores by mma.sync you must use ptx by asm. WMMA is introduced first in Volta(CUDA exposes these operations as warp-level matrix operations in the CUDA C++ WMMA API). Using TensorCores by mma.syncThe following is a simple example that using mma.sync. The layout of A,B,C and D for mma.m8n8k4 is shown in PTX ISA.The mannual of inline PTX assembly is shown in here 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122/** * @author gtyinstinct * GEMM by TensorCores with mma.sync * D = A*B + C*/#include &lt;iostream&gt;#include &lt;iomanip&gt;#include &lt;string&gt;#include &lt;stdlib.h&gt;dim3 grid(1);dim3 block(32);const int M = 8;const int N = 8;const int K = 4;using A_TYPE = double;using B_TYPE = double;using C_TYPE = double;using D_TYPE = double;template&lt;typename A_TYPE, typename B_TYPE, typename C_TYPE, typename D_TYPE, int M, int N, int K &gt;__global__ void mma_test(A_TYPE *A,B_TYPE *B,C_TYPE *C,D_TYPE *D){ asm volatile(&quot;mma.sync.aligned.m8n8k4.row.col.f64.f64.f64.f64 {%0,%1}, {%2}, {%3}, {%4,%5};\\n&quot; : &quot;=d&quot;(D[threadIdx.x*2]), &quot;=d&quot;(D[threadIdx.x*2+1]) : &quot;d&quot;(A[threadIdx.x]), &quot;d&quot;(B[threadIdx.x%4*8+threadIdx.x/4]), &quot;d&quot;(C[threadIdx.x*2]), &quot;d&quot;(C[threadIdx.x*2+1]));} template&lt;typename A_TYPE, typename B_TYPE, typename C_TYPE, typename D_TYPE, int M, int N, int K &gt;__global__ void reference(A_TYPE *A,B_TYPE *B,C_TYPE *C,D_TYPE *D){ for(int idx=threadIdx.x;idx&lt;M*N;idx+=blockDim.x){ int row = idx / N; int col = idx % N; D_TYPE acc = 0; for(int i=0;i&lt;K;i++){ acc += A[row*K+i] * B[i*N+col]; } D[idx] = C[idx] + acc; }}template&lt;typename T&gt;void print(std::string des,T *arr,int row,int col){ std::cout &lt;&lt; &quot;-----&quot; &lt;&lt; des &lt;&lt; &quot;-----&quot; &lt;&lt; '\\n'; for(int i=0;i&lt;row;i++){ for(int j=0;j&lt;col;j++){ std::cout &lt;&lt; std::setw(4) &lt;&lt; arr[i*col+j] &lt;&lt; ' '; } std::cout &lt;&lt; '\\n'; }}template&lt;typename T&gt;void fill_matrix(T *arr,int size){ for(int i=0;i&lt;size;i++){ arr[i] = rand()%64; }}template&lt;typename T&gt;bool validate(T *arr,T *arr_ref,int size){ for(int i=0;i&lt;size;i++){ if(arr[i] != arr_ref[i]){ std::printf(&quot;at %d expected %f but got %f\\n&quot;,i,arr_ref[i],arr[i]); return 0; } } return 1;}int main(){ srand(time(NULL)); A_TYPE *A; B_TYPE *B; C_TYPE *C; D_TYPE *D,*D_ref; cudaMallocManaged(&amp;A,M*K*sizeof(A_TYPE)); cudaMallocManaged(&amp;B,K*N*sizeof(B_TYPE)); cudaMallocManaged(&amp;C,M*N*sizeof(C_TYPE)); cudaMallocManaged(&amp;D,M*N*sizeof(D_TYPE)); cudaMallocManaged(&amp;D_ref,M*N*sizeof(D_TYPE)); fill_matrix&lt;A_TYPE&gt;(A,M*K); fill_matrix&lt;B_TYPE&gt;(B,K*N); fill_matrix&lt;C_TYPE&gt;(C,M*N); print(&quot;A&quot;,A,M,K); print(&quot;B&quot;,B,K,N); print(&quot;C&quot;,C,M,N); mma_test&lt;A_TYPE,B_TYPE,C_TYPE,D_TYPE,M,N,K&gt;&lt;&lt;&lt;grid,block&gt;&gt;&gt;(A,B,C,D); reference&lt;A_TYPE,B_TYPE,C_TYPE,D_TYPE,M,N,K&gt;&lt;&lt;&lt;grid,block&gt;&gt;&gt;(A,B,C,D_ref); cudaDeviceSynchronize(); print(&quot;D&quot;,D,M,N); print(&quot;D_ref&quot;,D_ref,M,N); if(validate(D,D_ref,M*N)){ std::printf(&quot;PASS\\n&quot;); }else{ std::printf(&quot;FAIL\\n&quot;); }} mma.sync in CUTLASSDifferent shape/layout/arch mma.sync used in cutlass is listed at “cutlass/arch/mma*.h”. You can reference that to customize your GEMM. wmma API in CUTLASSDifferent shape/layout/arch wmma API used in cutlass is listed at “cutlass/arch/wmma*.h”.","link":"/2023/04/02/learn-cutlass-3/"},{"title":"learn-cutlass-4","text":"Cutlass examples gives us many examples to learn cutlass. At this time, 13_two_tensor_op_fusion is introduced. What is “two tensor op fusion”?Fusing two back to back GEMMs/Convolutions into one kernel. Why fusing?To avoid intermediate results round trip to memory. How fusing?Cutlass uses threadblockTile and WarpTile to respresent the shape of final result matrix which is calculated by a threadblock or warp respectively. First, if we want to fuse two kernel of GEMM , the number of threadblocks of two kernels must be the same. Second, to use the intermediate result the dimension M of two GEMM must be the same. Otherwisely, the input of second GEMM's threadblock may lie at other threadblock. Similarly, the dimension N of GEMM and the dimension N of threadblock must be the same.thread_block_tile_N = problem_N Third, the dimension N of threadblock and the dimension N of warp must be the same when using register, which can be relaxed when using share memory because different warps in a threadblock can exchange data with each other.warp_tile_N = thread_block_tile_Nrelaxed constraints","link":"/2023/04/16/learn-cutlass-4/"},{"title":"learn-cutlass-5","text":"Cutlass use abstract layout to express the mapping rules from logic index to physical index. Affine2 18_amphere_fp64_tensorop_affine2_gemm Affine2 is a speical layout in cutlass. In the normal GEMM, the fast changing dimension of a matrix always has strideequals to 1, e.g. ColumnMajor and RowMajor matrix. Affine2 matrix can havelarger than 1 stride in both dimensions. To support such layout, we need tochange to method to visit the global memory: We can only visit 1 element a time because elements are not storedconsecutively anymore. Vectorized load/store is not possible. One extra multiplication is needed in calculating the global memoryaddressaddr = base_pointer + coord1 * stride1 + coord2 * stride2 The explanation is a little abstract, let’s create an example to illustrate it. 12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;iostream&gt;#include &quot;cutlass/util/host_tensor.h&quot;#include &quot;cutlass/util/tensor_view_io.h&quot;#include &quot;cutlass/util/reference/host/tensor_fill.h&quot;using ElementInputA = double; // Data type of elements in input tensorusing LayoutInputA = cutlass::layout::AffineRank2ColumnMajor;int main() { // Construct Gemm ProblemSize with user defined output size cutlass::gemm::GemmCoord problem_size = {4, 4, 4}; typename LayoutInputA::Stride::Index stride_factor_A[] = {2, 2}; // Initialize tensors using CUTLASS helper functions cutlass::HostTensor&lt;ElementInputA, LayoutInputA&gt; tensor_a(problem_size.mk(), cutlass::layout::Affine2Layout_Factory&lt;LayoutInputA&gt;::layout_factory(problem_size.mk(), stride_factor_A)); // Fill input and output matrices on host using CUTLASS helper functions cutlass::reference::host::TensorFillRandomUniform( tensor_a.host_view(), 1, ElementInputA(100), ElementInputA(-100), 0); // &lt;- Fill matrix A on host with uniform-distribution random data std::cout &lt;&lt; tensor_a.host_view() &lt;&lt; &quot;\\n\\n&quot;; std::cout &lt;&lt; tensor_a.capacity() &lt;&lt; &quot;\\n&quot;; ElementInputA *a = tensor_a.host_data(); for(int i=0;i&lt;tensor_a.capacity();i++){ std::cout &lt;&lt; a[i] &lt;&lt; ' '; } std::cout &lt;&lt; '\\n';} And the output should be 123456768, -21, 56, 59,82, -60, -32, 53,-44, 10, -4, 25,-27, 2, 90, 836468 0 82 0 -44 0 -27 0 0 0 0 0 0 0 0 0 -21 0 -60 0 10 0 2 0 0 0 0 0 0 0 0 0 56 0 -32 0 -4 0 90 0 0 0 0 0 0 0 0 0 59 0 53 0 25 0 83 0 0 0 0 0 0 0 0 0 So affine2 is a layout that builds a submatrix through extracting original matrix based on the given stride. Quaternion 21_quaternion_gemm Quaternion is an interesting concept mostly used in computer graphics. In my opinion, it can be seen as analogy to complex number.The detailed information about quaternion can be found here.","link":"/2023/05/14/learn-cutlass-5/"},{"title":"冰雹","text":"中午在微博上看到广州下冰雹了，但是自己却没有遇见，今天下午刚到超算，听见外面有几声巨大的雷响，就发现外面在下冰雹，于是想出去感受下被冰雹砸中的感觉，刚到楼下就转为雨点了，这冰雹持续时间也太短了吧。记得上一次遇见冰雹还是在高中时候快高考在教室中模考，当时还特意去窗户边看了下。","link":"/2023/03/24/%E5%86%B0%E9%9B%B9/"}],"tags":[{"name":"NVIDIA","slug":"NVIDIA","link":"/tags/NVIDIA/"},{"name":"GPU","slug":"GPU","link":"/tags/GPU/"},{"name":"IR","slug":"IR","link":"/tags/IR/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"一生一芯","slug":"一生一芯","link":"/tags/%E4%B8%80%E7%94%9F%E4%B8%80%E8%8A%AF/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Icarus","slug":"Icarus","link":"/tags/Icarus/"},{"name":"ebook","slug":"ebook","link":"/tags/ebook/"},{"name":"tutorial","slug":"tutorial","link":"/tags/tutorial/"},{"name":"GEMM","slug":"GEMM","link":"/tags/GEMM/"},{"name":"cutlass","slug":"cutlass","link":"/tags/cutlass/"},{"name":"github","slug":"github","link":"/tags/github/"},{"name":"冰雹","slug":"冰雹","link":"/tags/%E5%86%B0%E9%9B%B9/"}],"categories":[{"name":"Technology","slug":"Technology","link":"/categories/Technology/"},{"name":"essay","slug":"essay","link":"/categories/essay/"},{"name":"log","slug":"log","link":"/categories/log/"}],"pages":[{"title":"Tianyu Guo (郭天宇)","text":"E-mail : guoty9[at]mail2.sysu.edu.cn My Resume Education Experience year university degree 2022 - now Sun Yat-Sen University Master 2018 - 2022 Xidian University B.S. Experience Teaching Assistant of “SYSU-DCS3013 : Computer Architecture” [2022f] release SYSU-ARCH LAB Projects Optimize GEMM step by step PTX-EMU: A simple emulator for CUDA program A simple CNN training framework support on CPU and GPU(CUDNN) GEMM by CUDA WMMA (tensor core) Mouse Controler(APP): Use your phone as a mouse eChat(APP): A simple Internet chatroom BCI: Web interface for mindwave MOBILE2(NeuroSky) Research Weekly Paper Sharing HPCA23 “DIMM-Link: Enabling Efficient Inter-DIMM Communication for Near-Memory Processing” AI final Homework “A Convolutional Neural Network Framework support on CPU and GPU” Bachelor’s dissertation “General Computing optimization for GPU based on Cache management”","link":"/info/index.html"}]}