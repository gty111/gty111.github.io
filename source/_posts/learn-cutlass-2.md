---
title: learn-cutlass-2
toc: true
date: 2023-03-24 14:29:03
categories:
- Technology
tags:
- cutlass
---

I always wonder why cutlass provides many kinds of implementions of GEMM instead of just only one. In my opinion, in different situations the best implementions of GEMM differs. So that is what differs cutlass from cublas. You can make your own custiomlized implemention of GEMM to provide the best performance.

<!-- more -->

---

The annotation in cutlass:

When the template variables are passed to instantiate CUTLASS GEMM kernel, it internally deduce the amount of threads needed per thread-block, amount of shared memory, storing data in bank-conflict free manner, and ton of other variables required to compose, initialize and launch a high performance GEMM kernel. This is the beauty of CUTLASS, it relieves developer from understanding and coding complicated hardware optimizations which can easily go wrong.


## InstructionShape
I think it is only used in tensor core operations for specifying the basic GEMM (M,N,K) such as 
```c++ 
using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>; // TensorCore instruction shape
```
You can view [this](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#element-types-and-matrix-sizes) for more info.

## Epilogue

> generated by chatgpt

The "epilogue" in Cutlass refers to the portion of the kernel that executes after the main computation is complete. The epilogue is responsible for performing final operations on the results of the computation, such as adding biases, applying activation functions, and quantizing the output.

## 06_splitK_gemm

`splitK` is partitioning a GEMM with its K dimension.

![](/img/splitK.svg)

\\(C = \sum_{i=1}^{K}{A_i}*B_i\\)

```c++
// Define cutlass::gemm::device::GemmSplitKParallel
#include "cutlass/gemm/device/gemm_splitk_parallel.h"
```

In cutlass, templates are arguments/context of a function. Not all the combinations of templates work. So you need to know which combination is correct. To know its more, try to build a splitK using arch sm80 instead of sm70.

