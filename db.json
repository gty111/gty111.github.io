{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-icarus/source/img/avatar.png","path":"img/avatar.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/img/favicon.svg","path":"img/favicon.svg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/img/logo.svg","path":"img/logo.svg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/img/og_image.png","path":"img/og_image.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/img/razor-bottom-black.svg","path":"img/razor-bottom-black.svg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/img/razor-top-black.svg","path":"img/razor-top-black.svg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/js/animation.js","path":"js/animation.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/js/back_to_top.js","path":"js/back_to_top.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/js/column.js","path":"js/column.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/css/cyberpunk.styl","path":"css/cyberpunk.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/css/default.styl","path":"css/default.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"source/img/logo.jpg","path":"img/logo.jpg","modified":0,"renderable":0},{"_id":"source/img/avatar.jpg","path":"img/avatar.jpg","modified":0,"renderable":0},{"_id":"source/info/doc/A Convolutional Neural Network Framework support on CPU and GPU.pdf","path":"info/doc/A Convolutional Neural Network Framework support on CPU and GPU.pdf","modified":0,"renderable":0},{"_id":"source/info/doc/Bachelor's dissertation.pdf","path":"info/doc/Bachelor's dissertation.pdf","modified":0,"renderable":0},{"_id":"source/info/doc/resume.pdf","path":"info/doc/resume.pdf","modified":0,"renderable":0},{"_id":"source/img/batched_gemm.jpg","path":"img/batched_gemm.jpg","modified":0,"renderable":0}],"Cache":[{"_id":"source/_posts/hello-world.md","hash":"2af856dead283c32d9d61de9e9a2d0d13a6b1c95","modified":1679307461339},{"_id":"node_modules/hexo-theme-icarus/layout/comment/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1679290006291},{"_id":"node_modules/hexo-theme-icarus/layout/misc/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1679290006291},{"_id":"node_modules/hexo-theme-icarus/layout/search/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1679290006291},{"_id":"node_modules/hexo-theme-icarus/layout/share/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1679290006291},{"_id":"node_modules/hexo-theme-icarus/layout/donate/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1679290006291},{"_id":"node_modules/hexo-theme-icarus/include/schema/comment/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1679290006291},{"_id":"node_modules/hexo-theme-icarus/include/schema/donate/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1679290006291},{"_id":"node_modules/hexo-theme-icarus/include/schema/misc/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1679290006291},{"_id":"node_modules/hexo-theme-icarus/include/schema/search/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1679290006291},{"_id":"node_modules/hexo-theme-icarus/include/schema/share/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1679290006291},{"_id":"node_modules/hexo-theme-icarus/CONTRIBUTING.md","hash":"70254c6778c1e41bb2ff222bbf3a70b2239b9bc1","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/LICENSE","hash":"86037e5335a49321fa73b7815cab542057fac944","modified":1679290006291},{"_id":"node_modules/hexo-theme-icarus/README.md","hash":"32f9f4fc8cd7ec60b30544bd2e558b593519ae5d","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/package.json","hash":"75db783b805785377db28d4cb844ee65bb7be613","modified":1679290006301},{"_id":"node_modules/hexo-theme-icarus/include/config.js","hash":"1ff0f174e9670074ad2bee890d5b6da486800c9a","modified":1679290006291},{"_id":"node_modules/hexo-theme-icarus/include/dependency.js","hash":"0ca35dec92ccf383f45db905db1a5a0e92d7209e","modified":1679290006291},{"_id":"node_modules/hexo-theme-icarus/include/register.js","hash":"ec6596b63bfb4349ba61792d905abe8e06fea625","modified":1679290006291},{"_id":"node_modules/hexo-theme-icarus/layout/archive.jsx","hash":"99bf235042d0c57af15d2f108ba5eda77443fea8","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/layout/categories.jsx","hash":"b8ad43e28a4990d222bfbb95b032f88555492347","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/layout/category.jsx","hash":"fd15e4eac32de9ac8687aeb3dbe179ab61375700","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/layout/index.jsx","hash":"0a84a2348394fa9fc5080dd396bd28d357594f47","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/layout/layout.jsx","hash":"ac7c4e3465a116c7f05f8c2e09ee6d6b9467abf1","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/layout/page.jsx","hash":"d26c2db57e5a88d6483a03aeb51cda9d191d8cea","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/layout/post.jsx","hash":"d26c2db57e5a88d6483a03aeb51cda9d191d8cea","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/layout/tag.jsx","hash":"d2f18cac32ca2725d34ccff3f2051c623be6c892","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/layout/tags.jsx","hash":"2c42cb64778235dd220c563a27a92108ddc50cc4","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/languages/de.yml","hash":"78421f09961ca0b24756a0688fb2cb2e2696e25f","modified":1679290006411},{"_id":"node_modules/hexo-theme-icarus/languages/en.yml","hash":"3d674204d9f723c829226da745afddd180c1131d","modified":1679290006411},{"_id":"node_modules/hexo-theme-icarus/languages/fr.yml","hash":"06d5c819d6108a42b28cff7b52e5410d0bed55d1","modified":1679290006411},{"_id":"node_modules/hexo-theme-icarus/languages/es.yml","hash":"38579b8fad4b6997362acc770615bcd85ff20f68","modified":1679290006411},{"_id":"node_modules/hexo-theme-icarus/languages/id.yml","hash":"5e48b1d62378cadeb64b88349477726a5c1bae47","modified":1679290006411},{"_id":"node_modules/hexo-theme-icarus/languages/ja.yml","hash":"801d9930fef48d6a3f80470d5bed4f3eb78147e6","modified":1679290006411},{"_id":"node_modules/hexo-theme-icarus/languages/pl.yml","hash":"2e7debb44cd91096f30efc87bf8d6b1d0d0214c9","modified":1679290006411},{"_id":"node_modules/hexo-theme-icarus/languages/pt-BR.yml","hash":"ee8f73350e4c6e2f63b7fc72b34472a6b1e21244","modified":1679290006411},{"_id":"node_modules/hexo-theme-icarus/languages/ko.yml","hash":"e3374265377809c1518114cf352b595840c0b416","modified":1679290006411},{"_id":"node_modules/hexo-theme-icarus/languages/ru.yml","hash":"9d91358c2acbe7a0f2a25daf7f65b999ff32d068","modified":1679290006411},{"_id":"node_modules/hexo-theme-icarus/languages/tk.yml","hash":"ca583168bd2025124a1cd0e977da475d7a7496fd","modified":1679290006411},{"_id":"node_modules/hexo-theme-icarus/languages/tr.yml","hash":"dd0a7bfe14848d6e1aa229198fe1db03e08e305e","modified":1679290006411},{"_id":"node_modules/hexo-theme-icarus/languages/vn.yml","hash":"5f2fffa642110c81d8f529949711c9d19ad6bbbe","modified":1679290006411},{"_id":"node_modules/hexo-theme-icarus/languages/zh-CN.yml","hash":"02475ba14afc70dfeaf5678467cee307835e4efa","modified":1679290006411},{"_id":"node_modules/hexo-theme-icarus/scripts/index.js","hash":"0c666db6fcb4ffc4d300f4e108c00ee42b1cbbe6","modified":1679290006291},{"_id":"node_modules/hexo-theme-icarus/languages/zh-TW.yml","hash":"a6826e0c8cdb9ad286324b682b466a9e2ad78e6f","modified":1679290006411},{"_id":"node_modules/hexo-theme-icarus/include/migration/head.js","hash":"7189efe33d18927d3790e8afb06642fb293b8603","modified":1679290006291},{"_id":"node_modules/hexo-theme-icarus/include/migration/v2_v3.js","hash":"3ccb2d2ce11018bebd7172da66faecc3983bff00","modified":1679290006291},{"_id":"node_modules/hexo-theme-icarus/include/migration/v3_v4.js","hash":"9faf2184d7fe87debfbe007f3fc9079dcbcafcfe","modified":1679290006301},{"_id":"node_modules/hexo-theme-icarus/include/migration/v4_v5.js","hash":"6342310892d113763b5544789b45d44c0ccf2854","modified":1679290006301},{"_id":"node_modules/hexo-theme-icarus/include/migration/v5_v5.1.js","hash":"073f22bd16e34b56f016633b1676dab2e7d8843d","modified":1679290006301},{"_id":"node_modules/hexo-theme-icarus/include/schema/config.json","hash":"f233678cd656c0e300181ca79dd30cb42fc213b3","modified":1679290006301},{"_id":"node_modules/hexo-theme-icarus/include/util/console.js","hash":"59cf9d277d3ac85a496689bd811b1c316001641d","modified":1679290006291},{"_id":"node_modules/hexo-theme-icarus/include/style/article.styl","hash":"105c983871b6c9148d97a0f756886e56411572bd","modified":1679290006401},{"_id":"node_modules/hexo-theme-icarus/include/style/base.styl","hash":"2bca6ad099949d52236c87db8db1002ffb99774c","modified":1679290006401},{"_id":"node_modules/hexo-theme-icarus/include/style/button.styl","hash":"0fb35b4786be1b387c751fa2849bc71523fcedd4","modified":1679290006401},{"_id":"node_modules/hexo-theme-icarus/include/style/card.styl","hash":"f78674422eb408cd17c17bbdc3ee1ebe4a453e05","modified":1679290006401},{"_id":"node_modules/hexo-theme-icarus/include/style/codeblock.styl","hash":"ec54dc24eb4d9802d8fefc44c210558bc1641109","modified":1679290006401},{"_id":"node_modules/hexo-theme-icarus/include/style/donate.styl","hash":"8d0af00628c13134b5f30a558608e7bebf18c2ec","modified":1679290006401},{"_id":"node_modules/hexo-theme-icarus/include/style/footer.styl","hash":"a4ad715dee38b249538ac6cce94efc9b355a904b","modified":1679290006401},{"_id":"node_modules/hexo-theme-icarus/include/style/helper.styl","hash":"9f3393e6122cc9f351091bfab960674e962da343","modified":1679290006401},{"_id":"node_modules/hexo-theme-icarus/include/style/navbar.styl","hash":"34f09b144cb46a25ec2cc7260a6c207dd34ff1fe","modified":1679290006401},{"_id":"node_modules/hexo-theme-icarus/include/style/pagination.styl","hash":"b81bcd7ff915b4e9299533addc01bc4575ec35e3","modified":1679290006401},{"_id":"node_modules/hexo-theme-icarus/include/style/plugin.styl","hash":"084843d5a522029e0f84a4fe791fbcb2cabd4c36","modified":1679290006401},{"_id":"node_modules/hexo-theme-icarus/include/style/responsive.styl","hash":"207083fe287612cddee6608b541861b14ac8de81","modified":1679290006401},{"_id":"node_modules/hexo-theme-icarus/include/style/search.styl","hash":"416737e1da4e7e907bd03609b0fee9e2aacfe56c","modified":1679290006411},{"_id":"node_modules/hexo-theme-icarus/include/style/timeline.styl","hash":"ea61798a09bffdda07efb93c2ff800b63bddc4c4","modified":1679290006411},{"_id":"node_modules/hexo-theme-icarus/include/style/widget.styl","hash":"c746902251136544eb3fe523235b3183f4189460","modified":1679290006411},{"_id":"node_modules/hexo-theme-icarus/layout/common/article.jsx","hash":"1d06eee32ea1fcb3162227eb1d7d19be39b6f5e3","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/layout/common/comment.jsx","hash":"427089c33002707b76e2f38709459a6824fd0f9b","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/layout/common/donates.jsx","hash":"889fb0a7ccc502f0a43b4a18eb330e351e50493c","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/layout/common/footer.jsx","hash":"de966666f1e4ef80e0d15081b2709c3065b246dd","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/layout/common/head.jsx","hash":"2ec1f511f32e3a9c86d49f1338f57ae5ece18898","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/layout/common/navbar.jsx","hash":"d96e501e52861056474659f96ee0206588d8c93a","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/layout/common/plugins.jsx","hash":"f6826c1a5f5f59f4a0aa00c63bdb0ad4ff4eab69","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/layout/common/scripts.jsx","hash":"4816c9099a881b5f7b13af3e42caae36edbffccd","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/layout/common/search.jsx","hash":"6f244a37293031670a2964fe424ecd062e591d7b","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/layout/common/share.jsx","hash":"c9fb0319ad5e5a10ad3636b26a6c2afed14c590f","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/layout/common/widgets.jsx","hash":"251263b97de12f2b8d1fce2514e83430f2515b94","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/layout/plugin/animejs.jsx","hash":"e2aa27c3501a58ef1e91e511557b77395c2c02aa","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/layout/plugin/back_to_top.jsx","hash":"7fc0c5aaabd7d0eaff04cb68ec139442dc3414e8","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/layout/widget/profile.jsx","hash":"0d3a7fd922c12cc45d2c8d26a8f4d3a9a6ed0ae0","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/source/img/avatar.png","hash":"0d8236dcca871735500e9d06bbdbe0853ed6775b","modified":1679290006311},{"_id":"node_modules/hexo-theme-icarus/source/img/favicon.svg","hash":"16fd847265845063a16596761cddb32926073dd2","modified":1679290006411},{"_id":"node_modules/hexo-theme-icarus/source/img/logo.svg","hash":"e9b5c1438ddb576693a15d0713b2a1d9ceda4be9","modified":1679290006411},{"_id":"node_modules/hexo-theme-icarus/source/img/og_image.png","hash":"b03f163096ca9c350ec962feee9836277b5c2509","modified":1679290006401},{"_id":"node_modules/hexo-theme-icarus/source/img/razor-bottom-black.svg","hash":"a3eda07b1c605b456da9cdf335a1075db5e5d72c","modified":1679290006411},{"_id":"node_modules/hexo-theme-icarus/source/img/razor-top-black.svg","hash":"201f1171a43ce667a39091fe47c0f278857f18f0","modified":1679290006411},{"_id":"node_modules/hexo-theme-icarus/source/js/animation.js","hash":"0a8e361c353daa3194f4de3d646b96025d128e1a","modified":1679290006291},{"_id":"node_modules/hexo-theme-icarus/source/js/back_to_top.js","hash":"d91f10c08c726135a13dfa1f422c49d8764ef03f","modified":1679290006291},{"_id":"node_modules/hexo-theme-icarus/source/js/column.js","hash":"0baee024ab67474c073a4c41b495f3e7f0df4505","modified":1679290006291},{"_id":"node_modules/hexo-theme-icarus/source/js/main.js","hash":"08a2641765eeaf712157ad134dd675e3f7708ae2","modified":1679290006291},{"_id":"node_modules/hexo-theme-icarus/source/css/cyberpunk.styl","hash":"ae17d3528df0c3f089df14a06b7bd82f1bc5fed9","modified":1679290006401},{"_id":"node_modules/hexo-theme-icarus/source/css/default.styl","hash":"b01da3028e5a1267a40aaae5c86a11187a2259e3","modified":1679290006401},{"_id":"node_modules/hexo-theme-icarus/source/css/style.styl","hash":"5b9815586e993a6ccbe8cdcfc0c65ea38fc315ac","modified":1679290006411},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/article.json","hash":"e2502c39045c6a26ccd8e880858f93e78c7bda35","modified":1679290006301},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/comment.json","hash":"f49270b619f5d2c3decde6b0b5a0c3bbab4b54a5","modified":1679290006301},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/donates.json","hash":"ae86e6f177bedf4afbe638502c12635027539305","modified":1679290006301},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/footer.json","hash":"e85c9d7f2579805beb252a1b6345d5a668a13baa","modified":1679290006301},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/head.json","hash":"98889f059c635e6bdbd51effd04cf1cf44968a66","modified":1679290006301},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/navbar.json","hash":"6691e587284c4cf450e0288680d5ff0f3565f090","modified":1679290006301},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/plugins.json","hash":"6036a805749816416850d944f7d64aaae62e5e75","modified":1679290006301},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/providers.json","hash":"97ec953d497fb53594227ae98acaef8a8baa91da","modified":1679290006301},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/search.json","hash":"985fbcbf47054af714ead1a124869d54f2a8b607","modified":1679290006301},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/share.json","hash":"cf4f9ff4fb27c3541b35f57db355c228fa6873e4","modified":1679290006301},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/sidebar.json","hash":"eb241beaec4c73e3085dfb3139ce72e827e20549","modified":1679290006301},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/widgets.json","hash":"cadd9dc942740ecd5037d3943e72f8b6a8399bbe","modified":1679290006301},{"_id":"node_modules/hexo-theme-icarus/include/schema/plugin/animejs.json","hash":"e62ab6e20bd8862efa1ed32e7c0db0f8acbcfdec","modified":1679290006301},{"_id":"node_modules/hexo-theme-icarus/include/schema/plugin/back_to_top.json","hash":"dc0febab7e7b67075d0ad3f80f5ec8b798b68dea","modified":1679290006301},{"_id":"node_modules/hexo-theme-icarus/include/schema/widget/profile.json","hash":"690ee1b0791cab47ea03cf42b5b4932ed2aa5675","modified":1679290006301},{"_id":"source/logo.jpg","hash":"9df20c44fae9c3b25be1603ade0c91d4551cc2a2","modified":1679291415851},{"_id":"source/img/logo.jpg","hash":"af002a111cb40bcd7cdd070e58827c840fc6393f","modified":1679291612416},{"_id":"source/doc/A Convolutional Neural Network Framework support on CPU and GPU.pdf","hash":"ce8c54f1892b4aedc5bd83a972a812de8a674f9f","modified":1679303726690},{"_id":"source/home/index.md","hash":"40d35caca83269bdf194d328f550c922365f079e","modified":1679303726710},{"_id":"source/doc/resume.pdf","hash":"bbf324f7aceb80ec3672b25ed71a92af8f6b1b89","modified":1679303726710},{"_id":"source/img/avatar.jpg","hash":"e2fe014b6234172847599124e08a97296d0d5a63","modified":1679303726710},{"_id":"source/doc/Bachelor's dissertation.pdf","hash":"ea44b5b4104cadb900922535fd1505cc7ef5b8e6","modified":1679303726710},{"_id":"source/resume/index.md","hash":"40d35caca83269bdf194d328f550c922365f079e","modified":1679303726710},{"_id":"source/resume/doc/resume.pdf","hash":"bbf324f7aceb80ec3672b25ed71a92af8f6b1b89","modified":1679303726710},{"_id":"source/resume/doc/A Convolutional Neural Network Framework support on CPU and GPU.pdf","hash":"ce8c54f1892b4aedc5bd83a972a812de8a674f9f","modified":1679303726690},{"_id":"source/resume/doc/Bachelor's dissertation.pdf","hash":"ea44b5b4104cadb900922535fd1505cc7ef5b8e6","modified":1679303726710},{"_id":"source/info/index.md","hash":"e2fe9358d79ffa7d51cde469be2024bc0c9de43b","modified":1679313266359},{"_id":"source/info/doc/resume.pdf","hash":"bbf324f7aceb80ec3672b25ed71a92af8f6b1b89","modified":1679303726710},{"_id":"source/info/doc/A Convolutional Neural Network Framework support on CPU and GPU.pdf","hash":"ce8c54f1892b4aedc5bd83a972a812de8a674f9f","modified":1679303726690},{"_id":"source/info/doc/Bachelor's dissertation.pdf","hash":"ea44b5b4104cadb900922535fd1505cc7ef5b8e6","modified":1679303726710},{"_id":"source/index.md","hash":"fb1bdd813bda7129e7ff04dc4229f09f3bff88d0","modified":1679305747519},{"_id":"source/test.md","hash":"fb1bdd813bda7129e7ff04dc4229f09f3bff88d0","modified":1679305747519},{"_id":"source/_posts/build_web.md","hash":"9ecea3c385433630beddd4cb41a90f1c285a311c","modified":1679322691178},{"_id":"public/js/algolia.js","hash":"a8df0c0abeeb4ee1d2d720161f3aea7339380704","modified":1679310480983},{"_id":"public/js/google_cse.js","hash":"1a9881669dfdeb2b3214074eee0d3e01e52db2c4","modified":1679310480983},{"_id":"public/js/insight.js","hash":"86bbdb7305d9bf19ad62d2ca2cf169fc8d9f9d31","modified":1679310480983},{"_id":"public/js/toc.js","hash":"da6fb757a1b083b8ed138bf29aad3a7bf8ec4f11","modified":1679310480983},{"_id":"public/content.json","hash":"8dc472669c5ae65fba39685e194cbf69c3f64a6b","modified":1679310480983},{"_id":"public/manifest.json","hash":"2aefae3bb4579bef42c6fc3a509dcece98fe34b1","modified":1679310480983},{"_id":"public/archives/index.html","hash":"43cf942a28796753c0615e399b9ae6dccf9dc5fb","modified":1679310480983},{"_id":"public/archives/2023/index.html","hash":"9bf6ade40c04ec360c266c7406d3e8be318e7d2a","modified":1679310480983},{"_id":"public/archives/2023/03/index.html","hash":"724b0573c24830e72fa64dd9ab84b2cdbfd5d803","modified":1679310480983},{"_id":"public/categories/index.html","hash":"610192178eb3ee135c433ac81ce8ac2200220454","modified":1679310480983},{"_id":"public/2023/03/20/build_web/index.html","hash":"3fd1788567ae2ca81e3225775a3ff10238d8f7e3","modified":1679310480983},{"_id":"public/info/index.html","hash":"9654f0b9d62b24f11024bf98e6ed291d8883fb7c","modified":1679310480983},{"_id":"public/categories/blog/index.html","hash":"46165f66d29a00665c37c0691d6e780995c4245d","modified":1679309094397},{"_id":"public/tags/Hexo/index.html","hash":"3b9753a4b2889f8e4f488bd7e360c6c95626d329","modified":1679310480983},{"_id":"public/index.html","hash":"31c75f53ef5ad9326faa759f33418bf6d2e3b4ef","modified":1679310480983},{"_id":"public/tags/Icarus/index.html","hash":"832bed9d4b0697a0287d632a6ec83264a815fdcf","modified":1679310480983},{"_id":"public/tags/index.html","hash":"9d7f538de2966fc06fabeb80defb2b795a236c2a","modified":1679310480983},{"_id":"public/img/avatar.png","hash":"0d8236dcca871735500e9d06bbdbe0853ed6775b","modified":1679310480983},{"_id":"public/img/favicon.svg","hash":"16fd847265845063a16596761cddb32926073dd2","modified":1679310480983},{"_id":"public/img/logo.svg","hash":"e9b5c1438ddb576693a15d0713b2a1d9ceda4be9","modified":1679310480983},{"_id":"public/img/razor-bottom-black.svg","hash":"a3eda07b1c605b456da9cdf335a1075db5e5d72c","modified":1679310480983},{"_id":"public/img/og_image.png","hash":"b03f163096ca9c350ec962feee9836277b5c2509","modified":1679310480983},{"_id":"public/img/razor-top-black.svg","hash":"201f1171a43ce667a39091fe47c0f278857f18f0","modified":1679310480983},{"_id":"public/info/doc/resume.pdf","hash":"bbf324f7aceb80ec3672b25ed71a92af8f6b1b89","modified":1679310480983},{"_id":"public/js/back_to_top.js","hash":"d91f10c08c726135a13dfa1f422c49d8764ef03f","modified":1679310480983},{"_id":"public/js/column.js","hash":"0baee024ab67474c073a4c41b495f3e7f0df4505","modified":1679310480983},{"_id":"public/js/main.js","hash":"08a2641765eeaf712157ad134dd675e3f7708ae2","modified":1679310480983},{"_id":"public/js/animation.js","hash":"0a8e361c353daa3194f4de3d646b96025d128e1a","modified":1679310480983},{"_id":"public/css/cyberpunk.css","hash":"b462177bee11f76e84e649f8192d5a68cb4b1402","modified":1679310480983},{"_id":"public/css/style.css","hash":"8717dd0ed1fe2a4237fbfb4f4f6e566cb1ee16b7","modified":1679310480983},{"_id":"public/css/default.css","hash":"8717dd0ed1fe2a4237fbfb4f4f6e566cb1ee16b7","modified":1679310480983},{"_id":"public/img/avatar.jpg","hash":"e2fe014b6234172847599124e08a97296d0d5a63","modified":1679310480983},{"_id":"public/img/logo.jpg","hash":"af002a111cb40bcd7cdd070e58827c840fc6393f","modified":1679310480983},{"_id":"public/info/doc/A Convolutional Neural Network Framework support on CPU and GPU.pdf","hash":"ce8c54f1892b4aedc5bd83a972a812de8a674f9f","modified":1679310480983},{"_id":"public/info/doc/Bachelor's dissertation.pdf","hash":"ea44b5b4104cadb900922535fd1505cc7ef5b8e6","modified":1679310480983},{"_id":"public/categories/Technology/index.html","hash":"9116f6792809cd63d74bb242477aad7c7a7e8916","modified":1679310480983},{"_id":"source/_posts/learn-cutlass-0.md","hash":"cbb6a7ed6b17fb80c4bb5ea4540a5fd204a0e080","modified":1679321974098},{"_id":"source/_posts/learn-cutlass-1.md","hash":"510ff5538099577fae301766f346d4b2b469f2fd","modified":1679363714637},{"_id":"source/img/batched_gemm.jpg","hash":"4f78d083a5d56c3776e819d561096f867a088612","modified":1679363504738}],"Category":[{"name":"Diary","_id":"clfgmcq4y00015r7v5d5y0zxu"},{"name":"D","_id":"clfgmcroy00035r7v8507fe4l"},{"name":"test","_id":"clfgmcvzm00055r7vfj5pedph"},{"name":"test=","_id":"clfgmd82p000l5r7veubv5ph5"},{"name":"test-","_id":"clfgmd8um000o5r7v4qdq3t8k"},{"name":"test-ca","_id":"clfgmd9es000q5r7vhnixfdpn"},{"name":"test-cat","_id":"clfgmd9n1000s5r7vc5ezgl2s"},{"name":"test-cate","_id":"clfgmd9to000u5r7vcbwd7oen"},{"name":"test-categor","_id":"clfgmdagu000w5r7vhiqi9i5c"},{"name":"test-category","_id":"clfgmdavd000y5r7vf1ap9gxb"},{"name":"blog","_id":"clfgo5q8c0000bv7vbaihfs36"},{"name":"Technology","_id":"clfgq3prx000038z3aixz3fox"}],"Data":[],"Page":[{"title":"Tianyu Guo (郭天宇)","_content":"\n\n\nE-mail : guoty9[at]mail2.sysu.edu.cn\n\n## [My Resume](doc/resume.pdf)\n\n---\n\n## Education Experience\n\n| year        | university             | degree |\n| ----------- | ---------------------- | ------ |\n| 2018 - 2022 | Xidian University      | B.S.   |\n| 2022 - now  | Sun Yat-Sen University | Master |\n\n## Experience\n\n- Teaching Assistant of \"SYSU-DCS3013 : Computer Architecture\" [2022f]\n  \n  > release [SYSU-ARCH LAB](https://arcsysu.github.io/SYSU-ARCH)\n\n## Research\n\n- [Bachelor's dissertation](doc/Bachelor's%20dissertation.pdf) “General Computing optimization for GPU based on Cache management”\n\n- [AI final Homework](doc/A%20Convolutional%20Neural%20Network%20Framework%20support%20on%20CPU%20and%20GPU.pdf) \"A Convolutional Neural Network Framework support on CPU and GPU\"\n","source":"info/index.md","raw":"---\ntitle: Tianyu Guo (郭天宇)\n---\n\n\n\nE-mail : guoty9[at]mail2.sysu.edu.cn\n\n## [My Resume](doc/resume.pdf)\n\n---\n\n## Education Experience\n\n| year        | university             | degree |\n| ----------- | ---------------------- | ------ |\n| 2018 - 2022 | Xidian University      | B.S.   |\n| 2022 - now  | Sun Yat-Sen University | Master |\n\n## Experience\n\n- Teaching Assistant of \"SYSU-DCS3013 : Computer Architecture\" [2022f]\n  \n  > release [SYSU-ARCH LAB](https://arcsysu.github.io/SYSU-ARCH)\n\n## Research\n\n- [Bachelor's dissertation](doc/Bachelor's%20dissertation.pdf) “General Computing optimization for GPU based on Cache management”\n\n- [AI final Homework](doc/A%20Convolutional%20Neural%20Network%20Framework%20support%20on%20CPU%20and%20GPU.pdf) \"A Convolutional Neural Network Framework support on CPU and GPU\"\n","date":"2023-03-20T11:54:26.359Z","updated":"2023-03-20T11:54:26.359Z","path":"info/index.html","_id":"clfgmte2m0000zi7vbmmb8urr","comments":1,"layout":"page","content":"<p>E-mail : guoty9[at]mail2.sysu.edu.cn</p>\n<h2 id=\"My-Resume\"><a href=\"#My-Resume\" class=\"headerlink\" title=\"My Resume\"></a><a href=\"doc/resume.pdf\">My Resume</a></h2><hr>\n<h2 id=\"Education-Experience\"><a href=\"#Education-Experience\" class=\"headerlink\" title=\"Education Experience\"></a>Education Experience</h2><table>\n<thead>\n<tr>\n<th>year</th>\n<th>university</th>\n<th>degree</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>2018 - 2022</td>\n<td>Xidian University</td>\n<td>B.S.</td>\n</tr>\n<tr>\n<td>2022 - now</td>\n<td>Sun Yat-Sen University</td>\n<td>Master</td>\n</tr>\n</tbody></table>\n<h2 id=\"Experience\"><a href=\"#Experience\" class=\"headerlink\" title=\"Experience\"></a>Experience</h2><ul>\n<li><p>Teaching Assistant of “SYSU-DCS3013 : Computer Architecture” [2022f]</p>\n<blockquote>\n<p>release <a href=\"https://arcsysu.github.io/SYSU-ARCH\">SYSU-ARCH LAB</a></p>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"Research\"><a href=\"#Research\" class=\"headerlink\" title=\"Research\"></a>Research</h2><ul>\n<li><p><a href=\"doc/Bachelor's%20dissertation.pdf\">Bachelor’s dissertation</a> “General Computing optimization for GPU based on Cache management”</p>\n</li>\n<li><p><a href=\"doc/A%20Convolutional%20Neural%20Network%20Framework%20support%20on%20CPU%20and%20GPU.pdf\">AI final Homework</a> “A Convolutional Neural Network Framework support on CPU and GPU”</p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>E-mail : guoty9[at]mail2.sysu.edu.cn</p>\n<h2 id=\"My-Resume\"><a href=\"#My-Resume\" class=\"headerlink\" title=\"My Resume\"></a><a href=\"doc/resume.pdf\">My Resume</a></h2><hr>\n<h2 id=\"Education-Experience\"><a href=\"#Education-Experience\" class=\"headerlink\" title=\"Education Experience\"></a>Education Experience</h2><table>\n<thead>\n<tr>\n<th>year</th>\n<th>university</th>\n<th>degree</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>2018 - 2022</td>\n<td>Xidian University</td>\n<td>B.S.</td>\n</tr>\n<tr>\n<td>2022 - now</td>\n<td>Sun Yat-Sen University</td>\n<td>Master</td>\n</tr>\n</tbody></table>\n<h2 id=\"Experience\"><a href=\"#Experience\" class=\"headerlink\" title=\"Experience\"></a>Experience</h2><ul>\n<li><p>Teaching Assistant of “SYSU-DCS3013 : Computer Architecture” [2022f]</p>\n<blockquote>\n<p>release <a href=\"https://arcsysu.github.io/SYSU-ARCH\">SYSU-ARCH LAB</a></p>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"Research\"><a href=\"#Research\" class=\"headerlink\" title=\"Research\"></a>Research</h2><ul>\n<li><p><a href=\"doc/Bachelor's%20dissertation.pdf\">Bachelor’s dissertation</a> “General Computing optimization for GPU based on Cache management”</p>\n</li>\n<li><p><a href=\"doc/A%20Convolutional%20Neural%20Network%20Framework%20support%20on%20CPU%20and%20GPU.pdf\">AI final Homework</a> “A Convolutional Neural Network Framework support on CPU and GPU”</p>\n</li>\n</ul>\n"}],"Post":[{"title":"How to build this web","_content":"\nThis web is building by Hexo and Icarus.\n\n<!-- more -->\n\n## Install Node.js\n\n- for linux [reference this](https://github.com/nodesource/distributions)\n- for windows through [nvs](https://github.com/jasongin/nvs/) or [nvm](https://github.com/nvm-sh/nvm)\n- for mac through [Homebrew](https://brew.sh/) or [MacPorts](http://www.macports.org/)\n\n\n\n## Install Hexo (require Node.js)\n\n```bash\nnpm install -g hexo-cli\n```\n\n## Create your site\n```bash\nhexo init <folder> \ncd <folder>\nnpm install\nnpm install -S hexo-theme-icarus hexo-renderer-inferno # install icarus\nhexo config theme icarus # use theme icarus\nhexo server # start server at localhost\n```\n\n## Tips about Hexo or Icarus\n\n- Add `read more` to your blogs : just add `<!-- more -->` in your md at proper position.\n\n\n## Reference\n\n- [hexo-tutorial](https://hexo.io/zh-cn/docs/)\n- [getting-started-with-icarus](https://ppoffice.github.io/hexo-theme-icarus/uncategorized/getting-started-with-icarus/)\n\n\n\n","source":"_posts/build_web.md","raw":"---\ntitle: How to build this web\ncategories:\n- Technology\ntags:\n- Hexo\n- Icarus\n---\n\nThis web is building by Hexo and Icarus.\n\n<!-- more -->\n\n## Install Node.js\n\n- for linux [reference this](https://github.com/nodesource/distributions)\n- for windows through [nvs](https://github.com/jasongin/nvs/) or [nvm](https://github.com/nvm-sh/nvm)\n- for mac through [Homebrew](https://brew.sh/) or [MacPorts](http://www.macports.org/)\n\n\n\n## Install Hexo (require Node.js)\n\n```bash\nnpm install -g hexo-cli\n```\n\n## Create your site\n```bash\nhexo init <folder> \ncd <folder>\nnpm install\nnpm install -S hexo-theme-icarus hexo-renderer-inferno # install icarus\nhexo config theme icarus # use theme icarus\nhexo server # start server at localhost\n```\n\n## Tips about Hexo or Icarus\n\n- Add `read more` to your blogs : just add `<!-- more -->` in your md at proper position.\n\n\n## Reference\n\n- [hexo-tutorial](https://hexo.io/zh-cn/docs/)\n- [getting-started-with-icarus](https://ppoffice.github.io/hexo-theme-icarus/uncategorized/getting-started-with-icarus/)\n\n\n\n","slug":"build_web","published":1,"date":"2023-03-20T11:48:26.929Z","updated":"2023-03-20T14:31:31.178Z","_id":"clfgp9yzh0000j17v2zo7eyfy","comments":1,"layout":"post","photos":[],"link":"","content":"<p>This web is building by Hexo and Icarus.</p>\n<span id=\"more\"></span>\n\n<h2 id=\"Install-Node-js\"><a href=\"#Install-Node-js\" class=\"headerlink\" title=\"Install Node.js\"></a>Install Node.js</h2><ul>\n<li>for linux <a href=\"https://github.com/nodesource/distributions\">reference this</a></li>\n<li>for windows through <a href=\"https://github.com/jasongin/nvs/\">nvs</a> or <a href=\"https://github.com/nvm-sh/nvm\">nvm</a></li>\n<li>for mac through <a href=\"https://brew.sh/\">Homebrew</a> or <a href=\"http://www.macports.org/\">MacPorts</a></li>\n</ul>\n<h2 id=\"Install-Hexo-require-Node-js\"><a href=\"#Install-Hexo-require-Node-js\" class=\"headerlink\" title=\"Install Hexo (require Node.js)\"></a>Install Hexo (require Node.js)</h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install -g hexo-cli</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Create-your-site\"><a href=\"#Create-your-site\" class=\"headerlink\" title=\"Create your site\"></a>Create your site</h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo init &lt;folder&gt; </span><br><span class=\"line\"><span class=\"built_in\">cd</span> &lt;folder&gt;</span><br><span class=\"line\">npm install</span><br><span class=\"line\">npm install -S hexo-theme-icarus hexo-renderer-inferno <span class=\"comment\"># install icarus</span></span><br><span class=\"line\">hexo config theme icarus <span class=\"comment\"># use theme icarus</span></span><br><span class=\"line\">hexo server <span class=\"comment\"># start server at localhost</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Tips-about-Hexo-or-Icarus\"><a href=\"#Tips-about-Hexo-or-Icarus\" class=\"headerlink\" title=\"Tips about Hexo or Icarus\"></a>Tips about Hexo or Icarus</h2><ul>\n<li>Add <code>read more</code> to your blogs : just add <code>&lt;!-- more --&gt;</code> in your md at proper position.</li>\n</ul>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://hexo.io/zh-cn/docs/\">hexo-tutorial</a></li>\n<li><a href=\"https://ppoffice.github.io/hexo-theme-icarus/uncategorized/getting-started-with-icarus/\">getting-started-with-icarus</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>This web is building by Hexo and Icarus.</p>","more":"<h2 id=\"Install-Node-js\"><a href=\"#Install-Node-js\" class=\"headerlink\" title=\"Install Node.js\"></a>Install Node.js</h2><ul>\n<li>for linux <a href=\"https://github.com/nodesource/distributions\">reference this</a></li>\n<li>for windows through <a href=\"https://github.com/jasongin/nvs/\">nvs</a> or <a href=\"https://github.com/nvm-sh/nvm\">nvm</a></li>\n<li>for mac through <a href=\"https://brew.sh/\">Homebrew</a> or <a href=\"http://www.macports.org/\">MacPorts</a></li>\n</ul>\n<h2 id=\"Install-Hexo-require-Node-js\"><a href=\"#Install-Hexo-require-Node-js\" class=\"headerlink\" title=\"Install Hexo (require Node.js)\"></a>Install Hexo (require Node.js)</h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install -g hexo-cli</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Create-your-site\"><a href=\"#Create-your-site\" class=\"headerlink\" title=\"Create your site\"></a>Create your site</h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo init &lt;folder&gt; </span><br><span class=\"line\"><span class=\"built_in\">cd</span> &lt;folder&gt;</span><br><span class=\"line\">npm install</span><br><span class=\"line\">npm install -S hexo-theme-icarus hexo-renderer-inferno <span class=\"comment\"># install icarus</span></span><br><span class=\"line\">hexo config theme icarus <span class=\"comment\"># use theme icarus</span></span><br><span class=\"line\">hexo server <span class=\"comment\"># start server at localhost</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Tips-about-Hexo-or-Icarus\"><a href=\"#Tips-about-Hexo-or-Icarus\" class=\"headerlink\" title=\"Tips about Hexo or Icarus\"></a>Tips about Hexo or Icarus</h2><ul>\n<li>Add <code>read more</code> to your blogs : just add <code>&lt;!-- more --&gt;</code> in your md at proper position.</li>\n</ul>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://hexo.io/zh-cn/docs/\">hexo-tutorial</a></li>\n<li><a href=\"https://ppoffice.github.io/hexo-theme-icarus/uncategorized/getting-started-with-icarus/\">getting-started-with-icarus</a></li>\n</ul>"},{"title":"learn-cutlass-0","date":"2023-03-20T12:44:12.000Z","_content":"\n> learn cutlass is a series of tutorials to learn cutlass by reading its examples or source code\n\nCUTLASS is a header-only template library. After reading that, you will **be lost in templates**.\n<!-- more -->\n\n## 00_basic_gemm\n\n```c++\n// Defines cutlass::gemm::device::Gemm, the generic Gemm computation template class.\n#include \"cutlass/gemm/device/gemm.h\"\n\nusing CutlassGemm = cutlass::gemm::device::Gemm<A_TYPE,A_LAYOUT,B_TYPE,B_LAYOUT,C_TYPE,C_LAYOUT> ;\n// where A_TYPE is Data-type of A matrix and A_LAYOUT is Layout of A matrix\n\nCutlassGemm gemm_operator;\n\nCutlassGemm::Arguments args({M , N, K},  \n                            {A_POINTER, lda},    \n                            {B_POINTER, ldb},   \n                            {C_POINTER, ldc},    \n                            {C_POINTER, ldc},    \n                            {alpha, beta});\n// where A_POINTER is pointer of A matrix and lda is the number of elements between consecutive rows or colmns\n\ncutlass::Status status = gemm_operator(args);\n// call gemm operation\n\n```\n\n## 01_cutlass_utilities\n\n```c++\n// CUTLASS includes needed for half-precision GEMM kernel\n#include \"cutlass/cutlass.h\"\n#include \"cutlass/core_io.h\"\n#include \"cutlass/layout/matrix.h\"\n#include \"cutlass/gemm/device/gemm.h\"\n\n//\n// CUTLASS utility includes\n//\n\n// Defines operator<<() to write TensorView objects to std::ostream\n#include \"cutlass/util/tensor_view_io.h\"\n\n// Defines cutlass::HostTensor<>\n#include \"cutlass/util/host_tensor.h\"\n\n// Defines cutlass::half_t\n#include \"cutlass/numeric_types.h\"\n\n// Defines device_memory::copy_device_to_device()\n#include \"cutlass/util/device_memory.h\"\n\n// Defines cutlass::reference::device::TensorFillRandomGaussian()\n#include \"cutlass/util/reference/device/tensor_fill.h\"\n\n// Defines cutlass::reference::host::TensorEquals()\n#include \"cutlass/util/reference/host/tensor_compare.h\"\n\n// Defines cutlass::reference::host::Gemm()\n#include \"cutlass/util/reference/host/gemm.h\"\n\n\n// another way to call gemm without using Arguments\ncutlass::Status status = gemm_op({\n    {M, N, K},\n    {A, lda},\n    {B, ldb},\n    {C, ldc},\n    {C, ldc},\n    {alpha, beta}\n  });\n\n// define a tensor (M,N) in cutlass, where DTYPE is data type\ncutlass::HostTensor<DTYPE,LAYOUT> VAR(cutlass::MatrixCoord(M,N)) ;\ncutlass::HostTensor<cutlass::half_t, cutlass::layout::ColumnMajor> A(cutlass::MatrixCoord(M, K));\n\n// fill a tensor (RandomGaussian) where A.device_view() return TensorView of that tensor in cutlass\ncutlass::reference::device::TensorFillRandomGaussian(\n    A.device_view(),\n    seed,\n    mean,\n    stddev,\n    bits_less_than_one\n  );\n\n// copy data from device to device in cutlass where A.device_data() return pointer of that tensor\n// A.capacity() return the logical capacity based on extent and layout. May differ from size().\ncutlass::device_memory::copy_device_to_device(\n    C_reference.device_data(), \n    C_cutlass.device_data(), \n    C_cutlass.capacity());\n\n// Copies data from device to host\nA.sync_host();\n\n// Copies data from host to device\nA.sync_device();\n\n// Compute the reference result using the host-side GEMM reference implementation.\n// I think the only difference between TensorView and TensorRef is that TensorView is read-only \n// while TensorRef can return pointer of matrix\ncutlass::reference::host::Gemm<\n    cutlass::half_t,                           // ElementA\n    cutlass::layout::ColumnMajor,              // LayoutA\n    cutlass::half_t,                           // ElementB\n    cutlass::layout::ColumnMajor,              // LayoutB\n    cutlass::half_t,                           // ElementOutput\n    cutlass::layout::ColumnMajor,              // LayoutOutput\n    cutlass::half_t,                           // ScalarType\n    cutlass::half_t                            // ComputeType\n> gemm_ref;\n\ngemm_ref(\n    {M, N, K},                          // problem size (type: cutlass::gemm::GemmCoord)\n    alpha,                              // alpha        (type: cutlass::half_t)\n    A.host_ref(),                       // A            (type: TensorRef<half_t, ColumnMajor>)\n    B.host_ref(),                       // B            (type: TensorRef<half_t, ColumnMajor>)\n    beta,                               // beta         (type: cutlass::half_t)\n    C_reference.host_ref()              // C            (type: TensorRef<half_t, ColumnMajor>)\n);\n\n// Compare reference to computed results\ncutlass::reference::host::TensorEquals(\n    C_reference.host_view(), \n    C_cutlass.host_view());\n```\n\n## 04_tile_iterator\n\n```c++\n// CUTLASS includes\n#include \"cutlass/transform/threadblock/predicated_tile_iterator.h\"\n#include \"cutlass/layout/pitch_linear.h\"\n#include \"cutlass/transform/pitch_linear_thread_map.h\"\n\n//\n//  CUTLASS utility includes\n//\n\n// Defines operator<<() to write TensorView objects to std::ostream\n#include \"cutlass/util/tensor_view_io.h\"\n\n// Defines cutlass::HostTensor<>\n#include \"cutlass/util/host_tensor.h\"\n\n// Defines cutlass::reference::host::TensorFill() and\n// cutlass::reference::host::TensorFillBlockSequential()\n#include \"cutlass/util/reference/host/tensor_fill.h\"\n\n\n// For this example, we chose a <64, 4> tile shape. The PredicateTileIterator expects\n// PitchLinearShape and PitchLinear layout.\nusing Shape = cutlass::layout::PitchLinearShape<64, 4>;\nusing Layout = cutlass::layout::PitchLinear;\nusing Element = int;\nint const kThreads = 32;\n\n// ThreadMaps define how threads are mapped to a given tile. The PitchLinearStripminedThreadMap\n// stripmines a pitch-linear tile among a given number of threads, first along the contiguous\n// dimension then along the strided dimension.\nusing ThreadMap = cutlass::transform::PitchLinearStripminedThreadMap<Shape, kThreads>;\n\n// Define the PredicateTileIterator, using TileShape, Element, Layout, and ThreadMap types\nusing Iterator = cutlass::transform::threadblock::PredicatedTileIterator<\n    Shape, Element, Layout, 1, ThreadMap>;\n\n\ncutlass::Coord<2> copy_extent = cutlass::make_Coord(M, K);\ncutlass::Coord<2> alloc_extent = cutlass::make_Coord(M, K);\n\n// another way to define tensor\n// Allocate source and destination tensors\ncutlass::HostTensor<Element, Layout> src_tensor(alloc_extent);\ncutlass::HostTensor<Element, Layout> dst_tensor(alloc_extent);\n\nElement oob_value = Element(-1);\n\n// Initialize destination tensor with all -1s\ncutlass::reference::host::TensorFill(dst_tensor.host_view(), oob_value);\n// Initialize source tensor with sequentially increasing values\ncutlass::reference::host::BlockFillSequential(src_tensor.host_data(), src_tensor.capacity());\n\ndst_tensor.sync_device();\nsrc_tensor.sync_device();\n\ntypename Iterator::Params dst_params(dst_tensor.layout());\ntypename Iterator::Params src_params(src_tensor.layout());\n\ndim3 block(kThreads, 1);\ndim3 grid(1, 1);\n\n// Launch copy kernel to perform the copy\ncopy<Iterator><<< grid, block >>>(\n        dst_params,\n        dst_tensor.device_data(),\n        src_params,\n        src_tensor.device_data(),\n        copy_extent\n);\n\n// copy function\ntemplate <typename Iterator>\n__global__ void copy(\n    typename Iterator::Params dst_params,\n    typename Iterator::Element *dst_pointer,\n    typename Iterator::Params src_params,\n    typename Iterator::Element *src_pointer,\n    cutlass::Coord<2> extent) {\n\n    Iterator dst_iterator(dst_params, dst_pointer, extent, threadIdx.x);\n    Iterator src_iterator(src_params, src_pointer, extent, threadIdx.x);\n\n    // PredicatedTileIterator uses PitchLinear layout and therefore takes in a PitchLinearShape.\n    // The contiguous dimension can be accessed via Iterator::Shape::kContiguous and the strided\n    // dimension can be accessed via Iterator::Shape::kStrided\n    int iterations = (extent[1] + Iterator::Shape::kStrided - 1) / Iterator::Shape::kStrided;\n\n    typename Iterator::Fragment fragment;\n\n    for(; iterations > 0; --iterations) {\n      src_iterator.load(fragment);\n      dst_iterator.store(fragment);\n\n      ++src_iterator;\n      ++dst_iterator;\n    }\n}\n```\n\n\n\n\n\n","source":"_posts/learn-cutlass-0.md","raw":"---\ntitle: learn-cutlass-0\ndate: 2023-03-20 20:44:12\ncategories: \n- Technology\ntags:\n- cutlass\n---\n\n> learn cutlass is a series of tutorials to learn cutlass by reading its examples or source code\n\nCUTLASS is a header-only template library. After reading that, you will **be lost in templates**.\n<!-- more -->\n\n## 00_basic_gemm\n\n```c++\n// Defines cutlass::gemm::device::Gemm, the generic Gemm computation template class.\n#include \"cutlass/gemm/device/gemm.h\"\n\nusing CutlassGemm = cutlass::gemm::device::Gemm<A_TYPE,A_LAYOUT,B_TYPE,B_LAYOUT,C_TYPE,C_LAYOUT> ;\n// where A_TYPE is Data-type of A matrix and A_LAYOUT is Layout of A matrix\n\nCutlassGemm gemm_operator;\n\nCutlassGemm::Arguments args({M , N, K},  \n                            {A_POINTER, lda},    \n                            {B_POINTER, ldb},   \n                            {C_POINTER, ldc},    \n                            {C_POINTER, ldc},    \n                            {alpha, beta});\n// where A_POINTER is pointer of A matrix and lda is the number of elements between consecutive rows or colmns\n\ncutlass::Status status = gemm_operator(args);\n// call gemm operation\n\n```\n\n## 01_cutlass_utilities\n\n```c++\n// CUTLASS includes needed for half-precision GEMM kernel\n#include \"cutlass/cutlass.h\"\n#include \"cutlass/core_io.h\"\n#include \"cutlass/layout/matrix.h\"\n#include \"cutlass/gemm/device/gemm.h\"\n\n//\n// CUTLASS utility includes\n//\n\n// Defines operator<<() to write TensorView objects to std::ostream\n#include \"cutlass/util/tensor_view_io.h\"\n\n// Defines cutlass::HostTensor<>\n#include \"cutlass/util/host_tensor.h\"\n\n// Defines cutlass::half_t\n#include \"cutlass/numeric_types.h\"\n\n// Defines device_memory::copy_device_to_device()\n#include \"cutlass/util/device_memory.h\"\n\n// Defines cutlass::reference::device::TensorFillRandomGaussian()\n#include \"cutlass/util/reference/device/tensor_fill.h\"\n\n// Defines cutlass::reference::host::TensorEquals()\n#include \"cutlass/util/reference/host/tensor_compare.h\"\n\n// Defines cutlass::reference::host::Gemm()\n#include \"cutlass/util/reference/host/gemm.h\"\n\n\n// another way to call gemm without using Arguments\ncutlass::Status status = gemm_op({\n    {M, N, K},\n    {A, lda},\n    {B, ldb},\n    {C, ldc},\n    {C, ldc},\n    {alpha, beta}\n  });\n\n// define a tensor (M,N) in cutlass, where DTYPE is data type\ncutlass::HostTensor<DTYPE,LAYOUT> VAR(cutlass::MatrixCoord(M,N)) ;\ncutlass::HostTensor<cutlass::half_t, cutlass::layout::ColumnMajor> A(cutlass::MatrixCoord(M, K));\n\n// fill a tensor (RandomGaussian) where A.device_view() return TensorView of that tensor in cutlass\ncutlass::reference::device::TensorFillRandomGaussian(\n    A.device_view(),\n    seed,\n    mean,\n    stddev,\n    bits_less_than_one\n  );\n\n// copy data from device to device in cutlass where A.device_data() return pointer of that tensor\n// A.capacity() return the logical capacity based on extent and layout. May differ from size().\ncutlass::device_memory::copy_device_to_device(\n    C_reference.device_data(), \n    C_cutlass.device_data(), \n    C_cutlass.capacity());\n\n// Copies data from device to host\nA.sync_host();\n\n// Copies data from host to device\nA.sync_device();\n\n// Compute the reference result using the host-side GEMM reference implementation.\n// I think the only difference between TensorView and TensorRef is that TensorView is read-only \n// while TensorRef can return pointer of matrix\ncutlass::reference::host::Gemm<\n    cutlass::half_t,                           // ElementA\n    cutlass::layout::ColumnMajor,              // LayoutA\n    cutlass::half_t,                           // ElementB\n    cutlass::layout::ColumnMajor,              // LayoutB\n    cutlass::half_t,                           // ElementOutput\n    cutlass::layout::ColumnMajor,              // LayoutOutput\n    cutlass::half_t,                           // ScalarType\n    cutlass::half_t                            // ComputeType\n> gemm_ref;\n\ngemm_ref(\n    {M, N, K},                          // problem size (type: cutlass::gemm::GemmCoord)\n    alpha,                              // alpha        (type: cutlass::half_t)\n    A.host_ref(),                       // A            (type: TensorRef<half_t, ColumnMajor>)\n    B.host_ref(),                       // B            (type: TensorRef<half_t, ColumnMajor>)\n    beta,                               // beta         (type: cutlass::half_t)\n    C_reference.host_ref()              // C            (type: TensorRef<half_t, ColumnMajor>)\n);\n\n// Compare reference to computed results\ncutlass::reference::host::TensorEquals(\n    C_reference.host_view(), \n    C_cutlass.host_view());\n```\n\n## 04_tile_iterator\n\n```c++\n// CUTLASS includes\n#include \"cutlass/transform/threadblock/predicated_tile_iterator.h\"\n#include \"cutlass/layout/pitch_linear.h\"\n#include \"cutlass/transform/pitch_linear_thread_map.h\"\n\n//\n//  CUTLASS utility includes\n//\n\n// Defines operator<<() to write TensorView objects to std::ostream\n#include \"cutlass/util/tensor_view_io.h\"\n\n// Defines cutlass::HostTensor<>\n#include \"cutlass/util/host_tensor.h\"\n\n// Defines cutlass::reference::host::TensorFill() and\n// cutlass::reference::host::TensorFillBlockSequential()\n#include \"cutlass/util/reference/host/tensor_fill.h\"\n\n\n// For this example, we chose a <64, 4> tile shape. The PredicateTileIterator expects\n// PitchLinearShape and PitchLinear layout.\nusing Shape = cutlass::layout::PitchLinearShape<64, 4>;\nusing Layout = cutlass::layout::PitchLinear;\nusing Element = int;\nint const kThreads = 32;\n\n// ThreadMaps define how threads are mapped to a given tile. The PitchLinearStripminedThreadMap\n// stripmines a pitch-linear tile among a given number of threads, first along the contiguous\n// dimension then along the strided dimension.\nusing ThreadMap = cutlass::transform::PitchLinearStripminedThreadMap<Shape, kThreads>;\n\n// Define the PredicateTileIterator, using TileShape, Element, Layout, and ThreadMap types\nusing Iterator = cutlass::transform::threadblock::PredicatedTileIterator<\n    Shape, Element, Layout, 1, ThreadMap>;\n\n\ncutlass::Coord<2> copy_extent = cutlass::make_Coord(M, K);\ncutlass::Coord<2> alloc_extent = cutlass::make_Coord(M, K);\n\n// another way to define tensor\n// Allocate source and destination tensors\ncutlass::HostTensor<Element, Layout> src_tensor(alloc_extent);\ncutlass::HostTensor<Element, Layout> dst_tensor(alloc_extent);\n\nElement oob_value = Element(-1);\n\n// Initialize destination tensor with all -1s\ncutlass::reference::host::TensorFill(dst_tensor.host_view(), oob_value);\n// Initialize source tensor with sequentially increasing values\ncutlass::reference::host::BlockFillSequential(src_tensor.host_data(), src_tensor.capacity());\n\ndst_tensor.sync_device();\nsrc_tensor.sync_device();\n\ntypename Iterator::Params dst_params(dst_tensor.layout());\ntypename Iterator::Params src_params(src_tensor.layout());\n\ndim3 block(kThreads, 1);\ndim3 grid(1, 1);\n\n// Launch copy kernel to perform the copy\ncopy<Iterator><<< grid, block >>>(\n        dst_params,\n        dst_tensor.device_data(),\n        src_params,\n        src_tensor.device_data(),\n        copy_extent\n);\n\n// copy function\ntemplate <typename Iterator>\n__global__ void copy(\n    typename Iterator::Params dst_params,\n    typename Iterator::Element *dst_pointer,\n    typename Iterator::Params src_params,\n    typename Iterator::Element *src_pointer,\n    cutlass::Coord<2> extent) {\n\n    Iterator dst_iterator(dst_params, dst_pointer, extent, threadIdx.x);\n    Iterator src_iterator(src_params, src_pointer, extent, threadIdx.x);\n\n    // PredicatedTileIterator uses PitchLinear layout and therefore takes in a PitchLinearShape.\n    // The contiguous dimension can be accessed via Iterator::Shape::kContiguous and the strided\n    // dimension can be accessed via Iterator::Shape::kStrided\n    int iterations = (extent[1] + Iterator::Shape::kStrided - 1) / Iterator::Shape::kStrided;\n\n    typename Iterator::Fragment fragment;\n\n    for(; iterations > 0; --iterations) {\n      src_iterator.load(fragment);\n      dst_iterator.store(fragment);\n\n      ++src_iterator;\n      ++dst_iterator;\n    }\n}\n```\n\n\n\n\n\n","slug":"learn-cutlass-0","published":1,"updated":"2023-03-20T14:19:34.098Z","_id":"clfgwqypg0000bp7v31z0bdhk","comments":1,"layout":"post","photos":[],"link":"","content":"<blockquote>\n<p>learn cutlass is a series of tutorials to learn cutlass by reading its examples or source code</p>\n</blockquote>\n<p>CUTLASS is a header-only template library. After reading that, you will <strong>be lost in templates</strong>.</p>\n<span id=\"more\"></span>\n\n<h2 id=\"00-basic-gemm\"><a href=\"#00-basic-gemm\" class=\"headerlink\" title=\"00_basic_gemm\"></a>00_basic_gemm</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Defines cutlass::gemm::device::Gemm, the generic Gemm computation template class.</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/gemm/device/gemm.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> CutlassGemm = cutlass::gemm::device::Gemm&lt;A_TYPE,A_LAYOUT,B_TYPE,B_LAYOUT,C_TYPE,C_LAYOUT&gt; ;</span><br><span class=\"line\"><span class=\"comment\">// where A_TYPE is Data-type of A matrix and A_LAYOUT is Layout of A matrix</span></span><br><span class=\"line\"></span><br><span class=\"line\">CutlassGemm gemm_operator;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\">CutlassGemm::Arguments <span class=\"title\">args</span><span class=\"params\">(&#123;M , N, K&#125;,  </span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">                            &#123;A_POINTER, lda&#125;,    </span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">                            &#123;B_POINTER, ldb&#125;,   </span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">                            &#123;C_POINTER, ldc&#125;,    </span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">                            &#123;C_POINTER, ldc&#125;,    </span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">                            &#123;alpha, beta&#125;)</span></span>;</span><br><span class=\"line\"><span class=\"comment\">// where A_POINTER is pointer of A matrix and lda is the number of elements between consecutive rows or colmns</span></span><br><span class=\"line\"></span><br><span class=\"line\">cutlass::Status status = <span class=\"built_in\">gemm_operator</span>(args);</span><br><span class=\"line\"><span class=\"comment\">// call gemm operation</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"01-cutlass-utilities\"><a href=\"#01-cutlass-utilities\" class=\"headerlink\" title=\"01_cutlass_utilities\"></a>01_cutlass_utilities</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// CUTLASS includes needed for half-precision GEMM kernel</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/cutlass.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/core_io.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/layout/matrix.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/gemm/device/gemm.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"><span class=\"comment\">// CUTLASS utility includes</span></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines operator&lt;&lt;() to write TensorView objects to std::ostream</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/tensor_view_io.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::HostTensor&lt;&gt;</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/host_tensor.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::half_t</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/numeric_types.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines device_memory::copy_device_to_device()</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/device_memory.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::reference::device::TensorFillRandomGaussian()</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/reference/device/tensor_fill.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::reference::host::TensorEquals()</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/reference/host/tensor_compare.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::reference::host::Gemm()</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/reference/host/gemm.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// another way to call gemm without using Arguments</span></span><br><span class=\"line\">cutlass::Status status = <span class=\"built_in\">gemm_op</span>(&#123;</span><br><span class=\"line\">    &#123;M, N, K&#125;,</span><br><span class=\"line\">    &#123;A, lda&#125;,</span><br><span class=\"line\">    &#123;B, ldb&#125;,</span><br><span class=\"line\">    &#123;C, ldc&#125;,</span><br><span class=\"line\">    &#123;C, ldc&#125;,</span><br><span class=\"line\">    &#123;alpha, beta&#125;</span><br><span class=\"line\">  &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// define a tensor (M,N) in cutlass, where DTYPE is data type</span></span><br><span class=\"line\"><span class=\"function\">cutlass::HostTensor&lt;DTYPE,LAYOUT&gt; <span class=\"title\">VAR</span><span class=\"params\">(cutlass::MatrixCoord(M,N))</span> </span>;</span><br><span class=\"line\"><span class=\"function\">cutlass::HostTensor&lt;cutlass::<span class=\"type\">half_t</span>, cutlass::layout::ColumnMajor&gt; <span class=\"title\">A</span><span class=\"params\">(cutlass::MatrixCoord(M, K))</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// fill a tensor (RandomGaussian) where A.device_view() return TensorView of that tensor in cutlass</span></span><br><span class=\"line\">cutlass::reference::device::<span class=\"built_in\">TensorFillRandomGaussian</span>(</span><br><span class=\"line\">    A.<span class=\"built_in\">device_view</span>(),</span><br><span class=\"line\">    seed,</span><br><span class=\"line\">    mean,</span><br><span class=\"line\">    stddev,</span><br><span class=\"line\">    bits_less_than_one</span><br><span class=\"line\">  );</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// copy data from device to device in cutlass where A.device_data() return pointer of that tensor</span></span><br><span class=\"line\"><span class=\"comment\">// A.capacity() return the logical capacity based on extent and layout. May differ from size().</span></span><br><span class=\"line\">cutlass::device_memory::<span class=\"built_in\">copy_device_to_device</span>(</span><br><span class=\"line\">    C_reference.<span class=\"built_in\">device_data</span>(), </span><br><span class=\"line\">    C_cutlass.<span class=\"built_in\">device_data</span>(), </span><br><span class=\"line\">    C_cutlass.<span class=\"built_in\">capacity</span>());</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Copies data from device to host</span></span><br><span class=\"line\">A.<span class=\"built_in\">sync_host</span>();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Copies data from host to device</span></span><br><span class=\"line\">A.<span class=\"built_in\">sync_device</span>();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Compute the reference result using the host-side GEMM reference implementation.</span></span><br><span class=\"line\"><span class=\"comment\">// I think the only difference between TensorView and TensorRef is that TensorView is read-only </span></span><br><span class=\"line\"><span class=\"comment\">// while TensorRef can return pointer of matrix</span></span><br><span class=\"line\">cutlass::reference::host::Gemm&lt;</span><br><span class=\"line\">    cutlass::<span class=\"type\">half_t</span>,                           <span class=\"comment\">// ElementA</span></span><br><span class=\"line\">    cutlass::layout::ColumnMajor,              <span class=\"comment\">// LayoutA</span></span><br><span class=\"line\">    cutlass::<span class=\"type\">half_t</span>,                           <span class=\"comment\">// ElementB</span></span><br><span class=\"line\">    cutlass::layout::ColumnMajor,              <span class=\"comment\">// LayoutB</span></span><br><span class=\"line\">    cutlass::<span class=\"type\">half_t</span>,                           <span class=\"comment\">// ElementOutput</span></span><br><span class=\"line\">    cutlass::layout::ColumnMajor,              <span class=\"comment\">// LayoutOutput</span></span><br><span class=\"line\">    cutlass::<span class=\"type\">half_t</span>,                           <span class=\"comment\">// ScalarType</span></span><br><span class=\"line\">    cutlass::<span class=\"type\">half_t</span>                            <span class=\"comment\">// ComputeType</span></span><br><span class=\"line\">&gt; gemm_ref;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">gemm_ref</span>(</span><br><span class=\"line\">    &#123;M, N, K&#125;,                          <span class=\"comment\">// problem size (type: cutlass::gemm::GemmCoord)</span></span><br><span class=\"line\">    alpha,                              <span class=\"comment\">// alpha        (type: cutlass::half_t)</span></span><br><span class=\"line\">    A.<span class=\"built_in\">host_ref</span>(),                       <span class=\"comment\">// A            (type: TensorRef&lt;half_t, ColumnMajor&gt;)</span></span><br><span class=\"line\">    B.<span class=\"built_in\">host_ref</span>(),                       <span class=\"comment\">// B            (type: TensorRef&lt;half_t, ColumnMajor&gt;)</span></span><br><span class=\"line\">    beta,                               <span class=\"comment\">// beta         (type: cutlass::half_t)</span></span><br><span class=\"line\">    C_reference.<span class=\"built_in\">host_ref</span>()              <span class=\"comment\">// C            (type: TensorRef&lt;half_t, ColumnMajor&gt;)</span></span><br><span class=\"line\">);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Compare reference to computed results</span></span><br><span class=\"line\">cutlass::reference::host::<span class=\"built_in\">TensorEquals</span>(</span><br><span class=\"line\">    C_reference.<span class=\"built_in\">host_view</span>(), </span><br><span class=\"line\">    C_cutlass.<span class=\"built_in\">host_view</span>());</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"04-tile-iterator\"><a href=\"#04-tile-iterator\" class=\"headerlink\" title=\"04_tile_iterator\"></a>04_tile_iterator</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// CUTLASS includes</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/transform/threadblock/predicated_tile_iterator.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/layout/pitch_linear.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/transform/pitch_linear_thread_map.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"><span class=\"comment\">//  CUTLASS utility includes</span></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines operator&lt;&lt;() to write TensorView objects to std::ostream</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/tensor_view_io.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::HostTensor&lt;&gt;</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/host_tensor.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::reference::host::TensorFill() and</span></span><br><span class=\"line\"><span class=\"comment\">// cutlass::reference::host::TensorFillBlockSequential()</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/reference/host/tensor_fill.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// For this example, we chose a &lt;64, 4&gt; tile shape. The PredicateTileIterator expects</span></span><br><span class=\"line\"><span class=\"comment\">// PitchLinearShape and PitchLinear layout.</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> Shape = cutlass::layout::PitchLinearShape&lt;<span class=\"number\">64</span>, <span class=\"number\">4</span>&gt;;</span><br><span class=\"line\"><span class=\"keyword\">using</span> Layout = cutlass::layout::PitchLinear;</span><br><span class=\"line\"><span class=\"keyword\">using</span> Element = <span class=\"type\">int</span>;</span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"type\">const</span> kThreads = <span class=\"number\">32</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// ThreadMaps define how threads are mapped to a given tile. The PitchLinearStripminedThreadMap</span></span><br><span class=\"line\"><span class=\"comment\">// stripmines a pitch-linear tile among a given number of threads, first along the contiguous</span></span><br><span class=\"line\"><span class=\"comment\">// dimension then along the strided dimension.</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> ThreadMap = cutlass::transform::PitchLinearStripminedThreadMap&lt;Shape, kThreads&gt;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Define the PredicateTileIterator, using TileShape, Element, Layout, and ThreadMap types</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> Iterator = cutlass::transform::threadblock::PredicatedTileIterator&lt;</span><br><span class=\"line\">    Shape, Element, Layout, <span class=\"number\">1</span>, ThreadMap&gt;;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">cutlass::Coord&lt;<span class=\"number\">2</span>&gt; copy_extent = cutlass::<span class=\"built_in\">make_Coord</span>(M, K);</span><br><span class=\"line\">cutlass::Coord&lt;<span class=\"number\">2</span>&gt; alloc_extent = cutlass::<span class=\"built_in\">make_Coord</span>(M, K);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// another way to define tensor</span></span><br><span class=\"line\"><span class=\"comment\">// Allocate source and destination tensors</span></span><br><span class=\"line\"><span class=\"function\">cutlass::HostTensor&lt;Element, Layout&gt; <span class=\"title\">src_tensor</span><span class=\"params\">(alloc_extent)</span></span>;</span><br><span class=\"line\"><span class=\"function\">cutlass::HostTensor&lt;Element, Layout&gt; <span class=\"title\">dst_tensor</span><span class=\"params\">(alloc_extent)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">Element oob_value = <span class=\"built_in\">Element</span>(<span class=\"number\">-1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Initialize destination tensor with all -1s</span></span><br><span class=\"line\">cutlass::reference::host::<span class=\"built_in\">TensorFill</span>(dst_tensor.<span class=\"built_in\">host_view</span>(), oob_value);</span><br><span class=\"line\"><span class=\"comment\">// Initialize source tensor with sequentially increasing values</span></span><br><span class=\"line\">cutlass::reference::host::<span class=\"built_in\">BlockFillSequential</span>(src_tensor.<span class=\"built_in\">host_data</span>(), src_tensor.<span class=\"built_in\">capacity</span>());</span><br><span class=\"line\"></span><br><span class=\"line\">dst_tensor.<span class=\"built_in\">sync_device</span>();</span><br><span class=\"line\">src_tensor.<span class=\"built_in\">sync_device</span>();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">typename</span> Iterator::Params <span class=\"title\">dst_params</span><span class=\"params\">(dst_tensor.layout())</span></span>;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">typename</span> Iterator::Params <span class=\"title\">src_params</span><span class=\"params\">(src_tensor.layout())</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\">dim3 <span class=\"title\">block</span><span class=\"params\">(kThreads, <span class=\"number\">1</span>)</span></span>;</span><br><span class=\"line\"><span class=\"function\">dim3 <span class=\"title\">grid</span><span class=\"params\">(<span class=\"number\">1</span>, <span class=\"number\">1</span>)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Launch copy kernel to perform the copy</span></span><br><span class=\"line\">copy&lt;Iterator&gt;&lt;&lt;&lt; grid, block &gt;&gt;&gt;(</span><br><span class=\"line\">        dst_params,</span><br><span class=\"line\">        dst_tensor.<span class=\"built_in\">device_data</span>(),</span><br><span class=\"line\">        src_params,</span><br><span class=\"line\">        src_tensor.<span class=\"built_in\">device_data</span>(),</span><br><span class=\"line\">        copy_extent</span><br><span class=\"line\">);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// copy function</span></span><br><span class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Iterator&gt;</span><br><span class=\"line\"><span class=\"function\">__global__ <span class=\"type\">void</span> <span class=\"title\">copy</span><span class=\"params\">(</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">    <span class=\"keyword\">typename</span> Iterator::Params dst_params,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">    <span class=\"keyword\">typename</span> Iterator::Element *dst_pointer,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">    <span class=\"keyword\">typename</span> Iterator::Params src_params,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">    <span class=\"keyword\">typename</span> Iterator::Element *src_pointer,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">    cutlass::Coord&lt;<span class=\"number\">2</span>&gt; extent)</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\">Iterator <span class=\"title\">dst_iterator</span><span class=\"params\">(dst_params, dst_pointer, extent, threadIdx.x)</span></span>;</span><br><span class=\"line\">    <span class=\"function\">Iterator <span class=\"title\">src_iterator</span><span class=\"params\">(src_params, src_pointer, extent, threadIdx.x)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// PredicatedTileIterator uses PitchLinear layout and therefore takes in a PitchLinearShape.</span></span><br><span class=\"line\">    <span class=\"comment\">// The contiguous dimension can be accessed via Iterator::Shape::kContiguous and the strided</span></span><br><span class=\"line\">    <span class=\"comment\">// dimension can be accessed via Iterator::Shape::kStrided</span></span><br><span class=\"line\">    <span class=\"type\">int</span> iterations = (extent[<span class=\"number\">1</span>] + Iterator::Shape::kStrided - <span class=\"number\">1</span>) / Iterator::Shape::kStrided;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">typename</span> Iterator::Fragment fragment;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(; iterations &gt; <span class=\"number\">0</span>; --iterations) &#123;</span><br><span class=\"line\">      src_iterator.<span class=\"built_in\">load</span>(fragment);</span><br><span class=\"line\">      dst_iterator.<span class=\"built_in\">store</span>(fragment);</span><br><span class=\"line\"></span><br><span class=\"line\">      ++src_iterator;</span><br><span class=\"line\">      ++dst_iterator;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n\n\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>learn cutlass is a series of tutorials to learn cutlass by reading its examples or source code</p>\n</blockquote>\n<p>CUTLASS is a header-only template library. After reading that, you will <strong>be lost in templates</strong>.</p>","more":"<h2 id=\"00-basic-gemm\"><a href=\"#00-basic-gemm\" class=\"headerlink\" title=\"00_basic_gemm\"></a>00_basic_gemm</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Defines cutlass::gemm::device::Gemm, the generic Gemm computation template class.</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/gemm/device/gemm.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> CutlassGemm = cutlass::gemm::device::Gemm&lt;A_TYPE,A_LAYOUT,B_TYPE,B_LAYOUT,C_TYPE,C_LAYOUT&gt; ;</span><br><span class=\"line\"><span class=\"comment\">// where A_TYPE is Data-type of A matrix and A_LAYOUT is Layout of A matrix</span></span><br><span class=\"line\"></span><br><span class=\"line\">CutlassGemm gemm_operator;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\">CutlassGemm::Arguments <span class=\"title\">args</span><span class=\"params\">(&#123;M , N, K&#125;,  </span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">                            &#123;A_POINTER, lda&#125;,    </span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">                            &#123;B_POINTER, ldb&#125;,   </span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">                            &#123;C_POINTER, ldc&#125;,    </span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">                            &#123;C_POINTER, ldc&#125;,    </span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">                            &#123;alpha, beta&#125;)</span></span>;</span><br><span class=\"line\"><span class=\"comment\">// where A_POINTER is pointer of A matrix and lda is the number of elements between consecutive rows or colmns</span></span><br><span class=\"line\"></span><br><span class=\"line\">cutlass::Status status = <span class=\"built_in\">gemm_operator</span>(args);</span><br><span class=\"line\"><span class=\"comment\">// call gemm operation</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"01-cutlass-utilities\"><a href=\"#01-cutlass-utilities\" class=\"headerlink\" title=\"01_cutlass_utilities\"></a>01_cutlass_utilities</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// CUTLASS includes needed for half-precision GEMM kernel</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/cutlass.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/core_io.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/layout/matrix.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/gemm/device/gemm.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"><span class=\"comment\">// CUTLASS utility includes</span></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines operator&lt;&lt;() to write TensorView objects to std::ostream</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/tensor_view_io.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::HostTensor&lt;&gt;</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/host_tensor.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::half_t</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/numeric_types.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines device_memory::copy_device_to_device()</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/device_memory.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::reference::device::TensorFillRandomGaussian()</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/reference/device/tensor_fill.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::reference::host::TensorEquals()</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/reference/host/tensor_compare.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::reference::host::Gemm()</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/reference/host/gemm.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// another way to call gemm without using Arguments</span></span><br><span class=\"line\">cutlass::Status status = <span class=\"built_in\">gemm_op</span>(&#123;</span><br><span class=\"line\">    &#123;M, N, K&#125;,</span><br><span class=\"line\">    &#123;A, lda&#125;,</span><br><span class=\"line\">    &#123;B, ldb&#125;,</span><br><span class=\"line\">    &#123;C, ldc&#125;,</span><br><span class=\"line\">    &#123;C, ldc&#125;,</span><br><span class=\"line\">    &#123;alpha, beta&#125;</span><br><span class=\"line\">  &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// define a tensor (M,N) in cutlass, where DTYPE is data type</span></span><br><span class=\"line\"><span class=\"function\">cutlass::HostTensor&lt;DTYPE,LAYOUT&gt; <span class=\"title\">VAR</span><span class=\"params\">(cutlass::MatrixCoord(M,N))</span> </span>;</span><br><span class=\"line\"><span class=\"function\">cutlass::HostTensor&lt;cutlass::<span class=\"type\">half_t</span>, cutlass::layout::ColumnMajor&gt; <span class=\"title\">A</span><span class=\"params\">(cutlass::MatrixCoord(M, K))</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// fill a tensor (RandomGaussian) where A.device_view() return TensorView of that tensor in cutlass</span></span><br><span class=\"line\">cutlass::reference::device::<span class=\"built_in\">TensorFillRandomGaussian</span>(</span><br><span class=\"line\">    A.<span class=\"built_in\">device_view</span>(),</span><br><span class=\"line\">    seed,</span><br><span class=\"line\">    mean,</span><br><span class=\"line\">    stddev,</span><br><span class=\"line\">    bits_less_than_one</span><br><span class=\"line\">  );</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// copy data from device to device in cutlass where A.device_data() return pointer of that tensor</span></span><br><span class=\"line\"><span class=\"comment\">// A.capacity() return the logical capacity based on extent and layout. May differ from size().</span></span><br><span class=\"line\">cutlass::device_memory::<span class=\"built_in\">copy_device_to_device</span>(</span><br><span class=\"line\">    C_reference.<span class=\"built_in\">device_data</span>(), </span><br><span class=\"line\">    C_cutlass.<span class=\"built_in\">device_data</span>(), </span><br><span class=\"line\">    C_cutlass.<span class=\"built_in\">capacity</span>());</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Copies data from device to host</span></span><br><span class=\"line\">A.<span class=\"built_in\">sync_host</span>();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Copies data from host to device</span></span><br><span class=\"line\">A.<span class=\"built_in\">sync_device</span>();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Compute the reference result using the host-side GEMM reference implementation.</span></span><br><span class=\"line\"><span class=\"comment\">// I think the only difference between TensorView and TensorRef is that TensorView is read-only </span></span><br><span class=\"line\"><span class=\"comment\">// while TensorRef can return pointer of matrix</span></span><br><span class=\"line\">cutlass::reference::host::Gemm&lt;</span><br><span class=\"line\">    cutlass::<span class=\"type\">half_t</span>,                           <span class=\"comment\">// ElementA</span></span><br><span class=\"line\">    cutlass::layout::ColumnMajor,              <span class=\"comment\">// LayoutA</span></span><br><span class=\"line\">    cutlass::<span class=\"type\">half_t</span>,                           <span class=\"comment\">// ElementB</span></span><br><span class=\"line\">    cutlass::layout::ColumnMajor,              <span class=\"comment\">// LayoutB</span></span><br><span class=\"line\">    cutlass::<span class=\"type\">half_t</span>,                           <span class=\"comment\">// ElementOutput</span></span><br><span class=\"line\">    cutlass::layout::ColumnMajor,              <span class=\"comment\">// LayoutOutput</span></span><br><span class=\"line\">    cutlass::<span class=\"type\">half_t</span>,                           <span class=\"comment\">// ScalarType</span></span><br><span class=\"line\">    cutlass::<span class=\"type\">half_t</span>                            <span class=\"comment\">// ComputeType</span></span><br><span class=\"line\">&gt; gemm_ref;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">gemm_ref</span>(</span><br><span class=\"line\">    &#123;M, N, K&#125;,                          <span class=\"comment\">// problem size (type: cutlass::gemm::GemmCoord)</span></span><br><span class=\"line\">    alpha,                              <span class=\"comment\">// alpha        (type: cutlass::half_t)</span></span><br><span class=\"line\">    A.<span class=\"built_in\">host_ref</span>(),                       <span class=\"comment\">// A            (type: TensorRef&lt;half_t, ColumnMajor&gt;)</span></span><br><span class=\"line\">    B.<span class=\"built_in\">host_ref</span>(),                       <span class=\"comment\">// B            (type: TensorRef&lt;half_t, ColumnMajor&gt;)</span></span><br><span class=\"line\">    beta,                               <span class=\"comment\">// beta         (type: cutlass::half_t)</span></span><br><span class=\"line\">    C_reference.<span class=\"built_in\">host_ref</span>()              <span class=\"comment\">// C            (type: TensorRef&lt;half_t, ColumnMajor&gt;)</span></span><br><span class=\"line\">);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Compare reference to computed results</span></span><br><span class=\"line\">cutlass::reference::host::<span class=\"built_in\">TensorEquals</span>(</span><br><span class=\"line\">    C_reference.<span class=\"built_in\">host_view</span>(), </span><br><span class=\"line\">    C_cutlass.<span class=\"built_in\">host_view</span>());</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"04-tile-iterator\"><a href=\"#04-tile-iterator\" class=\"headerlink\" title=\"04_tile_iterator\"></a>04_tile_iterator</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// CUTLASS includes</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/transform/threadblock/predicated_tile_iterator.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/layout/pitch_linear.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/transform/pitch_linear_thread_map.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"><span class=\"comment\">//  CUTLASS utility includes</span></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines operator&lt;&lt;() to write TensorView objects to std::ostream</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/tensor_view_io.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::HostTensor&lt;&gt;</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/host_tensor.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::reference::host::TensorFill() and</span></span><br><span class=\"line\"><span class=\"comment\">// cutlass::reference::host::TensorFillBlockSequential()</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/reference/host/tensor_fill.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// For this example, we chose a &lt;64, 4&gt; tile shape. The PredicateTileIterator expects</span></span><br><span class=\"line\"><span class=\"comment\">// PitchLinearShape and PitchLinear layout.</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> Shape = cutlass::layout::PitchLinearShape&lt;<span class=\"number\">64</span>, <span class=\"number\">4</span>&gt;;</span><br><span class=\"line\"><span class=\"keyword\">using</span> Layout = cutlass::layout::PitchLinear;</span><br><span class=\"line\"><span class=\"keyword\">using</span> Element = <span class=\"type\">int</span>;</span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"type\">const</span> kThreads = <span class=\"number\">32</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// ThreadMaps define how threads are mapped to a given tile. The PitchLinearStripminedThreadMap</span></span><br><span class=\"line\"><span class=\"comment\">// stripmines a pitch-linear tile among a given number of threads, first along the contiguous</span></span><br><span class=\"line\"><span class=\"comment\">// dimension then along the strided dimension.</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> ThreadMap = cutlass::transform::PitchLinearStripminedThreadMap&lt;Shape, kThreads&gt;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Define the PredicateTileIterator, using TileShape, Element, Layout, and ThreadMap types</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> Iterator = cutlass::transform::threadblock::PredicatedTileIterator&lt;</span><br><span class=\"line\">    Shape, Element, Layout, <span class=\"number\">1</span>, ThreadMap&gt;;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">cutlass::Coord&lt;<span class=\"number\">2</span>&gt; copy_extent = cutlass::<span class=\"built_in\">make_Coord</span>(M, K);</span><br><span class=\"line\">cutlass::Coord&lt;<span class=\"number\">2</span>&gt; alloc_extent = cutlass::<span class=\"built_in\">make_Coord</span>(M, K);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// another way to define tensor</span></span><br><span class=\"line\"><span class=\"comment\">// Allocate source and destination tensors</span></span><br><span class=\"line\"><span class=\"function\">cutlass::HostTensor&lt;Element, Layout&gt; <span class=\"title\">src_tensor</span><span class=\"params\">(alloc_extent)</span></span>;</span><br><span class=\"line\"><span class=\"function\">cutlass::HostTensor&lt;Element, Layout&gt; <span class=\"title\">dst_tensor</span><span class=\"params\">(alloc_extent)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">Element oob_value = <span class=\"built_in\">Element</span>(<span class=\"number\">-1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Initialize destination tensor with all -1s</span></span><br><span class=\"line\">cutlass::reference::host::<span class=\"built_in\">TensorFill</span>(dst_tensor.<span class=\"built_in\">host_view</span>(), oob_value);</span><br><span class=\"line\"><span class=\"comment\">// Initialize source tensor with sequentially increasing values</span></span><br><span class=\"line\">cutlass::reference::host::<span class=\"built_in\">BlockFillSequential</span>(src_tensor.<span class=\"built_in\">host_data</span>(), src_tensor.<span class=\"built_in\">capacity</span>());</span><br><span class=\"line\"></span><br><span class=\"line\">dst_tensor.<span class=\"built_in\">sync_device</span>();</span><br><span class=\"line\">src_tensor.<span class=\"built_in\">sync_device</span>();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">typename</span> Iterator::Params <span class=\"title\">dst_params</span><span class=\"params\">(dst_tensor.layout())</span></span>;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">typename</span> Iterator::Params <span class=\"title\">src_params</span><span class=\"params\">(src_tensor.layout())</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\">dim3 <span class=\"title\">block</span><span class=\"params\">(kThreads, <span class=\"number\">1</span>)</span></span>;</span><br><span class=\"line\"><span class=\"function\">dim3 <span class=\"title\">grid</span><span class=\"params\">(<span class=\"number\">1</span>, <span class=\"number\">1</span>)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Launch copy kernel to perform the copy</span></span><br><span class=\"line\">copy&lt;Iterator&gt;&lt;&lt;&lt; grid, block &gt;&gt;&gt;(</span><br><span class=\"line\">        dst_params,</span><br><span class=\"line\">        dst_tensor.<span class=\"built_in\">device_data</span>(),</span><br><span class=\"line\">        src_params,</span><br><span class=\"line\">        src_tensor.<span class=\"built_in\">device_data</span>(),</span><br><span class=\"line\">        copy_extent</span><br><span class=\"line\">);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// copy function</span></span><br><span class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Iterator&gt;</span><br><span class=\"line\"><span class=\"function\">__global__ <span class=\"type\">void</span> <span class=\"title\">copy</span><span class=\"params\">(</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">    <span class=\"keyword\">typename</span> Iterator::Params dst_params,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">    <span class=\"keyword\">typename</span> Iterator::Element *dst_pointer,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">    <span class=\"keyword\">typename</span> Iterator::Params src_params,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">    <span class=\"keyword\">typename</span> Iterator::Element *src_pointer,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">    cutlass::Coord&lt;<span class=\"number\">2</span>&gt; extent)</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\">Iterator <span class=\"title\">dst_iterator</span><span class=\"params\">(dst_params, dst_pointer, extent, threadIdx.x)</span></span>;</span><br><span class=\"line\">    <span class=\"function\">Iterator <span class=\"title\">src_iterator</span><span class=\"params\">(src_params, src_pointer, extent, threadIdx.x)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// PredicatedTileIterator uses PitchLinear layout and therefore takes in a PitchLinearShape.</span></span><br><span class=\"line\">    <span class=\"comment\">// The contiguous dimension can be accessed via Iterator::Shape::kContiguous and the strided</span></span><br><span class=\"line\">    <span class=\"comment\">// dimension can be accessed via Iterator::Shape::kStrided</span></span><br><span class=\"line\">    <span class=\"type\">int</span> iterations = (extent[<span class=\"number\">1</span>] + Iterator::Shape::kStrided - <span class=\"number\">1</span>) / Iterator::Shape::kStrided;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">typename</span> Iterator::Fragment fragment;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(; iterations &gt; <span class=\"number\">0</span>; --iterations) &#123;</span><br><span class=\"line\">      src_iterator.<span class=\"built_in\">load</span>(fragment);</span><br><span class=\"line\">      dst_iterator.<span class=\"built_in\">store</span>(fragment);</span><br><span class=\"line\"></span><br><span class=\"line\">      ++src_iterator;</span><br><span class=\"line\">      ++dst_iterator;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"title":"learn-cutlass-1","date":"2023-03-21T01:00:19.000Z","_content":"\nIn cutlass 3.0, it introduces a new library, Cute, to describe and manipulate tensors of threads and data. I think the core of cutlass is GEMM(or other computations) and data movement.\n\n## RowMajorInterleaved (ColumnMajorInterleaved)\n\n```c++\n#include \"cutlass/layout/matrix.h\"\ntemplate<int Interleave> struct cutlass::layout::RowMajorInterleaved<Interleave>;\n```\n\nRowMajorInterleaved is a layout which confused me. I didn't know the meaning of Interleaved.So I create an example to figure it out.\n\n<!-- more -->\n\n```c++\n\n#include <iostream>\n#include <cstdio>\n\n// Defines cutlass::layout::RowMajorInterleave\n#include \"cutlass/layout/matrix.h\"\n\n// Defines cutlass::HostTensor<>\n#include \"cutlass/util/host_tensor.h\"\n\n// Defines cutlass::MatrixCoord\n#include \"cutlass/matrix_coord.h\"\n\n#define M 4\n#define N 4\n\nint main(){\n    cutlass::HostTensor<int,cutlass::layout::RowMajorInterleaved<2> > A(cutlass::MatrixCoord(M,N));\n    \n    int num = 0;\n    for(int i=0;i<M;i++)\n    for(int j=0;j<N;j++){\n        A.at({i,j}) = ++num; \n    }\n\n    int *A_ = A.host_data();\n    for(int i=0;i<A.capacity();i++){\n        printf(\"%3d \",A_[i]);\n        // if((i+1)%N==0)printf(\"\\n\");\n    }\n    /**\n     *  output:\n     *  1 5 2 6 3 7 4 8 9 13 10 14 11 15 12 16\n     *  \n    */\n}\n```\n\nIf tensor A is a simple RowMajor, the output should be this\n\n```\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\n```\nIn my opinion, `Interleaved` means it will iterate in shape(1) with size `Interleave` and then iterate in shape(0). \nOther things need to mind is `Interleaved` may cause padding of a matrix, like\n\n```c++\n#define M 3\n#define N 3\ncutlass::HostTensor<int,cutlass::layout::RowMajorInterleaved<2> > A(cutlass::MatrixCoord(M,N));\nint num = 0;\nfor(int i=0;i<M;i++)\nfor(int j=0;j<N;j++){\n    A.at({i,j}) = ++num; \n}\n/**\n * the element in A should be \n * 1 4 2 5 3 6 7 0 8 0 9 0\n```\n\n## 05_batched_gemm\n\nBatched gemm can be illustrated as follows\n![](/img/batched_gemm.jpg)\n\n","source":"_posts/learn-cutlass-1.md","raw":"---\ntitle: learn-cutlass-1\ndate: 2023-03-21 09:00:19\ncategories:\n- Technology\ntags:\n- cutlass\n---\n\nIn cutlass 3.0, it introduces a new library, Cute, to describe and manipulate tensors of threads and data. I think the core of cutlass is GEMM(or other computations) and data movement.\n\n## RowMajorInterleaved (ColumnMajorInterleaved)\n\n```c++\n#include \"cutlass/layout/matrix.h\"\ntemplate<int Interleave> struct cutlass::layout::RowMajorInterleaved<Interleave>;\n```\n\nRowMajorInterleaved is a layout which confused me. I didn't know the meaning of Interleaved.So I create an example to figure it out.\n\n<!-- more -->\n\n```c++\n\n#include <iostream>\n#include <cstdio>\n\n// Defines cutlass::layout::RowMajorInterleave\n#include \"cutlass/layout/matrix.h\"\n\n// Defines cutlass::HostTensor<>\n#include \"cutlass/util/host_tensor.h\"\n\n// Defines cutlass::MatrixCoord\n#include \"cutlass/matrix_coord.h\"\n\n#define M 4\n#define N 4\n\nint main(){\n    cutlass::HostTensor<int,cutlass::layout::RowMajorInterleaved<2> > A(cutlass::MatrixCoord(M,N));\n    \n    int num = 0;\n    for(int i=0;i<M;i++)\n    for(int j=0;j<N;j++){\n        A.at({i,j}) = ++num; \n    }\n\n    int *A_ = A.host_data();\n    for(int i=0;i<A.capacity();i++){\n        printf(\"%3d \",A_[i]);\n        // if((i+1)%N==0)printf(\"\\n\");\n    }\n    /**\n     *  output:\n     *  1 5 2 6 3 7 4 8 9 13 10 14 11 15 12 16\n     *  \n    */\n}\n```\n\nIf tensor A is a simple RowMajor, the output should be this\n\n```\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\n```\nIn my opinion, `Interleaved` means it will iterate in shape(1) with size `Interleave` and then iterate in shape(0). \nOther things need to mind is `Interleaved` may cause padding of a matrix, like\n\n```c++\n#define M 3\n#define N 3\ncutlass::HostTensor<int,cutlass::layout::RowMajorInterleaved<2> > A(cutlass::MatrixCoord(M,N));\nint num = 0;\nfor(int i=0;i<M;i++)\nfor(int j=0;j<N;j++){\n    A.at({i,j}) = ++num; \n}\n/**\n * the element in A should be \n * 1 4 2 5 3 6 7 0 8 0 9 0\n```\n\n## 05_batched_gemm\n\nBatched gemm can be illustrated as follows\n![](/img/batched_gemm.jpg)\n\n","slug":"learn-cutlass-1","published":1,"updated":"2023-03-21T01:55:14.637Z","_id":"clfhkj0ml0000k07vdorp3po1","comments":1,"layout":"post","photos":[],"link":"","content":"<p>In cutlass 3.0, it introduces a new library, Cute, to describe and manipulate tensors of threads and data. I think the core of cutlass is GEMM(or other computations) and data movement.</p>\n<h2 id=\"RowMajorInterleaved-ColumnMajorInterleaved\"><a href=\"#RowMajorInterleaved-ColumnMajorInterleaved\" class=\"headerlink\" title=\"RowMajorInterleaved (ColumnMajorInterleaved)\"></a>RowMajorInterleaved (ColumnMajorInterleaved)</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/layout/matrix.h&quot;</span></span></span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"type\">int</span> Interleave&gt; <span class=\"keyword\">struct</span> <span class=\"title class_\">cutlass</span>::layout::RowMajorInterleaved&lt;Interleave&gt;;</span><br></pre></td></tr></table></figure>\n\n<p>RowMajorInterleaved is a layout which confused me. I didn’t know the meaning of Interleaved.So I create an example to figure it out.</p>\n<span id=\"more\"></span>\n\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;cstdio&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::layout::RowMajorInterleave</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/layout/matrix.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::HostTensor&lt;&gt;</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/host_tensor.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::MatrixCoord</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/matrix_coord.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> M 4</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> N 4</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">    cutlass::HostTensor&lt;<span class=\"type\">int</span>,cutlass::layout::RowMajorInterleaved&lt;<span class=\"number\">2</span>&gt; &gt; <span class=\"built_in\">A</span>(cutlass::<span class=\"built_in\">MatrixCoord</span>(M,N));</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"type\">int</span> num = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;M;i++)</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> j=<span class=\"number\">0</span>;j&lt;N;j++)&#123;</span><br><span class=\"line\">        A.<span class=\"built_in\">at</span>(&#123;i,j&#125;) = ++num; </span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> *A_ = A.<span class=\"built_in\">host_data</span>();</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;A.<span class=\"built_in\">capacity</span>();i++)&#123;</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%3d &quot;</span>,A_[i]);</span><br><span class=\"line\">        <span class=\"comment\">// if((i+1)%N==0)printf(&quot;\\n&quot;);</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     *  output:</span></span><br><span class=\"line\"><span class=\"comment\">     *  1 5 2 6 3 7 4 8 9 13 10 14 11 15 12 16</span></span><br><span class=\"line\"><span class=\"comment\">     *  </span></span><br><span class=\"line\"><span class=\"comment\">    */</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>If tensor A is a simple RowMajor, the output should be this</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16</span><br></pre></td></tr></table></figure>\n<p>In my opinion, <code>Interleaved</code> means it will iterate in shape(1) with size <code>Interleave</code> and then iterate in shape(0).<br>Other things need to mind is <code>Interleaved</code> may cause padding of a matrix, like</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> M 3</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> N 3</span></span><br><span class=\"line\">cutlass::HostTensor&lt;<span class=\"type\">int</span>,cutlass::layout::RowMajorInterleaved&lt;<span class=\"number\">2</span>&gt; &gt; <span class=\"built_in\">A</span>(cutlass::<span class=\"built_in\">MatrixCoord</span>(M,N));</span><br><span class=\"line\"><span class=\"type\">int</span> num = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;M;i++)</span><br><span class=\"line\"><span class=\"keyword\">for</span>(<span class=\"type\">int</span> j=<span class=\"number\">0</span>;j&lt;N;j++)&#123;</span><br><span class=\"line\">    A.<span class=\"built_in\">at</span>(&#123;i,j&#125;) = ++num; </span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * the element in A should be </span></span><br><span class=\"line\"><span class=\"comment\"> * 1 4 2 5 3 6 7 0 8 0 9 0</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"05-batched-gemm\"><a href=\"#05-batched-gemm\" class=\"headerlink\" title=\"05_batched_gemm\"></a>05_batched_gemm</h2><p>Batched gemm can be illustrated as follows<br><img src=\"/img/batched_gemm.jpg\"></p>\n","site":{"data":{}},"excerpt":"<p>In cutlass 3.0, it introduces a new library, Cute, to describe and manipulate tensors of threads and data. I think the core of cutlass is GEMM(or other computations) and data movement.</p>\n<h2 id=\"RowMajorInterleaved-ColumnMajorInterleaved\"><a href=\"#RowMajorInterleaved-ColumnMajorInterleaved\" class=\"headerlink\" title=\"RowMajorInterleaved (ColumnMajorInterleaved)\"></a>RowMajorInterleaved (ColumnMajorInterleaved)</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/layout/matrix.h&quot;</span></span></span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"type\">int</span> Interleave&gt; <span class=\"keyword\">struct</span> <span class=\"title class_\">cutlass</span>::layout::RowMajorInterleaved&lt;Interleave&gt;;</span><br></pre></td></tr></table></figure>\n\n<p>RowMajorInterleaved is a layout which confused me. I didn’t know the meaning of Interleaved.So I create an example to figure it out.</p>","more":"<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;cstdio&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::layout::RowMajorInterleave</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/layout/matrix.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::HostTensor&lt;&gt;</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/host_tensor.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::MatrixCoord</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/matrix_coord.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> M 4</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> N 4</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">    cutlass::HostTensor&lt;<span class=\"type\">int</span>,cutlass::layout::RowMajorInterleaved&lt;<span class=\"number\">2</span>&gt; &gt; <span class=\"built_in\">A</span>(cutlass::<span class=\"built_in\">MatrixCoord</span>(M,N));</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"type\">int</span> num = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;M;i++)</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> j=<span class=\"number\">0</span>;j&lt;N;j++)&#123;</span><br><span class=\"line\">        A.<span class=\"built_in\">at</span>(&#123;i,j&#125;) = ++num; </span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> *A_ = A.<span class=\"built_in\">host_data</span>();</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;A.<span class=\"built_in\">capacity</span>();i++)&#123;</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%3d &quot;</span>,A_[i]);</span><br><span class=\"line\">        <span class=\"comment\">// if((i+1)%N==0)printf(&quot;\\n&quot;);</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     *  output:</span></span><br><span class=\"line\"><span class=\"comment\">     *  1 5 2 6 3 7 4 8 9 13 10 14 11 15 12 16</span></span><br><span class=\"line\"><span class=\"comment\">     *  </span></span><br><span class=\"line\"><span class=\"comment\">    */</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>If tensor A is a simple RowMajor, the output should be this</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16</span><br></pre></td></tr></table></figure>\n<p>In my opinion, <code>Interleaved</code> means it will iterate in shape(1) with size <code>Interleave</code> and then iterate in shape(0).<br>Other things need to mind is <code>Interleaved</code> may cause padding of a matrix, like</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> M 3</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> N 3</span></span><br><span class=\"line\">cutlass::HostTensor&lt;<span class=\"type\">int</span>,cutlass::layout::RowMajorInterleaved&lt;<span class=\"number\">2</span>&gt; &gt; <span class=\"built_in\">A</span>(cutlass::<span class=\"built_in\">MatrixCoord</span>(M,N));</span><br><span class=\"line\"><span class=\"type\">int</span> num = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;M;i++)</span><br><span class=\"line\"><span class=\"keyword\">for</span>(<span class=\"type\">int</span> j=<span class=\"number\">0</span>;j&lt;N;j++)&#123;</span><br><span class=\"line\">    A.<span class=\"built_in\">at</span>(&#123;i,j&#125;) = ++num; </span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * the element in A should be </span></span><br><span class=\"line\"><span class=\"comment\"> * 1 4 2 5 3 6 7 0 8 0 9 0</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"05-batched-gemm\"><a href=\"#05-batched-gemm\" class=\"headerlink\" title=\"05_batched_gemm\"></a>05_batched_gemm</h2><p>Batched gemm can be illustrated as follows<br><img src=\"/img/batched_gemm.jpg\"></p>"}],"PostAsset":[],"PostCategory":[{"post_id":"clfgp9yzh0000j17v2zo7eyfy","category_id":"clfgq3prx000038z3aixz3fox","_id":"clfgq3ps1000138z3cn1khtmf"},{"post_id":"clfgwqypg0000bp7v31z0bdhk","category_id":"clfgq3prx000038z3aixz3fox","_id":"clfgwqyph0002bp7vcwj39u4y"},{"post_id":"clfhkj0ml0000k07vdorp3po1","category_id":"clfgq3prx000038z3aixz3fox","_id":"clfhkj0mm0002k07vai8efi5c"}],"PostTag":[{"post_id":"clfgp9yzh0000j17v2zo7eyfy","tag_id":"clfgo5q8d0001bv7v0m25e6wv","_id":"clfgp9yzm0001j17ve5fjbkmc"},{"post_id":"clfgp9yzh0000j17v2zo7eyfy","tag_id":"clfgo5q8g0002bv7v6n02g28a","_id":"clfgp9yzm0002j17vdciu3d3p"},{"post_id":"clfgwqypg0000bp7v31z0bdhk","tag_id":"clfgwqyph0001bp7vgmue0dcf","_id":"clfgwqypi0003bp7ve40b53cz"},{"post_id":"clfhkj0ml0000k07vdorp3po1","tag_id":"clfgwqyph0001bp7vgmue0dcf","_id":"clfhkj0mm0001k07v7wu4frpb"}],"Tag":[{"name":"t","_id":"clfgmd42q00075r7veka6fxr8"},{"name":"te","_id":"clfgmd4gc00095r7v4ugmfv95"},{"name":"tes","_id":"clfgmd4qd000b5r7v4e2y3dsd"},{"name":"test","_id":"clfgmd4z0000d5r7vh1qw0xjb"},{"name":"test-","_id":"clfgmd601000f5r7v2eo1bui8"},{"name":"test-ta","_id":"clfgmd6ef000h5r7v0y4vdo3z"},{"name":"test-tag","_id":"clfgmd6qu000j5r7v090p7rba"},{"name":"Hexo","_id":"clfgo5q8d0001bv7v0m25e6wv"},{"name":"Icarus","_id":"clfgo5q8g0002bv7v6n02g28a"},{"name":"cutlass","_id":"clfgwqyph0001bp7vgmue0dcf"}]}}