{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[{"_id":"themes/icarus/source/css/cyberpunk.styl","path":"css/cyberpunk.styl","modified":0,"renderable":1},{"_id":"themes/icarus/source/css/default.styl","path":"css/default.styl","modified":0,"renderable":1},{"_id":"themes/icarus/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/icarus/source/img/avatar.png","path":"img/avatar.png","modified":0,"renderable":1},{"_id":"themes/icarus/source/img/favicon.svg","path":"img/favicon.svg","modified":0,"renderable":1},{"_id":"themes/icarus/source/img/logo.svg","path":"img/logo.svg","modified":0,"renderable":1},{"_id":"themes/icarus/source/img/og_image.png","path":"img/og_image.png","modified":0,"renderable":1},{"_id":"themes/icarus/source/img/razor-bottom-black.svg","path":"img/razor-bottom-black.svg","modified":0,"renderable":1},{"_id":"themes/icarus/source/img/razor-top-black.svg","path":"img/razor-top-black.svg","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/animation.js","path":"js/animation.js","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/back_to_top.js","path":"js/back_to_top.js","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/column.js","path":"js/column.js","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"source/img/avatar.jpg","path":"img/avatar.jpg","modified":0,"renderable":0},{"_id":"source/img/batched_gemm.jpg","path":"img/batched_gemm.jpg","modified":0,"renderable":0},{"_id":"source/img/logo.jpg","path":"img/logo.jpg","modified":0,"renderable":0},{"_id":"source/info/doc/A Convolutional Neural Network Framework support on CPU and GPU.pdf","path":"info/doc/A Convolutional Neural Network Framework support on CPU and GPU.pdf","modified":0,"renderable":0},{"_id":"source/info/doc/Bachelor's dissertation.pdf","path":"info/doc/Bachelor's dissertation.pdf","modified":0,"renderable":0},{"_id":"source/info/doc/resume.pdf","path":"info/doc/resume.pdf","modified":0,"renderable":0}],"Cache":[{"_id":"source/_posts/YSYX.md","hash":"41118966a81faa82dcbf52630d1fcd696b564864","modified":1679570589378},{"_id":"source/_posts/冰雹.md","hash":"0b5ba41a9c1ec0c5a2db081940455cb058fc77d7","modified":1679638878595},{"_id":"source/_posts/build_web.md","hash":"ca0fcb6ff4231526812ae99a87e11d7595f9afd2","modified":1679572915747},{"_id":"source/_posts/learn-cutlass-1.md","hash":"04fafad6737866359abed9f650a96004450996b2","modified":1679558710189},{"_id":"source/_posts/learn-cutlass-0.md","hash":"5414b25eb9e59afbc6a847ad307c42100d17c36b","modified":1679373506306},{"_id":"source/info/index.md","hash":"e2fe9358d79ffa7d51cde469be2024bc0c9de43b","modified":1679313266359},{"_id":"themes/icarus/layout/comment/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1679555332370},{"_id":"themes/icarus/layout/donate/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1679555332370},{"_id":"themes/icarus/layout/misc/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1679555332370},{"_id":"themes/icarus/layout/search/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1679555332370},{"_id":"themes/icarus/layout/share/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1679555332370},{"_id":"themes/icarus/include/schema/comment/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1679555332370},{"_id":"themes/icarus/include/schema/donate/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1679555332370},{"_id":"themes/icarus/include/schema/misc/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1679555332370},{"_id":"themes/icarus/include/schema/search/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1679555332370},{"_id":"themes/icarus/include/schema/share/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1679555332370},{"_id":"source/info/doc/resume.pdf","hash":"bbf324f7aceb80ec3672b25ed71a92af8f6b1b89","modified":1679312906959},{"_id":"themes/icarus/.gitignore","hash":"36084c0851a22bfd393933abb8e262562ffc6802","modified":1679555332370},{"_id":"themes/icarus/.eslintignore","hash":"5410a1bef9807f666cd92a0d2020f700e67e4096","modified":1679555332370},{"_id":"themes/icarus/.eslintrc.json","hash":"ca45d8e5d80ad6b7004cddacae7fc7d8040b2d6f","modified":1679555332370},{"_id":"themes/icarus/.npmignore","hash":"42242c8da7a020a3295e7dd3d18bf022cb08b661","modified":1679555332370},{"_id":"themes/icarus/CONTRIBUTING.md","hash":"70254c6778c1e41bb2ff222bbf3a70b2239b9bc1","modified":1679555332370},{"_id":"themes/icarus/LICENSE","hash":"86037e5335a49321fa73b7815cab542057fac944","modified":1679555332370},{"_id":"themes/icarus/include/config.js","hash":"1ff0f174e9670074ad2bee890d5b6da486800c9a","modified":1679555332370},{"_id":"themes/icarus/include/dependency.js","hash":"0ca35dec92ccf383f45db905db1a5a0e92d7209e","modified":1679555332370},{"_id":"themes/icarus/package.json","hash":"75db783b805785377db28d4cb844ee65bb7be613","modified":1679555332370},{"_id":"themes/icarus/include/register.js","hash":"ec6596b63bfb4349ba61792d905abe8e06fea625","modified":1679555332370},{"_id":"themes/icarus/.github/dependabot.yml","hash":"d532d0db30e42211f35823b9885f3ed1b2d51777","modified":1679555332370},{"_id":"themes/icarus/.github/stale.yml","hash":"88c73f6216c5666d2f60b1e8fe690a3f6e561e42","modified":1679555332370},{"_id":"themes/icarus/languages/de.yml","hash":"78421f09961ca0b24756a0688fb2cb2e2696e25f","modified":1679555332370},{"_id":"themes/icarus/languages/es.yml","hash":"38579b8fad4b6997362acc770615bcd85ff20f68","modified":1679555332370},{"_id":"themes/icarus/languages/en.yml","hash":"3d674204d9f723c829226da745afddd180c1131d","modified":1679555332370},{"_id":"themes/icarus/README.md","hash":"32f9f4fc8cd7ec60b30544bd2e558b593519ae5d","modified":1679555332370},{"_id":"themes/icarus/languages/fr.yml","hash":"06d5c819d6108a42b28cff7b52e5410d0bed55d1","modified":1679555332370},{"_id":"themes/icarus/languages/ja.yml","hash":"801d9930fef48d6a3f80470d5bed4f3eb78147e6","modified":1679555332370},{"_id":"themes/icarus/languages/id.yml","hash":"5e48b1d62378cadeb64b88349477726a5c1bae47","modified":1679555332370},{"_id":"themes/icarus/languages/ko.yml","hash":"e3374265377809c1518114cf352b595840c0b416","modified":1679555332370},{"_id":"themes/icarus/languages/pl.yml","hash":"2e7debb44cd91096f30efc87bf8d6b1d0d0214c9","modified":1679555332370},{"_id":"themes/icarus/languages/tr.yml","hash":"dd0a7bfe14848d6e1aa229198fe1db03e08e305e","modified":1679555332370},{"_id":"themes/icarus/languages/ru.yml","hash":"9d91358c2acbe7a0f2a25daf7f65b999ff32d068","modified":1679555332370},{"_id":"themes/icarus/languages/tk.yml","hash":"ca583168bd2025124a1cd0e977da475d7a7496fd","modified":1679555332370},{"_id":"themes/icarus/languages/zh-CN.yml","hash":"02475ba14afc70dfeaf5678467cee307835e4efa","modified":1679555332370},{"_id":"themes/icarus/languages/vn.yml","hash":"5f2fffa642110c81d8f529949711c9d19ad6bbbe","modified":1679555332370},{"_id":"themes/icarus/layout/archive.jsx","hash":"99bf235042d0c57af15d2f108ba5eda77443fea8","modified":1679555332370},{"_id":"themes/icarus/languages/zh-TW.yml","hash":"a6826e0c8cdb9ad286324b682b466a9e2ad78e6f","modified":1679555332370},{"_id":"themes/icarus/layout/category.jsx","hash":"fd15e4eac32de9ac8687aeb3dbe179ab61375700","modified":1679555332370},{"_id":"themes/icarus/layout/categories.jsx","hash":"b8ad43e28a4990d222bfbb95b032f88555492347","modified":1679555332370},{"_id":"themes/icarus/layout/index.jsx","hash":"0a84a2348394fa9fc5080dd396bd28d357594f47","modified":1679555332370},{"_id":"themes/icarus/languages/pt-BR.yml","hash":"ee8f73350e4c6e2f63b7fc72b34472a6b1e21244","modified":1679555332370},{"_id":"themes/icarus/layout/post.jsx","hash":"d26c2db57e5a88d6483a03aeb51cda9d191d8cea","modified":1679555332370},{"_id":"themes/icarus/layout/page.jsx","hash":"d26c2db57e5a88d6483a03aeb51cda9d191d8cea","modified":1679555332370},{"_id":"themes/icarus/scripts/index.js","hash":"0c666db6fcb4ffc4d300f4e108c00ee42b1cbbe6","modified":1679555332370},{"_id":"themes/icarus/layout/tag.jsx","hash":"d2f18cac32ca2725d34ccff3f2051c623be6c892","modified":1679555332370},{"_id":"themes/icarus/layout/tags.jsx","hash":"2c42cb64778235dd220c563a27a92108ddc50cc4","modified":1679555332370},{"_id":"themes/icarus/.github/ISSUE_TEMPLATE/Bug反馈.md","hash":"d801be6937df21e63113be8840b2e23c465db87b","modified":1679555332370},{"_id":"themes/icarus/layout/layout.jsx","hash":"2f882ac4d124a557ecdc2be5bc4875a994f179ec","modified":1679560337406},{"_id":"themes/icarus/.github/ISSUE_TEMPLATE/bug_report.md","hash":"5d3abdc1a2a79ef9822865184eced6e31066d8f6","modified":1679555332370},{"_id":"themes/icarus/.github/ISSUE_TEMPLATE/feature_request.md","hash":"05313e241a3d43fdadb0f74390b562c07c25d4d0","modified":1679555332370},{"_id":"themes/icarus/.github/ISSUE_TEMPLATE/config.yml","hash":"b49270a202d839c298e584b5d017f56140c7e7a0","modified":1679555332370},{"_id":"themes/icarus/.github/ISSUE_TEMPLATE/功能建议.md","hash":"c6f60f37e6cd0bc21a81b3b07f84e04e9e80fb57","modified":1679555332370},{"_id":"themes/icarus/.github/workflows/npm-publish.yml","hash":"03942b094362e71def2ff78ff0489d38482144d4","modified":1679555332370},{"_id":"themes/icarus/.github/PULL_REQUEST_TEMPLATE/pull_request_template.md","hash":"e106fa240c24c8bfbdd623549651600c6d786c9c","modified":1679555332370},{"_id":"themes/icarus/.github/workflows/github-release.yml","hash":"d9830273e7e23c4151b10840f131f0b78b961bae","modified":1679555332370},{"_id":"themes/icarus/include/migration/head.js","hash":"7189efe33d18927d3790e8afb06642fb293b8603","modified":1679555332370},{"_id":"themes/icarus/.github/workflows/lint.yml","hash":"17d374bf92867a67b15d657c2d41f8e4f9271102","modified":1679555332370},{"_id":"themes/icarus/.github/workflows/test.yml","hash":"a92a6c557fc18f8c161f7e656ec330bffdb0f3ee","modified":1679555332370},{"_id":"themes/icarus/include/migration/v5_v5.1.js","hash":"073f22bd16e34b56f016633b1676dab2e7d8843d","modified":1679555332370},{"_id":"themes/icarus/include/schema/config.json","hash":"f233678cd656c0e300181ca79dd30cb42fc213b3","modified":1679555332370},{"_id":"themes/icarus/include/migration/v2_v3.js","hash":"3ccb2d2ce11018bebd7172da66faecc3983bff00","modified":1679555332370},{"_id":"themes/icarus/include/migration/v3_v4.js","hash":"9faf2184d7fe87debfbe007f3fc9079dcbcafcfe","modified":1679555332370},{"_id":"themes/icarus/include/migration/v4_v5.js","hash":"6342310892d113763b5544789b45d44c0ccf2854","modified":1679555332370},{"_id":"themes/icarus/include/style/card.styl","hash":"f78674422eb408cd17c17bbdc3ee1ebe4a453e05","modified":1679555332370},{"_id":"themes/icarus/include/style/article.styl","hash":"105c983871b6c9148d97a0f756886e56411572bd","modified":1679555332370},{"_id":"themes/icarus/include/style/button.styl","hash":"0fb35b4786be1b387c751fa2849bc71523fcedd4","modified":1679555332370},{"_id":"themes/icarus/include/style/codeblock.styl","hash":"ec54dc24eb4d9802d8fefc44c210558bc1641109","modified":1679555332370},{"_id":"themes/icarus/include/style/donate.styl","hash":"8d0af00628c13134b5f30a558608e7bebf18c2ec","modified":1679555332370},{"_id":"themes/icarus/include/style/footer.styl","hash":"a4ad715dee38b249538ac6cce94efc9b355a904b","modified":1679555332370},{"_id":"themes/icarus/include/style/helper.styl","hash":"9f3393e6122cc9f351091bfab960674e962da343","modified":1679555332370},{"_id":"themes/icarus/include/style/pagination.styl","hash":"b81bcd7ff915b4e9299533addc01bc4575ec35e3","modified":1679555332370},{"_id":"themes/icarus/include/style/base.styl","hash":"2bca6ad099949d52236c87db8db1002ffb99774c","modified":1679559149359},{"_id":"themes/icarus/include/style/plugin.styl","hash":"084843d5a522029e0f84a4fe791fbcb2cabd4c36","modified":1679555332370},{"_id":"themes/icarus/include/style/responsive.styl","hash":"207083fe287612cddee6608b541861b14ac8de81","modified":1679555332370},{"_id":"themes/icarus/include/util/console.js","hash":"59cf9d277d3ac85a496689bd811b1c316001641d","modified":1679555332370},{"_id":"themes/icarus/include/style/timeline.styl","hash":"ea61798a09bffdda07efb93c2ff800b63bddc4c4","modified":1679555332370},{"_id":"themes/icarus/include/style/search.styl","hash":"416737e1da4e7e907bd03609b0fee9e2aacfe56c","modified":1679555332370},{"_id":"themes/icarus/include/style/widget.styl","hash":"c746902251136544eb3fe523235b3183f4189460","modified":1679555332370},{"_id":"themes/icarus/layout/common/article.jsx","hash":"1d06eee32ea1fcb3162227eb1d7d19be39b6f5e3","modified":1679555332370},{"_id":"themes/icarus/layout/common/footer.jsx","hash":"de966666f1e4ef80e0d15081b2709c3065b246dd","modified":1679555332370},{"_id":"themes/icarus/layout/common/donates.jsx","hash":"889fb0a7ccc502f0a43b4a18eb330e351e50493c","modified":1679555332370},{"_id":"themes/icarus/layout/common/comment.jsx","hash":"427089c33002707b76e2f38709459a6824fd0f9b","modified":1679555332370},{"_id":"themes/icarus/layout/common/navbar.jsx","hash":"d96e501e52861056474659f96ee0206588d8c93a","modified":1679555332370},{"_id":"themes/icarus/layout/common/plugins.jsx","hash":"f6826c1a5f5f59f4a0aa00c63bdb0ad4ff4eab69","modified":1679555332370},{"_id":"themes/icarus/layout/common/scripts.jsx","hash":"4816c9099a881b5f7b13af3e42caae36edbffccd","modified":1679555332370},{"_id":"themes/icarus/layout/common/head.jsx","hash":"2ec1f511f32e3a9c86d49f1338f57ae5ece18898","modified":1679555332370},{"_id":"themes/icarus/layout/common/search.jsx","hash":"6f244a37293031670a2964fe424ecd062e591d7b","modified":1679555332370},{"_id":"themes/icarus/layout/common/share.jsx","hash":"c9fb0319ad5e5a10ad3636b26a6c2afed14c590f","modified":1679555332370},{"_id":"themes/icarus/layout/plugin/back_to_top.jsx","hash":"7fc0c5aaabd7d0eaff04cb68ec139442dc3414e8","modified":1679555332370},{"_id":"themes/icarus/layout/plugin/animejs.jsx","hash":"e2aa27c3501a58ef1e91e511557b77395c2c02aa","modified":1679555332370},{"_id":"themes/icarus/layout/widget/profile.jsx","hash":"0d3a7fd922c12cc45d2c8d26a8f4d3a9a6ed0ae0","modified":1679555332370},{"_id":"themes/icarus/layout/common/widgets.jsx","hash":"e733c590556b33aa6f9ff53019858a17f48a4c99","modified":1679557803289},{"_id":"themes/icarus/source/css/default.styl","hash":"b01da3028e5a1267a40aaae5c86a11187a2259e3","modified":1679555332370},{"_id":"themes/icarus/source/css/style.styl","hash":"5b9815586e993a6ccbe8cdcfc0c65ea38fc315ac","modified":1679555332370},{"_id":"themes/icarus/source/css/cyberpunk.styl","hash":"ae17d3528df0c3f089df14a06b7bd82f1bc5fed9","modified":1679555332370},{"_id":"themes/icarus/source/img/avatar.png","hash":"0d8236dcca871735500e9d06bbdbe0853ed6775b","modified":1679555332370},{"_id":"themes/icarus/source/img/og_image.png","hash":"b03f163096ca9c350ec962feee9836277b5c2509","modified":1679555332370},{"_id":"themes/icarus/source/img/favicon.svg","hash":"16fd847265845063a16596761cddb32926073dd2","modified":1679555332370},{"_id":"themes/icarus/source/img/razor-bottom-black.svg","hash":"a3eda07b1c605b456da9cdf335a1075db5e5d72c","modified":1679555332370},{"_id":"themes/icarus/source/img/logo.svg","hash":"e9b5c1438ddb576693a15d0713b2a1d9ceda4be9","modified":1679555332370},{"_id":"themes/icarus/include/style/navbar.styl","hash":"34f09b144cb46a25ec2cc7260a6c207dd34ff1fe","modified":1679555332370},{"_id":"themes/icarus/source/img/razor-top-black.svg","hash":"201f1171a43ce667a39091fe47c0f278857f18f0","modified":1679555332370},{"_id":"themes/icarus/source/js/animation.js","hash":"0a8e361c353daa3194f4de3d646b96025d128e1a","modified":1679555332370},{"_id":"themes/icarus/source/js/.eslintrc.json","hash":"6bf0641cb69dffac97f69baea192d7fa3ab612cb","modified":1679555332370},{"_id":"themes/icarus/include/schema/common/article.json","hash":"e2502c39045c6a26ccd8e880858f93e78c7bda35","modified":1679555332370},{"_id":"themes/icarus/include/schema/common/comment.json","hash":"f49270b619f5d2c3decde6b0b5a0c3bbab4b54a5","modified":1679555332370},{"_id":"themes/icarus/include/schema/common/donates.json","hash":"ae86e6f177bedf4afbe638502c12635027539305","modified":1679555332370},{"_id":"themes/icarus/include/schema/common/footer.json","hash":"e85c9d7f2579805beb252a1b6345d5a668a13baa","modified":1679555332370},{"_id":"themes/icarus/include/schema/common/head.json","hash":"98889f059c635e6bdbd51effd04cf1cf44968a66","modified":1679555332370},{"_id":"themes/icarus/include/schema/common/navbar.json","hash":"6691e587284c4cf450e0288680d5ff0f3565f090","modified":1679555332370},{"_id":"themes/icarus/include/schema/common/plugins.json","hash":"6036a805749816416850d944f7d64aaae62e5e75","modified":1679555332370},{"_id":"themes/icarus/include/schema/common/providers.json","hash":"97ec953d497fb53594227ae98acaef8a8baa91da","modified":1679555332370},{"_id":"themes/icarus/include/schema/common/search.json","hash":"985fbcbf47054af714ead1a124869d54f2a8b607","modified":1679555332370},{"_id":"themes/icarus/include/schema/common/share.json","hash":"cf4f9ff4fb27c3541b35f57db355c228fa6873e4","modified":1679555332370},{"_id":"themes/icarus/include/schema/common/sidebar.json","hash":"eb241beaec4c73e3085dfb3139ce72e827e20549","modified":1679555332370},{"_id":"themes/icarus/include/schema/common/widgets.json","hash":"cadd9dc942740ecd5037d3943e72f8b6a8399bbe","modified":1679555332370},{"_id":"themes/icarus/include/schema/plugin/animejs.json","hash":"e62ab6e20bd8862efa1ed32e7c0db0f8acbcfdec","modified":1679555332370},{"_id":"themes/icarus/include/schema/plugin/back_to_top.json","hash":"dc0febab7e7b67075d0ad3f80f5ec8b798b68dea","modified":1679555332370},{"_id":"themes/icarus/source/js/back_to_top.js","hash":"d91f10c08c726135a13dfa1f422c49d8764ef03f","modified":1679555332370},{"_id":"themes/icarus/include/schema/widget/profile.json","hash":"690ee1b0791cab47ea03cf42b5b4932ed2aa5675","modified":1679555332370},{"_id":"themes/icarus/source/js/column.js","hash":"0baee024ab67474c073a4c41b495f3e7f0df4505","modified":1679555332370},{"_id":"source/img/batched_gemm.jpg","hash":"4f78d083a5d56c3776e819d561096f867a088612","modified":1679363504738},{"_id":"themes/icarus/source/js/main.js","hash":"08a2641765eeaf712157ad134dd675e3f7708ae2","modified":1679555332370},{"_id":"source/img/avatar.jpg","hash":"e2fe014b6234172847599124e08a97296d0d5a63","modified":1679312906929},{"_id":"source/img/logo.jpg","hash":"af002a111cb40bcd7cdd070e58827c840fc6393f","modified":1679312906929},{"_id":"source/info/doc/A Convolutional Neural Network Framework support on CPU and GPU.pdf","hash":"ce8c54f1892b4aedc5bd83a972a812de8a674f9f","modified":1679312906939},{"_id":"source/info/doc/Bachelor's dissertation.pdf","hash":"ea44b5b4104cadb900922535fd1505cc7ef5b8e6","modified":1679312906959},{"_id":"source/_posts/hail.md","hash":"4cced31b560252db9d85d4cc32a4ffc572d8cfe4","modified":1679638662895}],"Category":[{"name":"essay","_id":"clfm5kvrj0003717v22bb8u64"},{"name":"Technology","_id":"clfm5kvrm0006717v0gyg4qt1"},{"name":"log","_id":"clfm5mhsi0001gx7v3v9gend3"}],"Data":[],"Page":[{"title":"Tianyu Guo (郭天宇)","_content":"\n\n\nE-mail : guoty9[at]mail2.sysu.edu.cn\n\n## [My Resume](doc/resume.pdf)\n\n---\n\n## Education Experience\n\n| year        | university             | degree |\n| ----------- | ---------------------- | ------ |\n| 2018 - 2022 | Xidian University      | B.S.   |\n| 2022 - now  | Sun Yat-Sen University | Master |\n\n## Experience\n\n- Teaching Assistant of \"SYSU-DCS3013 : Computer Architecture\" [2022f]\n  \n  > release [SYSU-ARCH LAB](https://arcsysu.github.io/SYSU-ARCH)\n\n## Research\n\n- [Bachelor's dissertation](doc/Bachelor's%20dissertation.pdf) “General Computing optimization for GPU based on Cache management”\n\n- [AI final Homework](doc/A%20Convolutional%20Neural%20Network%20Framework%20support%20on%20CPU%20and%20GPU.pdf) \"A Convolutional Neural Network Framework support on CPU and GPU\"\n","source":"info/index.md","raw":"---\ntitle: Tianyu Guo (郭天宇)\n---\n\n\n\nE-mail : guoty9[at]mail2.sysu.edu.cn\n\n## [My Resume](doc/resume.pdf)\n\n---\n\n## Education Experience\n\n| year        | university             | degree |\n| ----------- | ---------------------- | ------ |\n| 2018 - 2022 | Xidian University      | B.S.   |\n| 2022 - now  | Sun Yat-Sen University | Master |\n\n## Experience\n\n- Teaching Assistant of \"SYSU-DCS3013 : Computer Architecture\" [2022f]\n  \n  > release [SYSU-ARCH LAB](https://arcsysu.github.io/SYSU-ARCH)\n\n## Research\n\n- [Bachelor's dissertation](doc/Bachelor's%20dissertation.pdf) “General Computing optimization for GPU based on Cache management”\n\n- [AI final Homework](doc/A%20Convolutional%20Neural%20Network%20Framework%20support%20on%20CPU%20and%20GPU.pdf) \"A Convolutional Neural Network Framework support on CPU and GPU\"\n","date":"2023-03-20T11:54:26.359Z","updated":"2023-03-20T11:54:26.359Z","path":"info/index.html","comments":1,"layout":"page","_id":"clfm5kvr90000717v5gct1pfi","content":"<p>E-mail : guoty9[at]mail2.sysu.edu.cn</p>\n<h2 id=\"My-Resume\"><a href=\"#My-Resume\" class=\"headerlink\" title=\"My Resume\"></a><a href=\"doc/resume.pdf\">My Resume</a></h2><hr>\n<h2 id=\"Education-Experience\"><a href=\"#Education-Experience\" class=\"headerlink\" title=\"Education Experience\"></a>Education Experience</h2><table>\n<thead>\n<tr>\n<th>year</th>\n<th>university</th>\n<th>degree</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>2018 - 2022</td>\n<td>Xidian University</td>\n<td>B.S.</td>\n</tr>\n<tr>\n<td>2022 - now</td>\n<td>Sun Yat-Sen University</td>\n<td>Master</td>\n</tr>\n</tbody></table>\n<h2 id=\"Experience\"><a href=\"#Experience\" class=\"headerlink\" title=\"Experience\"></a>Experience</h2><ul>\n<li><p>Teaching Assistant of “SYSU-DCS3013 : Computer Architecture” [2022f]</p>\n<blockquote>\n<p>release <a href=\"https://arcsysu.github.io/SYSU-ARCH\">SYSU-ARCH LAB</a></p>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"Research\"><a href=\"#Research\" class=\"headerlink\" title=\"Research\"></a>Research</h2><ul>\n<li><p><a href=\"doc/Bachelor's%20dissertation.pdf\">Bachelor’s dissertation</a> “General Computing optimization for GPU based on Cache management”</p>\n</li>\n<li><p><a href=\"doc/A%20Convolutional%20Neural%20Network%20Framework%20support%20on%20CPU%20and%20GPU.pdf\">AI final Homework</a> “A Convolutional Neural Network Framework support on CPU and GPU”</p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>E-mail : guoty9[at]mail2.sysu.edu.cn</p>\n<h2 id=\"My-Resume\"><a href=\"#My-Resume\" class=\"headerlink\" title=\"My Resume\"></a><a href=\"doc/resume.pdf\">My Resume</a></h2><hr>\n<h2 id=\"Education-Experience\"><a href=\"#Education-Experience\" class=\"headerlink\" title=\"Education Experience\"></a>Education Experience</h2><table>\n<thead>\n<tr>\n<th>year</th>\n<th>university</th>\n<th>degree</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>2018 - 2022</td>\n<td>Xidian University</td>\n<td>B.S.</td>\n</tr>\n<tr>\n<td>2022 - now</td>\n<td>Sun Yat-Sen University</td>\n<td>Master</td>\n</tr>\n</tbody></table>\n<h2 id=\"Experience\"><a href=\"#Experience\" class=\"headerlink\" title=\"Experience\"></a>Experience</h2><ul>\n<li><p>Teaching Assistant of “SYSU-DCS3013 : Computer Architecture” [2022f]</p>\n<blockquote>\n<p>release <a href=\"https://arcsysu.github.io/SYSU-ARCH\">SYSU-ARCH LAB</a></p>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"Research\"><a href=\"#Research\" class=\"headerlink\" title=\"Research\"></a>Research</h2><ul>\n<li><p><a href=\"doc/Bachelor's%20dissertation.pdf\">Bachelor’s dissertation</a> “General Computing optimization for GPU based on Cache management”</p>\n</li>\n<li><p><a href=\"doc/A%20Convolutional%20Neural%20Network%20Framework%20support%20on%20CPU%20and%20GPU.pdf\">AI final Homework</a> “A Convolutional Neural Network Framework support on CPU and GPU”</p>\n</li>\n</ul>\n"}],"Post":[{"title":"我的一生一芯","toc":true,"date":"2023-03-23T10:36:11.000Z","_content":"\n[一生一芯](https://ysyx.oscc.cc/)是一个开放社区性质的公益教学项目，主要是为了解决中国芯片设计人才缺失的问题，主办方是国科大和计算所，要求学生设计出一块自己的CPU并成功流片。\n<!-- more -->\n\n以下是我的一生一芯项目的记录\n- [github](https://github.com/gty111/YSYX)\n- [腾讯文档](https://docs.qq.com/sheet/DY0xnWmdGd0pDaUNV?tab=BB08J2)\n\n## 初识一生一芯\n\n记得还是在大四阶段接触到的一生一芯计划，当时是在西电保研群某个大佬发的链接，于是就报名参加了。大四应该是整个大学最轻松的时候（指心情方面），保研阶段结束，于是就开始刷各种实验或写毕设。\n\n## 深入一生一芯\n\n前期最终要的就是要完成两个实验，[数字电路基础实验(dlco)](https://nju-projectn.github.io/dlco-lecture-note/index.html)和[计算机系统基础实验(PA)](https://ysyx.oscc.cc/docs/ics-pa/)。\n\n### 数字电路基础实验\n\ndlco主要是通过[nvboard](https://github.com/NJU-ProjectN/nvboard)(一个虚拟FPGA开发板)进行实验，而实验语言便是verilog。`我之前从来没有接触过硬件相关知识，所以会习惯用软件编程的思想写硬件程序(verilog)，这也是我最终退出一生一芯的主要原因。`在完成dlco实验过程中，其中最令我印象深刻的是字符输入界面，我在上面投入了很多时间，最终也实现了几乎所有的扩展要求，当打开界面，在键盘上敲下各个按键时字母符号出现在屏幕上时，感觉十分有成就感。\n\n### 计算机系统基础实验\n\nPA实验是继CSAPP LAB后第二个令我震惊的实验，感觉我本科实验真的好lj。简单地介绍下PA是什么，它要求学生实现一个经过简化但功能完备的x86/mips32/riscv32(64)模拟器，最终可以运行galgame等游戏。PA可以说是由浅入深，让学生从零基础逐渐深入到计算机底层的各个领域。PA实验给我带来最大的帮助就是了解了riscv指令集和模拟器究竟是什么。\n\n### CPU编写\n一生一芯顾名思义肯定是要写CPU啦，许多人一听到写CPU就会感觉“哇，这也太难了吧”，其实你可以把它想得很简单，如果用软件编程的思想来看是一个流程：写程序（verilog或chisel）=> 逻辑综合(可以理解为软件的编译) => 剩下全部丢给后端去做了。所以芯片设计对于我的理解就是写verilog，我想“C++我都不怕，怎么可能怕verilog呢?”，于是开始了漫长的verilog编写，我甚至迭代了十几个版本（每个版本都通过了benchmark测试正确性），最后发现自己写的verilog还是lj。这时候，我才发现我缺少了最重要的一环，硬件设计思想，于是我默默地退出了。\n\n## 后记\n尽管我的一生一芯计划并没有走到最后，但是我在参加的过程中还是收货了很多知识，这对我之后的学习起了很大的帮助。我对于一生一芯计划评价还是很高的，它可以系统地训练或培养学生进行芯片设计，从中你可以学习到很多体系结构的知识，希望一生一芯可以一直走下去吧，也希望中国的芯片巨头早日出现(对标苹果,intel,Nvidia,AMD)。\n","source":"_posts/YSYX.md","raw":"---\ntitle: 我的一生一芯\ntoc: true\ndate: 2023-03-23 18:36:11\ncategories:\n- essay\ntags:\n- 一生一芯\n---\n\n[一生一芯](https://ysyx.oscc.cc/)是一个开放社区性质的公益教学项目，主要是为了解决中国芯片设计人才缺失的问题，主办方是国科大和计算所，要求学生设计出一块自己的CPU并成功流片。\n<!-- more -->\n\n以下是我的一生一芯项目的记录\n- [github](https://github.com/gty111/YSYX)\n- [腾讯文档](https://docs.qq.com/sheet/DY0xnWmdGd0pDaUNV?tab=BB08J2)\n\n## 初识一生一芯\n\n记得还是在大四阶段接触到的一生一芯计划，当时是在西电保研群某个大佬发的链接，于是就报名参加了。大四应该是整个大学最轻松的时候（指心情方面），保研阶段结束，于是就开始刷各种实验或写毕设。\n\n## 深入一生一芯\n\n前期最终要的就是要完成两个实验，[数字电路基础实验(dlco)](https://nju-projectn.github.io/dlco-lecture-note/index.html)和[计算机系统基础实验(PA)](https://ysyx.oscc.cc/docs/ics-pa/)。\n\n### 数字电路基础实验\n\ndlco主要是通过[nvboard](https://github.com/NJU-ProjectN/nvboard)(一个虚拟FPGA开发板)进行实验，而实验语言便是verilog。`我之前从来没有接触过硬件相关知识，所以会习惯用软件编程的思想写硬件程序(verilog)，这也是我最终退出一生一芯的主要原因。`在完成dlco实验过程中，其中最令我印象深刻的是字符输入界面，我在上面投入了很多时间，最终也实现了几乎所有的扩展要求，当打开界面，在键盘上敲下各个按键时字母符号出现在屏幕上时，感觉十分有成就感。\n\n### 计算机系统基础实验\n\nPA实验是继CSAPP LAB后第二个令我震惊的实验，感觉我本科实验真的好lj。简单地介绍下PA是什么，它要求学生实现一个经过简化但功能完备的x86/mips32/riscv32(64)模拟器，最终可以运行galgame等游戏。PA可以说是由浅入深，让学生从零基础逐渐深入到计算机底层的各个领域。PA实验给我带来最大的帮助就是了解了riscv指令集和模拟器究竟是什么。\n\n### CPU编写\n一生一芯顾名思义肯定是要写CPU啦，许多人一听到写CPU就会感觉“哇，这也太难了吧”，其实你可以把它想得很简单，如果用软件编程的思想来看是一个流程：写程序（verilog或chisel）=> 逻辑综合(可以理解为软件的编译) => 剩下全部丢给后端去做了。所以芯片设计对于我的理解就是写verilog，我想“C++我都不怕，怎么可能怕verilog呢?”，于是开始了漫长的verilog编写，我甚至迭代了十几个版本（每个版本都通过了benchmark测试正确性），最后发现自己写的verilog还是lj。这时候，我才发现我缺少了最重要的一环，硬件设计思想，于是我默默地退出了。\n\n## 后记\n尽管我的一生一芯计划并没有走到最后，但是我在参加的过程中还是收货了很多知识，这对我之后的学习起了很大的帮助。我对于一生一芯计划评价还是很高的，它可以系统地训练或培养学生进行芯片设计，从中你可以学习到很多体系结构的知识，希望一生一芯可以一直走下去吧，也希望中国的芯片巨头早日出现(对标苹果,intel,Nvidia,AMD)。\n","slug":"YSYX","published":1,"updated":"2023-03-23T11:23:09.378Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clfm5kvrg0001717vgwbmft8i","content":"<p><a href=\"https://ysyx.oscc.cc/\">一生一芯</a>是一个开放社区性质的公益教学项目，主要是为了解决中国芯片设计人才缺失的问题，主办方是国科大和计算所，要求学生设计出一块自己的CPU并成功流片。</p>\n<span id=\"more\"></span>\n\n<p>以下是我的一生一芯项目的记录</p>\n<ul>\n<li><a href=\"https://github.com/gty111/YSYX\">github</a></li>\n<li><a href=\"https://docs.qq.com/sheet/DY0xnWmdGd0pDaUNV?tab=BB08J2\">腾讯文档</a></li>\n</ul>\n<h2 id=\"初识一生一芯\"><a href=\"#初识一生一芯\" class=\"headerlink\" title=\"初识一生一芯\"></a>初识一生一芯</h2><p>记得还是在大四阶段接触到的一生一芯计划，当时是在西电保研群某个大佬发的链接，于是就报名参加了。大四应该是整个大学最轻松的时候（指心情方面），保研阶段结束，于是就开始刷各种实验或写毕设。</p>\n<h2 id=\"深入一生一芯\"><a href=\"#深入一生一芯\" class=\"headerlink\" title=\"深入一生一芯\"></a>深入一生一芯</h2><p>前期最终要的就是要完成两个实验，<a href=\"https://nju-projectn.github.io/dlco-lecture-note/index.html\">数字电路基础实验(dlco)</a>和<a href=\"https://ysyx.oscc.cc/docs/ics-pa/\">计算机系统基础实验(PA)</a>。</p>\n<h3 id=\"数字电路基础实验\"><a href=\"#数字电路基础实验\" class=\"headerlink\" title=\"数字电路基础实验\"></a>数字电路基础实验</h3><p>dlco主要是通过<a href=\"https://github.com/NJU-ProjectN/nvboard\">nvboard</a>(一个虚拟FPGA开发板)进行实验，而实验语言便是verilog。<code>我之前从来没有接触过硬件相关知识，所以会习惯用软件编程的思想写硬件程序(verilog)，这也是我最终退出一生一芯的主要原因。</code>在完成dlco实验过程中，其中最令我印象深刻的是字符输入界面，我在上面投入了很多时间，最终也实现了几乎所有的扩展要求，当打开界面，在键盘上敲下各个按键时字母符号出现在屏幕上时，感觉十分有成就感。</p>\n<h3 id=\"计算机系统基础实验\"><a href=\"#计算机系统基础实验\" class=\"headerlink\" title=\"计算机系统基础实验\"></a>计算机系统基础实验</h3><p>PA实验是继CSAPP LAB后第二个令我震惊的实验，感觉我本科实验真的好lj。简单地介绍下PA是什么，它要求学生实现一个经过简化但功能完备的x86&#x2F;mips32&#x2F;riscv32(64)模拟器，最终可以运行galgame等游戏。PA可以说是由浅入深，让学生从零基础逐渐深入到计算机底层的各个领域。PA实验给我带来最大的帮助就是了解了riscv指令集和模拟器究竟是什么。</p>\n<h3 id=\"CPU编写\"><a href=\"#CPU编写\" class=\"headerlink\" title=\"CPU编写\"></a>CPU编写</h3><p>一生一芯顾名思义肯定是要写CPU啦，许多人一听到写CPU就会感觉“哇，这也太难了吧”，其实你可以把它想得很简单，如果用软件编程的思想来看是一个流程：写程序（verilog或chisel）&#x3D;&gt; 逻辑综合(可以理解为软件的编译) &#x3D;&gt; 剩下全部丢给后端去做了。所以芯片设计对于我的理解就是写verilog，我想“C++我都不怕，怎么可能怕verilog呢?”，于是开始了漫长的verilog编写，我甚至迭代了十几个版本（每个版本都通过了benchmark测试正确性），最后发现自己写的verilog还是lj。这时候，我才发现我缺少了最重要的一环，硬件设计思想，于是我默默地退出了。</p>\n<h2 id=\"后记\"><a href=\"#后记\" class=\"headerlink\" title=\"后记\"></a>后记</h2><p>尽管我的一生一芯计划并没有走到最后，但是我在参加的过程中还是收货了很多知识，这对我之后的学习起了很大的帮助。我对于一生一芯计划评价还是很高的，它可以系统地训练或培养学生进行芯片设计，从中你可以学习到很多体系结构的知识，希望一生一芯可以一直走下去吧，也希望中国的芯片巨头早日出现(对标苹果,intel,Nvidia,AMD)。</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"https://ysyx.oscc.cc/\">一生一芯</a>是一个开放社区性质的公益教学项目，主要是为了解决中国芯片设计人才缺失的问题，主办方是国科大和计算所，要求学生设计出一块自己的CPU并成功流片。</p>","more":"<p>以下是我的一生一芯项目的记录</p>\n<ul>\n<li><a href=\"https://github.com/gty111/YSYX\">github</a></li>\n<li><a href=\"https://docs.qq.com/sheet/DY0xnWmdGd0pDaUNV?tab=BB08J2\">腾讯文档</a></li>\n</ul>\n<h2 id=\"初识一生一芯\"><a href=\"#初识一生一芯\" class=\"headerlink\" title=\"初识一生一芯\"></a>初识一生一芯</h2><p>记得还是在大四阶段接触到的一生一芯计划，当时是在西电保研群某个大佬发的链接，于是就报名参加了。大四应该是整个大学最轻松的时候（指心情方面），保研阶段结束，于是就开始刷各种实验或写毕设。</p>\n<h2 id=\"深入一生一芯\"><a href=\"#深入一生一芯\" class=\"headerlink\" title=\"深入一生一芯\"></a>深入一生一芯</h2><p>前期最终要的就是要完成两个实验，<a href=\"https://nju-projectn.github.io/dlco-lecture-note/index.html\">数字电路基础实验(dlco)</a>和<a href=\"https://ysyx.oscc.cc/docs/ics-pa/\">计算机系统基础实验(PA)</a>。</p>\n<h3 id=\"数字电路基础实验\"><a href=\"#数字电路基础实验\" class=\"headerlink\" title=\"数字电路基础实验\"></a>数字电路基础实验</h3><p>dlco主要是通过<a href=\"https://github.com/NJU-ProjectN/nvboard\">nvboard</a>(一个虚拟FPGA开发板)进行实验，而实验语言便是verilog。<code>我之前从来没有接触过硬件相关知识，所以会习惯用软件编程的思想写硬件程序(verilog)，这也是我最终退出一生一芯的主要原因。</code>在完成dlco实验过程中，其中最令我印象深刻的是字符输入界面，我在上面投入了很多时间，最终也实现了几乎所有的扩展要求，当打开界面，在键盘上敲下各个按键时字母符号出现在屏幕上时，感觉十分有成就感。</p>\n<h3 id=\"计算机系统基础实验\"><a href=\"#计算机系统基础实验\" class=\"headerlink\" title=\"计算机系统基础实验\"></a>计算机系统基础实验</h3><p>PA实验是继CSAPP LAB后第二个令我震惊的实验，感觉我本科实验真的好lj。简单地介绍下PA是什么，它要求学生实现一个经过简化但功能完备的x86&#x2F;mips32&#x2F;riscv32(64)模拟器，最终可以运行galgame等游戏。PA可以说是由浅入深，让学生从零基础逐渐深入到计算机底层的各个领域。PA实验给我带来最大的帮助就是了解了riscv指令集和模拟器究竟是什么。</p>\n<h3 id=\"CPU编写\"><a href=\"#CPU编写\" class=\"headerlink\" title=\"CPU编写\"></a>CPU编写</h3><p>一生一芯顾名思义肯定是要写CPU啦，许多人一听到写CPU就会感觉“哇，这也太难了吧”，其实你可以把它想得很简单，如果用软件编程的思想来看是一个流程：写程序（verilog或chisel）&#x3D;&gt; 逻辑综合(可以理解为软件的编译) &#x3D;&gt; 剩下全部丢给后端去做了。所以芯片设计对于我的理解就是写verilog，我想“C++我都不怕，怎么可能怕verilog呢?”，于是开始了漫长的verilog编写，我甚至迭代了十几个版本（每个版本都通过了benchmark测试正确性），最后发现自己写的verilog还是lj。这时候，我才发现我缺少了最重要的一环，硬件设计思想，于是我默默地退出了。</p>\n<h2 id=\"后记\"><a href=\"#后记\" class=\"headerlink\" title=\"后记\"></a>后记</h2><p>尽管我的一生一芯计划并没有走到最后，但是我在参加的过程中还是收货了很多知识，这对我之后的学习起了很大的帮助。我对于一生一芯计划评价还是很高的，它可以系统地训练或培养学生进行芯片设计，从中你可以学习到很多体系结构的知识，希望一生一芯可以一直走下去吧，也希望中国的芯片巨头早日出现(对标苹果,intel,Nvidia,AMD)。</p>"},{"title":"How to build this web","date":"2023-03-20T11:40:23.000Z","toc":true,"_content":"\nThis web is building by Hexo and Icarus.\n\n<!-- more -->\n\n## Install Node.js\n\n- for linux [reference this](https://github.com/nodesource/distributions)\n- for windows through [nvs](https://github.com/jasongin/nvs/) or [nvm](https://github.com/nvm-sh/nvm)\n- for mac through [Homebrew](https://brew.sh/) or [MacPorts](http://www.macports.org/)\n\n\n\n## Install Hexo (require Node.js)\n\n```bash\nnpm install -g hexo-cli\n```\n\n## Create your site\n```bash\nhexo init <folder> \ncd <folder>\nnpm install\nnpm install -S hexo-theme-icarus hexo-renderer-inferno # install icarus\nhexo config theme icarus # use theme icarus\nhexo server # start server at localhost\n```\n\n## Tips about Hexo or Icarus\n\n- Add `read more` to your blogs : just add `<!-- more -->` in your md at proper position.\n- To search icons to use, visit [here](https://fontawesome.com/)\n\n\n## Reference\n\n- [hexo-tutorial](https://hexo.io/zh-cn/docs/)\n- [getting-started-with-icarus](https://ppoffice.github.io/hexo-theme-icarus/uncategorized/getting-started-with-icarus/)\n\n\n\n","source":"_posts/build_web.md","raw":"---\ntitle: How to build this web\ndate: 2023-03-20 19:40:23\ncategories:\n- Technology\ntags:\n- Hexo\n- Icarus\ntoc: true\n---\n\nThis web is building by Hexo and Icarus.\n\n<!-- more -->\n\n## Install Node.js\n\n- for linux [reference this](https://github.com/nodesource/distributions)\n- for windows through [nvs](https://github.com/jasongin/nvs/) or [nvm](https://github.com/nvm-sh/nvm)\n- for mac through [Homebrew](https://brew.sh/) or [MacPorts](http://www.macports.org/)\n\n\n\n## Install Hexo (require Node.js)\n\n```bash\nnpm install -g hexo-cli\n```\n\n## Create your site\n```bash\nhexo init <folder> \ncd <folder>\nnpm install\nnpm install -S hexo-theme-icarus hexo-renderer-inferno # install icarus\nhexo config theme icarus # use theme icarus\nhexo server # start server at localhost\n```\n\n## Tips about Hexo or Icarus\n\n- Add `read more` to your blogs : just add `<!-- more -->` in your md at proper position.\n- To search icons to use, visit [here](https://fontawesome.com/)\n\n\n## Reference\n\n- [hexo-tutorial](https://hexo.io/zh-cn/docs/)\n- [getting-started-with-icarus](https://ppoffice.github.io/hexo-theme-icarus/uncategorized/getting-started-with-icarus/)\n\n\n\n","slug":"build_web","published":1,"updated":"2023-03-23T12:01:55.747Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clfm5kvri0002717vdfbs9lkr","content":"<p>This web is building by Hexo and Icarus.</p>\n<span id=\"more\"></span>\n\n<h2 id=\"Install-Node-js\"><a href=\"#Install-Node-js\" class=\"headerlink\" title=\"Install Node.js\"></a>Install Node.js</h2><ul>\n<li>for linux <a href=\"https://github.com/nodesource/distributions\">reference this</a></li>\n<li>for windows through <a href=\"https://github.com/jasongin/nvs/\">nvs</a> or <a href=\"https://github.com/nvm-sh/nvm\">nvm</a></li>\n<li>for mac through <a href=\"https://brew.sh/\">Homebrew</a> or <a href=\"http://www.macports.org/\">MacPorts</a></li>\n</ul>\n<h2 id=\"Install-Hexo-require-Node-js\"><a href=\"#Install-Hexo-require-Node-js\" class=\"headerlink\" title=\"Install Hexo (require Node.js)\"></a>Install Hexo (require Node.js)</h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install -g hexo-cli</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Create-your-site\"><a href=\"#Create-your-site\" class=\"headerlink\" title=\"Create your site\"></a>Create your site</h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo init &lt;folder&gt; </span><br><span class=\"line\"><span class=\"built_in\">cd</span> &lt;folder&gt;</span><br><span class=\"line\">npm install</span><br><span class=\"line\">npm install -S hexo-theme-icarus hexo-renderer-inferno <span class=\"comment\"># install icarus</span></span><br><span class=\"line\">hexo config theme icarus <span class=\"comment\"># use theme icarus</span></span><br><span class=\"line\">hexo server <span class=\"comment\"># start server at localhost</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Tips-about-Hexo-or-Icarus\"><a href=\"#Tips-about-Hexo-or-Icarus\" class=\"headerlink\" title=\"Tips about Hexo or Icarus\"></a>Tips about Hexo or Icarus</h2><ul>\n<li>Add <code>read more</code> to your blogs : just add <code>&lt;!-- more --&gt;</code> in your md at proper position.</li>\n<li>To search icons to use, visit <a href=\"https://fontawesome.com/\">here</a></li>\n</ul>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://hexo.io/zh-cn/docs/\">hexo-tutorial</a></li>\n<li><a href=\"https://ppoffice.github.io/hexo-theme-icarus/uncategorized/getting-started-with-icarus/\">getting-started-with-icarus</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>This web is building by Hexo and Icarus.</p>","more":"<h2 id=\"Install-Node-js\"><a href=\"#Install-Node-js\" class=\"headerlink\" title=\"Install Node.js\"></a>Install Node.js</h2><ul>\n<li>for linux <a href=\"https://github.com/nodesource/distributions\">reference this</a></li>\n<li>for windows through <a href=\"https://github.com/jasongin/nvs/\">nvs</a> or <a href=\"https://github.com/nvm-sh/nvm\">nvm</a></li>\n<li>for mac through <a href=\"https://brew.sh/\">Homebrew</a> or <a href=\"http://www.macports.org/\">MacPorts</a></li>\n</ul>\n<h2 id=\"Install-Hexo-require-Node-js\"><a href=\"#Install-Hexo-require-Node-js\" class=\"headerlink\" title=\"Install Hexo (require Node.js)\"></a>Install Hexo (require Node.js)</h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install -g hexo-cli</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Create-your-site\"><a href=\"#Create-your-site\" class=\"headerlink\" title=\"Create your site\"></a>Create your site</h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo init &lt;folder&gt; </span><br><span class=\"line\"><span class=\"built_in\">cd</span> &lt;folder&gt;</span><br><span class=\"line\">npm install</span><br><span class=\"line\">npm install -S hexo-theme-icarus hexo-renderer-inferno <span class=\"comment\"># install icarus</span></span><br><span class=\"line\">hexo config theme icarus <span class=\"comment\"># use theme icarus</span></span><br><span class=\"line\">hexo server <span class=\"comment\"># start server at localhost</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Tips-about-Hexo-or-Icarus\"><a href=\"#Tips-about-Hexo-or-Icarus\" class=\"headerlink\" title=\"Tips about Hexo or Icarus\"></a>Tips about Hexo or Icarus</h2><ul>\n<li>Add <code>read more</code> to your blogs : just add <code>&lt;!-- more --&gt;</code> in your md at proper position.</li>\n<li>To search icons to use, visit <a href=\"https://fontawesome.com/\">here</a></li>\n</ul>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://hexo.io/zh-cn/docs/\">hexo-tutorial</a></li>\n<li><a href=\"https://ppoffice.github.io/hexo-theme-icarus/uncategorized/getting-started-with-icarus/\">getting-started-with-icarus</a></li>\n</ul>"},{"title":"learn-cutlass-0","date":"2023-03-20T12:44:12.000Z","toc":true,"_content":"\n> learn cutlass is a series of tutorials to learn cutlass by reading its examples or source code\n\nCUTLASS is a header-only template library. After reading that, you will **be lost in templates**.\n<!-- more -->\n\n## 00_basic_gemm\n\n```c++\n// Defines cutlass::gemm::device::Gemm, the generic Gemm computation template class.\n#include \"cutlass/gemm/device/gemm.h\"\n\nusing CutlassGemm = cutlass::gemm::device::Gemm<A_TYPE,A_LAYOUT,B_TYPE,B_LAYOUT,C_TYPE,C_LAYOUT> ;\n// where A_TYPE is Data-type of A matrix and A_LAYOUT is Layout of A matrix\n\nCutlassGemm gemm_operator;\n\nCutlassGemm::Arguments args({M , N, K},  \n                            {A_POINTER, lda},    \n                            {B_POINTER, ldb},   \n                            {C_POINTER, ldc},    \n                            {C_POINTER, ldc},    \n                            {alpha, beta});\n// where A_POINTER is pointer of A matrix and lda is the number of elements between consecutive rows or colmns\n\ncutlass::Status status = gemm_operator(args);\n// call gemm operation\n\n```\n\n## 01_cutlass_utilities\n\n```c++\n// CUTLASS includes needed for half-precision GEMM kernel\n#include \"cutlass/cutlass.h\"\n#include \"cutlass/core_io.h\"\n#include \"cutlass/layout/matrix.h\"\n#include \"cutlass/gemm/device/gemm.h\"\n\n//\n// CUTLASS utility includes\n//\n\n// Defines operator<<() to write TensorView objects to std::ostream\n#include \"cutlass/util/tensor_view_io.h\"\n\n// Defines cutlass::HostTensor<>\n#include \"cutlass/util/host_tensor.h\"\n\n// Defines cutlass::half_t\n#include \"cutlass/numeric_types.h\"\n\n// Defines device_memory::copy_device_to_device()\n#include \"cutlass/util/device_memory.h\"\n\n// Defines cutlass::reference::device::TensorFillRandomGaussian()\n#include \"cutlass/util/reference/device/tensor_fill.h\"\n\n// Defines cutlass::reference::host::TensorEquals()\n#include \"cutlass/util/reference/host/tensor_compare.h\"\n\n// Defines cutlass::reference::host::Gemm()\n#include \"cutlass/util/reference/host/gemm.h\"\n\n\n// another way to call gemm without using Arguments\ncutlass::Status status = gemm_op({\n    {M, N, K},\n    {A, lda},\n    {B, ldb},\n    {C, ldc},\n    {C, ldc},\n    {alpha, beta}\n  });\n\n// define a tensor (M,N) in cutlass, where DTYPE is data type\ncutlass::HostTensor<DTYPE,LAYOUT> VAR(cutlass::MatrixCoord(M,N)) ;\ncutlass::HostTensor<cutlass::half_t, cutlass::layout::ColumnMajor> A(cutlass::MatrixCoord(M, K));\n\n// fill a tensor (RandomGaussian) where A.device_view() return TensorView of that tensor in cutlass\ncutlass::reference::device::TensorFillRandomGaussian(\n    A.device_view(),\n    seed,\n    mean,\n    stddev,\n    bits_less_than_one\n  );\n\n// copy data from device to device in cutlass where A.device_data() return pointer of that tensor\n// A.capacity() return the logical capacity based on extent and layout. May differ from size().\ncutlass::device_memory::copy_device_to_device(\n    C_reference.device_data(), \n    C_cutlass.device_data(), \n    C_cutlass.capacity());\n\n// Copies data from device to host\nA.sync_host();\n\n// Copies data from host to device\nA.sync_device();\n\n// Compute the reference result using the host-side GEMM reference implementation.\n// I think the only difference between TensorView and TensorRef is that TensorView is read-only \n// while TensorRef can return pointer of matrix\ncutlass::reference::host::Gemm<\n    cutlass::half_t,                           // ElementA\n    cutlass::layout::ColumnMajor,              // LayoutA\n    cutlass::half_t,                           // ElementB\n    cutlass::layout::ColumnMajor,              // LayoutB\n    cutlass::half_t,                           // ElementOutput\n    cutlass::layout::ColumnMajor,              // LayoutOutput\n    cutlass::half_t,                           // ScalarType\n    cutlass::half_t                            // ComputeType\n> gemm_ref;\n\ngemm_ref(\n    {M, N, K},                          // problem size (type: cutlass::gemm::GemmCoord)\n    alpha,                              // alpha        (type: cutlass::half_t)\n    A.host_ref(),                       // A            (type: TensorRef<half_t, ColumnMajor>)\n    B.host_ref(),                       // B            (type: TensorRef<half_t, ColumnMajor>)\n    beta,                               // beta         (type: cutlass::half_t)\n    C_reference.host_ref()              // C            (type: TensorRef<half_t, ColumnMajor>)\n);\n\n// Compare reference to computed results\ncutlass::reference::host::TensorEquals(\n    C_reference.host_view(), \n    C_cutlass.host_view());\n```\n\n## 04_tile_iterator\n\n```c++\n// CUTLASS includes\n#include \"cutlass/transform/threadblock/predicated_tile_iterator.h\"\n#include \"cutlass/layout/pitch_linear.h\"\n#include \"cutlass/transform/pitch_linear_thread_map.h\"\n\n//\n//  CUTLASS utility includes\n//\n\n// Defines operator<<() to write TensorView objects to std::ostream\n#include \"cutlass/util/tensor_view_io.h\"\n\n// Defines cutlass::HostTensor<>\n#include \"cutlass/util/host_tensor.h\"\n\n// Defines cutlass::reference::host::TensorFill() and\n// cutlass::reference::host::TensorFillBlockSequential()\n#include \"cutlass/util/reference/host/tensor_fill.h\"\n\n\n// For this example, we chose a <64, 4> tile shape. The PredicateTileIterator expects\n// PitchLinearShape and PitchLinear layout.\nusing Shape = cutlass::layout::PitchLinearShape<64, 4>;\nusing Layout = cutlass::layout::PitchLinear;\nusing Element = int;\nint const kThreads = 32;\n\n// ThreadMaps define how threads are mapped to a given tile. The PitchLinearStripminedThreadMap\n// stripmines a pitch-linear tile among a given number of threads, first along the contiguous\n// dimension then along the strided dimension.\nusing ThreadMap = cutlass::transform::PitchLinearStripminedThreadMap<Shape, kThreads>;\n\n// Define the PredicateTileIterator, using TileShape, Element, Layout, and ThreadMap types\nusing Iterator = cutlass::transform::threadblock::PredicatedTileIterator<\n    Shape, Element, Layout, 1, ThreadMap>;\n\n\ncutlass::Coord<2> copy_extent = cutlass::make_Coord(M, K);\ncutlass::Coord<2> alloc_extent = cutlass::make_Coord(M, K);\n\n// another way to define tensor\n// Allocate source and destination tensors\ncutlass::HostTensor<Element, Layout> src_tensor(alloc_extent);\ncutlass::HostTensor<Element, Layout> dst_tensor(alloc_extent);\n\nElement oob_value = Element(-1);\n\n// Initialize destination tensor with all -1s\ncutlass::reference::host::TensorFill(dst_tensor.host_view(), oob_value);\n// Initialize source tensor with sequentially increasing values\ncutlass::reference::host::BlockFillSequential(src_tensor.host_data(), src_tensor.capacity());\n\ndst_tensor.sync_device();\nsrc_tensor.sync_device();\n\ntypename Iterator::Params dst_params(dst_tensor.layout());\ntypename Iterator::Params src_params(src_tensor.layout());\n\ndim3 block(kThreads, 1);\ndim3 grid(1, 1);\n\n// Launch copy kernel to perform the copy\ncopy<Iterator><<< grid, block >>>(\n        dst_params,\n        dst_tensor.device_data(),\n        src_params,\n        src_tensor.device_data(),\n        copy_extent\n);\n\n// copy function\ntemplate <typename Iterator>\n__global__ void copy(\n    typename Iterator::Params dst_params,\n    typename Iterator::Element *dst_pointer,\n    typename Iterator::Params src_params,\n    typename Iterator::Element *src_pointer,\n    cutlass::Coord<2> extent) {\n\n    Iterator dst_iterator(dst_params, dst_pointer, extent, threadIdx.x);\n    Iterator src_iterator(src_params, src_pointer, extent, threadIdx.x);\n\n    // PredicatedTileIterator uses PitchLinear layout and therefore takes in a PitchLinearShape.\n    // The contiguous dimension can be accessed via Iterator::Shape::kContiguous and the strided\n    // dimension can be accessed via Iterator::Shape::kStrided\n    int iterations = (extent[1] + Iterator::Shape::kStrided - 1) / Iterator::Shape::kStrided;\n\n    typename Iterator::Fragment fragment;\n\n    for(; iterations > 0; --iterations) {\n      src_iterator.load(fragment);\n      dst_iterator.store(fragment);\n\n      ++src_iterator;\n      ++dst_iterator;\n    }\n}\n```\n\n\n\n\n\n","source":"_posts/learn-cutlass-0.md","raw":"---\ntitle: learn-cutlass-0\ndate: 2023-03-20 20:44:12\ncategories: \n- Technology\ntags:\n- cutlass\ntoc: true\n---\n\n> learn cutlass is a series of tutorials to learn cutlass by reading its examples or source code\n\nCUTLASS is a header-only template library. After reading that, you will **be lost in templates**.\n<!-- more -->\n\n## 00_basic_gemm\n\n```c++\n// Defines cutlass::gemm::device::Gemm, the generic Gemm computation template class.\n#include \"cutlass/gemm/device/gemm.h\"\n\nusing CutlassGemm = cutlass::gemm::device::Gemm<A_TYPE,A_LAYOUT,B_TYPE,B_LAYOUT,C_TYPE,C_LAYOUT> ;\n// where A_TYPE is Data-type of A matrix and A_LAYOUT is Layout of A matrix\n\nCutlassGemm gemm_operator;\n\nCutlassGemm::Arguments args({M , N, K},  \n                            {A_POINTER, lda},    \n                            {B_POINTER, ldb},   \n                            {C_POINTER, ldc},    \n                            {C_POINTER, ldc},    \n                            {alpha, beta});\n// where A_POINTER is pointer of A matrix and lda is the number of elements between consecutive rows or colmns\n\ncutlass::Status status = gemm_operator(args);\n// call gemm operation\n\n```\n\n## 01_cutlass_utilities\n\n```c++\n// CUTLASS includes needed for half-precision GEMM kernel\n#include \"cutlass/cutlass.h\"\n#include \"cutlass/core_io.h\"\n#include \"cutlass/layout/matrix.h\"\n#include \"cutlass/gemm/device/gemm.h\"\n\n//\n// CUTLASS utility includes\n//\n\n// Defines operator<<() to write TensorView objects to std::ostream\n#include \"cutlass/util/tensor_view_io.h\"\n\n// Defines cutlass::HostTensor<>\n#include \"cutlass/util/host_tensor.h\"\n\n// Defines cutlass::half_t\n#include \"cutlass/numeric_types.h\"\n\n// Defines device_memory::copy_device_to_device()\n#include \"cutlass/util/device_memory.h\"\n\n// Defines cutlass::reference::device::TensorFillRandomGaussian()\n#include \"cutlass/util/reference/device/tensor_fill.h\"\n\n// Defines cutlass::reference::host::TensorEquals()\n#include \"cutlass/util/reference/host/tensor_compare.h\"\n\n// Defines cutlass::reference::host::Gemm()\n#include \"cutlass/util/reference/host/gemm.h\"\n\n\n// another way to call gemm without using Arguments\ncutlass::Status status = gemm_op({\n    {M, N, K},\n    {A, lda},\n    {B, ldb},\n    {C, ldc},\n    {C, ldc},\n    {alpha, beta}\n  });\n\n// define a tensor (M,N) in cutlass, where DTYPE is data type\ncutlass::HostTensor<DTYPE,LAYOUT> VAR(cutlass::MatrixCoord(M,N)) ;\ncutlass::HostTensor<cutlass::half_t, cutlass::layout::ColumnMajor> A(cutlass::MatrixCoord(M, K));\n\n// fill a tensor (RandomGaussian) where A.device_view() return TensorView of that tensor in cutlass\ncutlass::reference::device::TensorFillRandomGaussian(\n    A.device_view(),\n    seed,\n    mean,\n    stddev,\n    bits_less_than_one\n  );\n\n// copy data from device to device in cutlass where A.device_data() return pointer of that tensor\n// A.capacity() return the logical capacity based on extent and layout. May differ from size().\ncutlass::device_memory::copy_device_to_device(\n    C_reference.device_data(), \n    C_cutlass.device_data(), \n    C_cutlass.capacity());\n\n// Copies data from device to host\nA.sync_host();\n\n// Copies data from host to device\nA.sync_device();\n\n// Compute the reference result using the host-side GEMM reference implementation.\n// I think the only difference between TensorView and TensorRef is that TensorView is read-only \n// while TensorRef can return pointer of matrix\ncutlass::reference::host::Gemm<\n    cutlass::half_t,                           // ElementA\n    cutlass::layout::ColumnMajor,              // LayoutA\n    cutlass::half_t,                           // ElementB\n    cutlass::layout::ColumnMajor,              // LayoutB\n    cutlass::half_t,                           // ElementOutput\n    cutlass::layout::ColumnMajor,              // LayoutOutput\n    cutlass::half_t,                           // ScalarType\n    cutlass::half_t                            // ComputeType\n> gemm_ref;\n\ngemm_ref(\n    {M, N, K},                          // problem size (type: cutlass::gemm::GemmCoord)\n    alpha,                              // alpha        (type: cutlass::half_t)\n    A.host_ref(),                       // A            (type: TensorRef<half_t, ColumnMajor>)\n    B.host_ref(),                       // B            (type: TensorRef<half_t, ColumnMajor>)\n    beta,                               // beta         (type: cutlass::half_t)\n    C_reference.host_ref()              // C            (type: TensorRef<half_t, ColumnMajor>)\n);\n\n// Compare reference to computed results\ncutlass::reference::host::TensorEquals(\n    C_reference.host_view(), \n    C_cutlass.host_view());\n```\n\n## 04_tile_iterator\n\n```c++\n// CUTLASS includes\n#include \"cutlass/transform/threadblock/predicated_tile_iterator.h\"\n#include \"cutlass/layout/pitch_linear.h\"\n#include \"cutlass/transform/pitch_linear_thread_map.h\"\n\n//\n//  CUTLASS utility includes\n//\n\n// Defines operator<<() to write TensorView objects to std::ostream\n#include \"cutlass/util/tensor_view_io.h\"\n\n// Defines cutlass::HostTensor<>\n#include \"cutlass/util/host_tensor.h\"\n\n// Defines cutlass::reference::host::TensorFill() and\n// cutlass::reference::host::TensorFillBlockSequential()\n#include \"cutlass/util/reference/host/tensor_fill.h\"\n\n\n// For this example, we chose a <64, 4> tile shape. The PredicateTileIterator expects\n// PitchLinearShape and PitchLinear layout.\nusing Shape = cutlass::layout::PitchLinearShape<64, 4>;\nusing Layout = cutlass::layout::PitchLinear;\nusing Element = int;\nint const kThreads = 32;\n\n// ThreadMaps define how threads are mapped to a given tile. The PitchLinearStripminedThreadMap\n// stripmines a pitch-linear tile among a given number of threads, first along the contiguous\n// dimension then along the strided dimension.\nusing ThreadMap = cutlass::transform::PitchLinearStripminedThreadMap<Shape, kThreads>;\n\n// Define the PredicateTileIterator, using TileShape, Element, Layout, and ThreadMap types\nusing Iterator = cutlass::transform::threadblock::PredicatedTileIterator<\n    Shape, Element, Layout, 1, ThreadMap>;\n\n\ncutlass::Coord<2> copy_extent = cutlass::make_Coord(M, K);\ncutlass::Coord<2> alloc_extent = cutlass::make_Coord(M, K);\n\n// another way to define tensor\n// Allocate source and destination tensors\ncutlass::HostTensor<Element, Layout> src_tensor(alloc_extent);\ncutlass::HostTensor<Element, Layout> dst_tensor(alloc_extent);\n\nElement oob_value = Element(-1);\n\n// Initialize destination tensor with all -1s\ncutlass::reference::host::TensorFill(dst_tensor.host_view(), oob_value);\n// Initialize source tensor with sequentially increasing values\ncutlass::reference::host::BlockFillSequential(src_tensor.host_data(), src_tensor.capacity());\n\ndst_tensor.sync_device();\nsrc_tensor.sync_device();\n\ntypename Iterator::Params dst_params(dst_tensor.layout());\ntypename Iterator::Params src_params(src_tensor.layout());\n\ndim3 block(kThreads, 1);\ndim3 grid(1, 1);\n\n// Launch copy kernel to perform the copy\ncopy<Iterator><<< grid, block >>>(\n        dst_params,\n        dst_tensor.device_data(),\n        src_params,\n        src_tensor.device_data(),\n        copy_extent\n);\n\n// copy function\ntemplate <typename Iterator>\n__global__ void copy(\n    typename Iterator::Params dst_params,\n    typename Iterator::Element *dst_pointer,\n    typename Iterator::Params src_params,\n    typename Iterator::Element *src_pointer,\n    cutlass::Coord<2> extent) {\n\n    Iterator dst_iterator(dst_params, dst_pointer, extent, threadIdx.x);\n    Iterator src_iterator(src_params, src_pointer, extent, threadIdx.x);\n\n    // PredicatedTileIterator uses PitchLinear layout and therefore takes in a PitchLinearShape.\n    // The contiguous dimension can be accessed via Iterator::Shape::kContiguous and the strided\n    // dimension can be accessed via Iterator::Shape::kStrided\n    int iterations = (extent[1] + Iterator::Shape::kStrided - 1) / Iterator::Shape::kStrided;\n\n    typename Iterator::Fragment fragment;\n\n    for(; iterations > 0; --iterations) {\n      src_iterator.load(fragment);\n      dst_iterator.store(fragment);\n\n      ++src_iterator;\n      ++dst_iterator;\n    }\n}\n```\n\n\n\n\n\n","slug":"learn-cutlass-0","published":1,"updated":"2023-03-21T04:38:26.306Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clfm5kvrl0005717v7lch1xqa","content":"<blockquote>\n<p>learn cutlass is a series of tutorials to learn cutlass by reading its examples or source code</p>\n</blockquote>\n<p>CUTLASS is a header-only template library. After reading that, you will <strong>be lost in templates</strong>.</p>\n<span id=\"more\"></span>\n\n<h2 id=\"00-basic-gemm\"><a href=\"#00-basic-gemm\" class=\"headerlink\" title=\"00_basic_gemm\"></a>00_basic_gemm</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Defines cutlass::gemm::device::Gemm, the generic Gemm computation template class.</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/gemm/device/gemm.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> CutlassGemm = cutlass::gemm::device::Gemm&lt;A_TYPE,A_LAYOUT,B_TYPE,B_LAYOUT,C_TYPE,C_LAYOUT&gt; ;</span><br><span class=\"line\"><span class=\"comment\">// where A_TYPE is Data-type of A matrix and A_LAYOUT is Layout of A matrix</span></span><br><span class=\"line\"></span><br><span class=\"line\">CutlassGemm gemm_operator;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\">CutlassGemm::Arguments <span class=\"title\">args</span><span class=\"params\">(&#123;M , N, K&#125;,  </span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">                            &#123;A_POINTER, lda&#125;,    </span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">                            &#123;B_POINTER, ldb&#125;,   </span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">                            &#123;C_POINTER, ldc&#125;,    </span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">                            &#123;C_POINTER, ldc&#125;,    </span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">                            &#123;alpha, beta&#125;)</span></span>;</span><br><span class=\"line\"><span class=\"comment\">// where A_POINTER is pointer of A matrix and lda is the number of elements between consecutive rows or colmns</span></span><br><span class=\"line\"></span><br><span class=\"line\">cutlass::Status status = <span class=\"built_in\">gemm_operator</span>(args);</span><br><span class=\"line\"><span class=\"comment\">// call gemm operation</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"01-cutlass-utilities\"><a href=\"#01-cutlass-utilities\" class=\"headerlink\" title=\"01_cutlass_utilities\"></a>01_cutlass_utilities</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// CUTLASS includes needed for half-precision GEMM kernel</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/cutlass.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/core_io.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/layout/matrix.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/gemm/device/gemm.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"><span class=\"comment\">// CUTLASS utility includes</span></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines operator&lt;&lt;() to write TensorView objects to std::ostream</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/tensor_view_io.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::HostTensor&lt;&gt;</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/host_tensor.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::half_t</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/numeric_types.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines device_memory::copy_device_to_device()</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/device_memory.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::reference::device::TensorFillRandomGaussian()</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/reference/device/tensor_fill.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::reference::host::TensorEquals()</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/reference/host/tensor_compare.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::reference::host::Gemm()</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/reference/host/gemm.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// another way to call gemm without using Arguments</span></span><br><span class=\"line\">cutlass::Status status = <span class=\"built_in\">gemm_op</span>(&#123;</span><br><span class=\"line\">    &#123;M, N, K&#125;,</span><br><span class=\"line\">    &#123;A, lda&#125;,</span><br><span class=\"line\">    &#123;B, ldb&#125;,</span><br><span class=\"line\">    &#123;C, ldc&#125;,</span><br><span class=\"line\">    &#123;C, ldc&#125;,</span><br><span class=\"line\">    &#123;alpha, beta&#125;</span><br><span class=\"line\">  &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// define a tensor (M,N) in cutlass, where DTYPE is data type</span></span><br><span class=\"line\"><span class=\"function\">cutlass::HostTensor&lt;DTYPE,LAYOUT&gt; <span class=\"title\">VAR</span><span class=\"params\">(cutlass::MatrixCoord(M,N))</span> </span>;</span><br><span class=\"line\"><span class=\"function\">cutlass::HostTensor&lt;cutlass::<span class=\"type\">half_t</span>, cutlass::layout::ColumnMajor&gt; <span class=\"title\">A</span><span class=\"params\">(cutlass::MatrixCoord(M, K))</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// fill a tensor (RandomGaussian) where A.device_view() return TensorView of that tensor in cutlass</span></span><br><span class=\"line\">cutlass::reference::device::<span class=\"built_in\">TensorFillRandomGaussian</span>(</span><br><span class=\"line\">    A.<span class=\"built_in\">device_view</span>(),</span><br><span class=\"line\">    seed,</span><br><span class=\"line\">    mean,</span><br><span class=\"line\">    stddev,</span><br><span class=\"line\">    bits_less_than_one</span><br><span class=\"line\">  );</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// copy data from device to device in cutlass where A.device_data() return pointer of that tensor</span></span><br><span class=\"line\"><span class=\"comment\">// A.capacity() return the logical capacity based on extent and layout. May differ from size().</span></span><br><span class=\"line\">cutlass::device_memory::<span class=\"built_in\">copy_device_to_device</span>(</span><br><span class=\"line\">    C_reference.<span class=\"built_in\">device_data</span>(), </span><br><span class=\"line\">    C_cutlass.<span class=\"built_in\">device_data</span>(), </span><br><span class=\"line\">    C_cutlass.<span class=\"built_in\">capacity</span>());</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Copies data from device to host</span></span><br><span class=\"line\">A.<span class=\"built_in\">sync_host</span>();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Copies data from host to device</span></span><br><span class=\"line\">A.<span class=\"built_in\">sync_device</span>();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Compute the reference result using the host-side GEMM reference implementation.</span></span><br><span class=\"line\"><span class=\"comment\">// I think the only difference between TensorView and TensorRef is that TensorView is read-only </span></span><br><span class=\"line\"><span class=\"comment\">// while TensorRef can return pointer of matrix</span></span><br><span class=\"line\">cutlass::reference::host::Gemm&lt;</span><br><span class=\"line\">    cutlass::<span class=\"type\">half_t</span>,                           <span class=\"comment\">// ElementA</span></span><br><span class=\"line\">    cutlass::layout::ColumnMajor,              <span class=\"comment\">// LayoutA</span></span><br><span class=\"line\">    cutlass::<span class=\"type\">half_t</span>,                           <span class=\"comment\">// ElementB</span></span><br><span class=\"line\">    cutlass::layout::ColumnMajor,              <span class=\"comment\">// LayoutB</span></span><br><span class=\"line\">    cutlass::<span class=\"type\">half_t</span>,                           <span class=\"comment\">// ElementOutput</span></span><br><span class=\"line\">    cutlass::layout::ColumnMajor,              <span class=\"comment\">// LayoutOutput</span></span><br><span class=\"line\">    cutlass::<span class=\"type\">half_t</span>,                           <span class=\"comment\">// ScalarType</span></span><br><span class=\"line\">    cutlass::<span class=\"type\">half_t</span>                            <span class=\"comment\">// ComputeType</span></span><br><span class=\"line\">&gt; gemm_ref;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">gemm_ref</span>(</span><br><span class=\"line\">    &#123;M, N, K&#125;,                          <span class=\"comment\">// problem size (type: cutlass::gemm::GemmCoord)</span></span><br><span class=\"line\">    alpha,                              <span class=\"comment\">// alpha        (type: cutlass::half_t)</span></span><br><span class=\"line\">    A.<span class=\"built_in\">host_ref</span>(),                       <span class=\"comment\">// A            (type: TensorRef&lt;half_t, ColumnMajor&gt;)</span></span><br><span class=\"line\">    B.<span class=\"built_in\">host_ref</span>(),                       <span class=\"comment\">// B            (type: TensorRef&lt;half_t, ColumnMajor&gt;)</span></span><br><span class=\"line\">    beta,                               <span class=\"comment\">// beta         (type: cutlass::half_t)</span></span><br><span class=\"line\">    C_reference.<span class=\"built_in\">host_ref</span>()              <span class=\"comment\">// C            (type: TensorRef&lt;half_t, ColumnMajor&gt;)</span></span><br><span class=\"line\">);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Compare reference to computed results</span></span><br><span class=\"line\">cutlass::reference::host::<span class=\"built_in\">TensorEquals</span>(</span><br><span class=\"line\">    C_reference.<span class=\"built_in\">host_view</span>(), </span><br><span class=\"line\">    C_cutlass.<span class=\"built_in\">host_view</span>());</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"04-tile-iterator\"><a href=\"#04-tile-iterator\" class=\"headerlink\" title=\"04_tile_iterator\"></a>04_tile_iterator</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// CUTLASS includes</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/transform/threadblock/predicated_tile_iterator.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/layout/pitch_linear.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/transform/pitch_linear_thread_map.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"><span class=\"comment\">//  CUTLASS utility includes</span></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines operator&lt;&lt;() to write TensorView objects to std::ostream</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/tensor_view_io.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::HostTensor&lt;&gt;</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/host_tensor.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::reference::host::TensorFill() and</span></span><br><span class=\"line\"><span class=\"comment\">// cutlass::reference::host::TensorFillBlockSequential()</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/reference/host/tensor_fill.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// For this example, we chose a &lt;64, 4&gt; tile shape. The PredicateTileIterator expects</span></span><br><span class=\"line\"><span class=\"comment\">// PitchLinearShape and PitchLinear layout.</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> Shape = cutlass::layout::PitchLinearShape&lt;<span class=\"number\">64</span>, <span class=\"number\">4</span>&gt;;</span><br><span class=\"line\"><span class=\"keyword\">using</span> Layout = cutlass::layout::PitchLinear;</span><br><span class=\"line\"><span class=\"keyword\">using</span> Element = <span class=\"type\">int</span>;</span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"type\">const</span> kThreads = <span class=\"number\">32</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// ThreadMaps define how threads are mapped to a given tile. The PitchLinearStripminedThreadMap</span></span><br><span class=\"line\"><span class=\"comment\">// stripmines a pitch-linear tile among a given number of threads, first along the contiguous</span></span><br><span class=\"line\"><span class=\"comment\">// dimension then along the strided dimension.</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> ThreadMap = cutlass::transform::PitchLinearStripminedThreadMap&lt;Shape, kThreads&gt;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Define the PredicateTileIterator, using TileShape, Element, Layout, and ThreadMap types</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> Iterator = cutlass::transform::threadblock::PredicatedTileIterator&lt;</span><br><span class=\"line\">    Shape, Element, Layout, <span class=\"number\">1</span>, ThreadMap&gt;;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">cutlass::Coord&lt;<span class=\"number\">2</span>&gt; copy_extent = cutlass::<span class=\"built_in\">make_Coord</span>(M, K);</span><br><span class=\"line\">cutlass::Coord&lt;<span class=\"number\">2</span>&gt; alloc_extent = cutlass::<span class=\"built_in\">make_Coord</span>(M, K);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// another way to define tensor</span></span><br><span class=\"line\"><span class=\"comment\">// Allocate source and destination tensors</span></span><br><span class=\"line\"><span class=\"function\">cutlass::HostTensor&lt;Element, Layout&gt; <span class=\"title\">src_tensor</span><span class=\"params\">(alloc_extent)</span></span>;</span><br><span class=\"line\"><span class=\"function\">cutlass::HostTensor&lt;Element, Layout&gt; <span class=\"title\">dst_tensor</span><span class=\"params\">(alloc_extent)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">Element oob_value = <span class=\"built_in\">Element</span>(<span class=\"number\">-1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Initialize destination tensor with all -1s</span></span><br><span class=\"line\">cutlass::reference::host::<span class=\"built_in\">TensorFill</span>(dst_tensor.<span class=\"built_in\">host_view</span>(), oob_value);</span><br><span class=\"line\"><span class=\"comment\">// Initialize source tensor with sequentially increasing values</span></span><br><span class=\"line\">cutlass::reference::host::<span class=\"built_in\">BlockFillSequential</span>(src_tensor.<span class=\"built_in\">host_data</span>(), src_tensor.<span class=\"built_in\">capacity</span>());</span><br><span class=\"line\"></span><br><span class=\"line\">dst_tensor.<span class=\"built_in\">sync_device</span>();</span><br><span class=\"line\">src_tensor.<span class=\"built_in\">sync_device</span>();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">typename</span> Iterator::Params <span class=\"title\">dst_params</span><span class=\"params\">(dst_tensor.layout())</span></span>;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">typename</span> Iterator::Params <span class=\"title\">src_params</span><span class=\"params\">(src_tensor.layout())</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\">dim3 <span class=\"title\">block</span><span class=\"params\">(kThreads, <span class=\"number\">1</span>)</span></span>;</span><br><span class=\"line\"><span class=\"function\">dim3 <span class=\"title\">grid</span><span class=\"params\">(<span class=\"number\">1</span>, <span class=\"number\">1</span>)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Launch copy kernel to perform the copy</span></span><br><span class=\"line\">copy&lt;Iterator&gt;&lt;&lt;&lt; grid, block &gt;&gt;&gt;(</span><br><span class=\"line\">        dst_params,</span><br><span class=\"line\">        dst_tensor.<span class=\"built_in\">device_data</span>(),</span><br><span class=\"line\">        src_params,</span><br><span class=\"line\">        src_tensor.<span class=\"built_in\">device_data</span>(),</span><br><span class=\"line\">        copy_extent</span><br><span class=\"line\">);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// copy function</span></span><br><span class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Iterator&gt;</span><br><span class=\"line\"><span class=\"function\">__global__ <span class=\"type\">void</span> <span class=\"title\">copy</span><span class=\"params\">(</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">    <span class=\"keyword\">typename</span> Iterator::Params dst_params,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">    <span class=\"keyword\">typename</span> Iterator::Element *dst_pointer,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">    <span class=\"keyword\">typename</span> Iterator::Params src_params,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">    <span class=\"keyword\">typename</span> Iterator::Element *src_pointer,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">    cutlass::Coord&lt;<span class=\"number\">2</span>&gt; extent)</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\">Iterator <span class=\"title\">dst_iterator</span><span class=\"params\">(dst_params, dst_pointer, extent, threadIdx.x)</span></span>;</span><br><span class=\"line\">    <span class=\"function\">Iterator <span class=\"title\">src_iterator</span><span class=\"params\">(src_params, src_pointer, extent, threadIdx.x)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// PredicatedTileIterator uses PitchLinear layout and therefore takes in a PitchLinearShape.</span></span><br><span class=\"line\">    <span class=\"comment\">// The contiguous dimension can be accessed via Iterator::Shape::kContiguous and the strided</span></span><br><span class=\"line\">    <span class=\"comment\">// dimension can be accessed via Iterator::Shape::kStrided</span></span><br><span class=\"line\">    <span class=\"type\">int</span> iterations = (extent[<span class=\"number\">1</span>] + Iterator::Shape::kStrided - <span class=\"number\">1</span>) / Iterator::Shape::kStrided;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">typename</span> Iterator::Fragment fragment;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(; iterations &gt; <span class=\"number\">0</span>; --iterations) &#123;</span><br><span class=\"line\">      src_iterator.<span class=\"built_in\">load</span>(fragment);</span><br><span class=\"line\">      dst_iterator.<span class=\"built_in\">store</span>(fragment);</span><br><span class=\"line\"></span><br><span class=\"line\">      ++src_iterator;</span><br><span class=\"line\">      ++dst_iterator;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n\n\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>learn cutlass is a series of tutorials to learn cutlass by reading its examples or source code</p>\n</blockquote>\n<p>CUTLASS is a header-only template library. After reading that, you will <strong>be lost in templates</strong>.</p>","more":"<h2 id=\"00-basic-gemm\"><a href=\"#00-basic-gemm\" class=\"headerlink\" title=\"00_basic_gemm\"></a>00_basic_gemm</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Defines cutlass::gemm::device::Gemm, the generic Gemm computation template class.</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/gemm/device/gemm.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> CutlassGemm = cutlass::gemm::device::Gemm&lt;A_TYPE,A_LAYOUT,B_TYPE,B_LAYOUT,C_TYPE,C_LAYOUT&gt; ;</span><br><span class=\"line\"><span class=\"comment\">// where A_TYPE is Data-type of A matrix and A_LAYOUT is Layout of A matrix</span></span><br><span class=\"line\"></span><br><span class=\"line\">CutlassGemm gemm_operator;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\">CutlassGemm::Arguments <span class=\"title\">args</span><span class=\"params\">(&#123;M , N, K&#125;,  </span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">                            &#123;A_POINTER, lda&#125;,    </span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">                            &#123;B_POINTER, ldb&#125;,   </span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">                            &#123;C_POINTER, ldc&#125;,    </span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">                            &#123;C_POINTER, ldc&#125;,    </span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">                            &#123;alpha, beta&#125;)</span></span>;</span><br><span class=\"line\"><span class=\"comment\">// where A_POINTER is pointer of A matrix and lda is the number of elements between consecutive rows or colmns</span></span><br><span class=\"line\"></span><br><span class=\"line\">cutlass::Status status = <span class=\"built_in\">gemm_operator</span>(args);</span><br><span class=\"line\"><span class=\"comment\">// call gemm operation</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"01-cutlass-utilities\"><a href=\"#01-cutlass-utilities\" class=\"headerlink\" title=\"01_cutlass_utilities\"></a>01_cutlass_utilities</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// CUTLASS includes needed for half-precision GEMM kernel</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/cutlass.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/core_io.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/layout/matrix.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/gemm/device/gemm.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"><span class=\"comment\">// CUTLASS utility includes</span></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines operator&lt;&lt;() to write TensorView objects to std::ostream</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/tensor_view_io.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::HostTensor&lt;&gt;</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/host_tensor.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::half_t</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/numeric_types.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines device_memory::copy_device_to_device()</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/device_memory.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::reference::device::TensorFillRandomGaussian()</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/reference/device/tensor_fill.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::reference::host::TensorEquals()</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/reference/host/tensor_compare.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::reference::host::Gemm()</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/reference/host/gemm.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// another way to call gemm without using Arguments</span></span><br><span class=\"line\">cutlass::Status status = <span class=\"built_in\">gemm_op</span>(&#123;</span><br><span class=\"line\">    &#123;M, N, K&#125;,</span><br><span class=\"line\">    &#123;A, lda&#125;,</span><br><span class=\"line\">    &#123;B, ldb&#125;,</span><br><span class=\"line\">    &#123;C, ldc&#125;,</span><br><span class=\"line\">    &#123;C, ldc&#125;,</span><br><span class=\"line\">    &#123;alpha, beta&#125;</span><br><span class=\"line\">  &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// define a tensor (M,N) in cutlass, where DTYPE is data type</span></span><br><span class=\"line\"><span class=\"function\">cutlass::HostTensor&lt;DTYPE,LAYOUT&gt; <span class=\"title\">VAR</span><span class=\"params\">(cutlass::MatrixCoord(M,N))</span> </span>;</span><br><span class=\"line\"><span class=\"function\">cutlass::HostTensor&lt;cutlass::<span class=\"type\">half_t</span>, cutlass::layout::ColumnMajor&gt; <span class=\"title\">A</span><span class=\"params\">(cutlass::MatrixCoord(M, K))</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// fill a tensor (RandomGaussian) where A.device_view() return TensorView of that tensor in cutlass</span></span><br><span class=\"line\">cutlass::reference::device::<span class=\"built_in\">TensorFillRandomGaussian</span>(</span><br><span class=\"line\">    A.<span class=\"built_in\">device_view</span>(),</span><br><span class=\"line\">    seed,</span><br><span class=\"line\">    mean,</span><br><span class=\"line\">    stddev,</span><br><span class=\"line\">    bits_less_than_one</span><br><span class=\"line\">  );</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// copy data from device to device in cutlass where A.device_data() return pointer of that tensor</span></span><br><span class=\"line\"><span class=\"comment\">// A.capacity() return the logical capacity based on extent and layout. May differ from size().</span></span><br><span class=\"line\">cutlass::device_memory::<span class=\"built_in\">copy_device_to_device</span>(</span><br><span class=\"line\">    C_reference.<span class=\"built_in\">device_data</span>(), </span><br><span class=\"line\">    C_cutlass.<span class=\"built_in\">device_data</span>(), </span><br><span class=\"line\">    C_cutlass.<span class=\"built_in\">capacity</span>());</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Copies data from device to host</span></span><br><span class=\"line\">A.<span class=\"built_in\">sync_host</span>();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Copies data from host to device</span></span><br><span class=\"line\">A.<span class=\"built_in\">sync_device</span>();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Compute the reference result using the host-side GEMM reference implementation.</span></span><br><span class=\"line\"><span class=\"comment\">// I think the only difference between TensorView and TensorRef is that TensorView is read-only </span></span><br><span class=\"line\"><span class=\"comment\">// while TensorRef can return pointer of matrix</span></span><br><span class=\"line\">cutlass::reference::host::Gemm&lt;</span><br><span class=\"line\">    cutlass::<span class=\"type\">half_t</span>,                           <span class=\"comment\">// ElementA</span></span><br><span class=\"line\">    cutlass::layout::ColumnMajor,              <span class=\"comment\">// LayoutA</span></span><br><span class=\"line\">    cutlass::<span class=\"type\">half_t</span>,                           <span class=\"comment\">// ElementB</span></span><br><span class=\"line\">    cutlass::layout::ColumnMajor,              <span class=\"comment\">// LayoutB</span></span><br><span class=\"line\">    cutlass::<span class=\"type\">half_t</span>,                           <span class=\"comment\">// ElementOutput</span></span><br><span class=\"line\">    cutlass::layout::ColumnMajor,              <span class=\"comment\">// LayoutOutput</span></span><br><span class=\"line\">    cutlass::<span class=\"type\">half_t</span>,                           <span class=\"comment\">// ScalarType</span></span><br><span class=\"line\">    cutlass::<span class=\"type\">half_t</span>                            <span class=\"comment\">// ComputeType</span></span><br><span class=\"line\">&gt; gemm_ref;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">gemm_ref</span>(</span><br><span class=\"line\">    &#123;M, N, K&#125;,                          <span class=\"comment\">// problem size (type: cutlass::gemm::GemmCoord)</span></span><br><span class=\"line\">    alpha,                              <span class=\"comment\">// alpha        (type: cutlass::half_t)</span></span><br><span class=\"line\">    A.<span class=\"built_in\">host_ref</span>(),                       <span class=\"comment\">// A            (type: TensorRef&lt;half_t, ColumnMajor&gt;)</span></span><br><span class=\"line\">    B.<span class=\"built_in\">host_ref</span>(),                       <span class=\"comment\">// B            (type: TensorRef&lt;half_t, ColumnMajor&gt;)</span></span><br><span class=\"line\">    beta,                               <span class=\"comment\">// beta         (type: cutlass::half_t)</span></span><br><span class=\"line\">    C_reference.<span class=\"built_in\">host_ref</span>()              <span class=\"comment\">// C            (type: TensorRef&lt;half_t, ColumnMajor&gt;)</span></span><br><span class=\"line\">);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Compare reference to computed results</span></span><br><span class=\"line\">cutlass::reference::host::<span class=\"built_in\">TensorEquals</span>(</span><br><span class=\"line\">    C_reference.<span class=\"built_in\">host_view</span>(), </span><br><span class=\"line\">    C_cutlass.<span class=\"built_in\">host_view</span>());</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"04-tile-iterator\"><a href=\"#04-tile-iterator\" class=\"headerlink\" title=\"04_tile_iterator\"></a>04_tile_iterator</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// CUTLASS includes</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/transform/threadblock/predicated_tile_iterator.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/layout/pitch_linear.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/transform/pitch_linear_thread_map.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"><span class=\"comment\">//  CUTLASS utility includes</span></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines operator&lt;&lt;() to write TensorView objects to std::ostream</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/tensor_view_io.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::HostTensor&lt;&gt;</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/host_tensor.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::reference::host::TensorFill() and</span></span><br><span class=\"line\"><span class=\"comment\">// cutlass::reference::host::TensorFillBlockSequential()</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/reference/host/tensor_fill.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// For this example, we chose a &lt;64, 4&gt; tile shape. The PredicateTileIterator expects</span></span><br><span class=\"line\"><span class=\"comment\">// PitchLinearShape and PitchLinear layout.</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> Shape = cutlass::layout::PitchLinearShape&lt;<span class=\"number\">64</span>, <span class=\"number\">4</span>&gt;;</span><br><span class=\"line\"><span class=\"keyword\">using</span> Layout = cutlass::layout::PitchLinear;</span><br><span class=\"line\"><span class=\"keyword\">using</span> Element = <span class=\"type\">int</span>;</span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"type\">const</span> kThreads = <span class=\"number\">32</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// ThreadMaps define how threads are mapped to a given tile. The PitchLinearStripminedThreadMap</span></span><br><span class=\"line\"><span class=\"comment\">// stripmines a pitch-linear tile among a given number of threads, first along the contiguous</span></span><br><span class=\"line\"><span class=\"comment\">// dimension then along the strided dimension.</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> ThreadMap = cutlass::transform::PitchLinearStripminedThreadMap&lt;Shape, kThreads&gt;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Define the PredicateTileIterator, using TileShape, Element, Layout, and ThreadMap types</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> Iterator = cutlass::transform::threadblock::PredicatedTileIterator&lt;</span><br><span class=\"line\">    Shape, Element, Layout, <span class=\"number\">1</span>, ThreadMap&gt;;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">cutlass::Coord&lt;<span class=\"number\">2</span>&gt; copy_extent = cutlass::<span class=\"built_in\">make_Coord</span>(M, K);</span><br><span class=\"line\">cutlass::Coord&lt;<span class=\"number\">2</span>&gt; alloc_extent = cutlass::<span class=\"built_in\">make_Coord</span>(M, K);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// another way to define tensor</span></span><br><span class=\"line\"><span class=\"comment\">// Allocate source and destination tensors</span></span><br><span class=\"line\"><span class=\"function\">cutlass::HostTensor&lt;Element, Layout&gt; <span class=\"title\">src_tensor</span><span class=\"params\">(alloc_extent)</span></span>;</span><br><span class=\"line\"><span class=\"function\">cutlass::HostTensor&lt;Element, Layout&gt; <span class=\"title\">dst_tensor</span><span class=\"params\">(alloc_extent)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">Element oob_value = <span class=\"built_in\">Element</span>(<span class=\"number\">-1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Initialize destination tensor with all -1s</span></span><br><span class=\"line\">cutlass::reference::host::<span class=\"built_in\">TensorFill</span>(dst_tensor.<span class=\"built_in\">host_view</span>(), oob_value);</span><br><span class=\"line\"><span class=\"comment\">// Initialize source tensor with sequentially increasing values</span></span><br><span class=\"line\">cutlass::reference::host::<span class=\"built_in\">BlockFillSequential</span>(src_tensor.<span class=\"built_in\">host_data</span>(), src_tensor.<span class=\"built_in\">capacity</span>());</span><br><span class=\"line\"></span><br><span class=\"line\">dst_tensor.<span class=\"built_in\">sync_device</span>();</span><br><span class=\"line\">src_tensor.<span class=\"built_in\">sync_device</span>();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">typename</span> Iterator::Params <span class=\"title\">dst_params</span><span class=\"params\">(dst_tensor.layout())</span></span>;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">typename</span> Iterator::Params <span class=\"title\">src_params</span><span class=\"params\">(src_tensor.layout())</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\">dim3 <span class=\"title\">block</span><span class=\"params\">(kThreads, <span class=\"number\">1</span>)</span></span>;</span><br><span class=\"line\"><span class=\"function\">dim3 <span class=\"title\">grid</span><span class=\"params\">(<span class=\"number\">1</span>, <span class=\"number\">1</span>)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Launch copy kernel to perform the copy</span></span><br><span class=\"line\">copy&lt;Iterator&gt;&lt;&lt;&lt; grid, block &gt;&gt;&gt;(</span><br><span class=\"line\">        dst_params,</span><br><span class=\"line\">        dst_tensor.<span class=\"built_in\">device_data</span>(),</span><br><span class=\"line\">        src_params,</span><br><span class=\"line\">        src_tensor.<span class=\"built_in\">device_data</span>(),</span><br><span class=\"line\">        copy_extent</span><br><span class=\"line\">);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// copy function</span></span><br><span class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Iterator&gt;</span><br><span class=\"line\"><span class=\"function\">__global__ <span class=\"type\">void</span> <span class=\"title\">copy</span><span class=\"params\">(</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">    <span class=\"keyword\">typename</span> Iterator::Params dst_params,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">    <span class=\"keyword\">typename</span> Iterator::Element *dst_pointer,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">    <span class=\"keyword\">typename</span> Iterator::Params src_params,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">    <span class=\"keyword\">typename</span> Iterator::Element *src_pointer,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">    cutlass::Coord&lt;<span class=\"number\">2</span>&gt; extent)</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\">Iterator <span class=\"title\">dst_iterator</span><span class=\"params\">(dst_params, dst_pointer, extent, threadIdx.x)</span></span>;</span><br><span class=\"line\">    <span class=\"function\">Iterator <span class=\"title\">src_iterator</span><span class=\"params\">(src_params, src_pointer, extent, threadIdx.x)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// PredicatedTileIterator uses PitchLinear layout and therefore takes in a PitchLinearShape.</span></span><br><span class=\"line\">    <span class=\"comment\">// The contiguous dimension can be accessed via Iterator::Shape::kContiguous and the strided</span></span><br><span class=\"line\">    <span class=\"comment\">// dimension can be accessed via Iterator::Shape::kStrided</span></span><br><span class=\"line\">    <span class=\"type\">int</span> iterations = (extent[<span class=\"number\">1</span>] + Iterator::Shape::kStrided - <span class=\"number\">1</span>) / Iterator::Shape::kStrided;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">typename</span> Iterator::Fragment fragment;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(; iterations &gt; <span class=\"number\">0</span>; --iterations) &#123;</span><br><span class=\"line\">      src_iterator.<span class=\"built_in\">load</span>(fragment);</span><br><span class=\"line\">      dst_iterator.<span class=\"built_in\">store</span>(fragment);</span><br><span class=\"line\"></span><br><span class=\"line\">      ++src_iterator;</span><br><span class=\"line\">      ++dst_iterator;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"title":"learn-cutlass-1","date":"2023-03-21T01:00:19.000Z","toc":true,"_content":"\nIn cutlass 3.0, it introduces a new library, Cute, to describe and manipulate tensors of threads and data. I think the core of cutlass is GEMM(or other computations) and data movement.\n\n<!-- more -->\n\n## Different types of GEMM\n\n| TYPE of GEMM | BITS of DATA | TYPE of DATA |\n| - | -| - |\n| HGEMM | 16 | floating-point number |\n| SGEMM | 32 | floating-point number |\n| DGEMM | 64 | floating-point number |\n| IGEMM | 8 or 16 or 32 or 64 | integer |\n\n## RowMajorInterleaved (ColumnMajorInterleaved)\n\n```c++\n#include \"cutlass/layout/matrix.h\"\ntemplate<int Interleave> struct cutlass::layout::RowMajorInterleaved<Interleave>;\n```\n\nRowMajorInterleaved is a layout which confused me. I didn't know the meaning of Interleaved.So I create an example to figure it out.\n\n```c++\n\n#include <iostream>\n#include <cstdio>\n\n// Defines cutlass::layout::RowMajorInterleave\n#include \"cutlass/layout/matrix.h\"\n\n// Defines cutlass::HostTensor<>\n#include \"cutlass/util/host_tensor.h\"\n\n// Defines cutlass::MatrixCoord\n#include \"cutlass/matrix_coord.h\"\n\n#define M 4\n#define N 4\n\nint main(){\n    cutlass::HostTensor<int,cutlass::layout::RowMajorInterleaved<2> > A(cutlass::MatrixCoord(M,N));\n    \n    int num = 0;\n    for(int i=0;i<M;i++)\n    for(int j=0;j<N;j++){\n        A.at({i,j}) = ++num; \n    }\n\n    int *A_ = A.host_data();\n    for(int i=0;i<A.capacity();i++){\n        printf(\"%3d \",A_[i]);\n        // if((i+1)%N==0)printf(\"\\n\");\n    }\n    /**\n     *  output:\n     *  1 5 2 6 3 7 4 8 9 13 10 14 11 15 12 16\n     *  \n    */\n}\n```\n\nIf tensor A is a simple RowMajor, the output should be this\n\n```\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\n```\nIn my opinion, `Interleaved` means it will iterate in shape(1) with size `Interleave` and then iterate in shape(0). \nOther things need to mind is `Interleaved` may cause padding of a matrix, like\n\n```c++\n#define M 3\n#define N 3\ncutlass::HostTensor<int,cutlass::layout::RowMajorInterleaved<2> > A(cutlass::MatrixCoord(M,N));\nint num = 0;\nfor(int i=0;i<M;i++)\nfor(int j=0;j<N;j++){\n    A.at({i,j}) = ++num; \n}\n/**\n * the element in A should be \n * 1 4 2 5 3 6 7 0 8 0 9 0\n```\n\n## typename in C++ \n\nIn cutlass, you will see `typename` everywhere. Obviously, you can use `typename` when building template. But it has other usage, such as \n\n> The following is generated by chatgpt\n\n```c++\ntemplate<typename T>\nvoid foo() {\n    typename T::iterator it;\n    // ...\n}\n``` \n\nIn C++, the typename keyword is used to specify that a dependent name is a type. A dependent name is a name that depends on a template parameter, and the compiler cannot determine whether the name refers to a type or some other entity until the template is instantiated with concrete types.\n\nConsider the above example.\n\nHere, T::iterator is a dependent name because it depends on the template parameter T. Without the typename keyword, the compiler would not be able to determine whether T::iterator is a type or a member variable until T is instantiated with a concrete type. The typename keyword tells the compiler that T::iterator is a type.\n\nNote that typename is only needed when using a dependent name as a type. If the name does not depend on a template parameter, or if it is not used as a type, the typename keyword should not be used.\n\n## ThreadblockSwizzle\n\n> The following is generated by chatgpt\n\nIn Cutlass, ThreadblockSwizzle is a feature that allows for different threadblock configurations to be used when performing matrix-multiplication operations. ThreadblockSwizzle can be used to optimize the performance of GEMM (General Matrix Multiply) operations on GPUs, by mapping the threadblocks to the data in a way that maximizes data reuse and minimizes thread divergence.\n\nThreadblockSwizzle functions in Cutlass can be used to map the blockIdx (block index) of the CUDA grid to GEMM problems. These functions implement different strategies for organizing the threads within a threadblock, such as tiling, padding, and transposing, in order to improve the efficiency of memory access patterns and to maximize the utilization of GPU resources.\n\nOverall, ThreadblockSwizzle in Cutlass is a powerful feature that can significantly improve the performance of matrix-multiplication operations on GPUs, particularly for large-scale problems.\n\n## Comparison between typedef and using\n\n> The following is generated by chatgpt\n\nIn C++, both typedef and using can be used to create aliases for types. Here are some key differences between the two:\n\n### Syntax\n\n```c++\ntypedef <existing_type> <new_type_alias>;\nusing <new_type_alias> = <existing_type>;\n```\n\n### Scope\n\ntypedef: A type alias created using typedef has the same scope as the original type. This means that it is accessible anywhere that the original type is accessible.\n\nusing: A type alias created using using has a narrower scope than the original type. It is only accessible within the block in which it is defined.\n\n### Functionality\ntypedef: typedef can only be used to create aliases for types. It cannot be used to create template aliases.\n\nusing: using can be used to create both type aliases and template aliases. This makes it more versatile than typedef.\n\n### Readability\n\ntypedef: typedef can only be used to create aliases for types. It cannot be used to create template aliases.\n\nusing: using can be used to create both type aliases and template aliases. This makes it more versatile than typedef.\n\n## 05_batched_gemm\n\nBatched gemm can be illustrated as follows\n\n<!-- ![](/img/batched_gemm.jpg) -->\n<img src=\"/img/batched_gemm.jpg\"  width=\"60%\">\n\nIn the example, it simply calls two APIs which is `cutlass::gemm::device::GemmArray` and `cutlass::gemm::device::GemmBatched`. So I think it is time to read the source of cutlass.\n\n### GemmArray\n\nLet's take GemmArray as an example. \n\n```c++\n// GemmArray is defined in following file\n#include \"cutlass/gemm/device/gemm_array.h\"\n\n// simplified defination of GemmArray\ntemlate<\n    typename ElementA_,\n    typename LayoutA_,\n    typename ElementB_,\n    typename LayoutB_,\n    typename ElementC_,\n    typename LayoutC_\n    //...\n>\nclass GemmArray{\n  public:\n    // ignore some detailed attribute and functions\n    using GemmKernel = kernel::GemmArray<typename DefaultGemmKernel::Mma, typename DefaultGemmKernel::Epilogue, ThreadblockSwizzle>;\n\n    Status run(cudaStream_t stream = nullptr) {\n        // ignore some detailed codes\n        cutlass::Kernel<GemmKernel><<<grid, block, smem_size, stream>>>(params_);\n    }\n\n    // overload operator () for calling gemm_op(...)\n    Status operator()(cudaStream_t stream = nullptr) {\n        return run(stream);\n    }\n\n};\n```\nSee, it is not very complicated. The class `GemmArray` is just built with many templates(the context of a class) and overloads operator `()` to call `cutlass::Kernel`. Then the question is coming. What is `cutlass:Kernal`?\n\n```c++\n#include \"cutlass/device_kernel.h\"\n\n/// Generic CUTLASS kernel template.\ntemplate <typename Operator>\n__global__\nvoid Kernel(typename Operator::Params params) {\n  // Dynamic shared memory base pointer\n  extern __shared__ int SharedStorageBase[];\n\n  // Declare pointer to dynamic shared memory.\n  typename Operator::SharedStorage *shared_storage =\n      reinterpret_cast<typename Operator::SharedStorage *>(SharedStorageBase);\n\n  Operator op;\n\n  op(params, *shared_storage);\n};\n```\n\nIt is just a kernel template. So the important is `Opearator` of `cutlass::Kernal` which stands for `cutlass::gemm::kernel::GemmArray`.\n\n```c++\n#include \"cutlass/gemm/kernel/gemm_array.h\"\n\ntemplate <\n  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate \n  typename Epilogue_,             ///! Epilogue\n  typename ThreadblockSwizzle_    ///! Threadblock swizzling function\n>\nstruct GemmArray{\n    // ignore some detailed attribute and functions\n    CUTLASS_DEVICE\n    void operator()(Params const &params, SharedStorage &shared_storage) {\n        // codes run on device\n    }\n};\n\n```\n\nSo `operator()` is the core of class/struct in cutlass. And all the others are the context of that class/struct.\n\n\n\n\n\n","source":"_posts/learn-cutlass-1.md","raw":"---\ntitle: learn-cutlass-1\ndate: 2023-03-21 09:00:19\ncategories:\n- Technology\ntags:\n- cutlass\ntoc: true\n---\n\nIn cutlass 3.0, it introduces a new library, Cute, to describe and manipulate tensors of threads and data. I think the core of cutlass is GEMM(or other computations) and data movement.\n\n<!-- more -->\n\n## Different types of GEMM\n\n| TYPE of GEMM | BITS of DATA | TYPE of DATA |\n| - | -| - |\n| HGEMM | 16 | floating-point number |\n| SGEMM | 32 | floating-point number |\n| DGEMM | 64 | floating-point number |\n| IGEMM | 8 or 16 or 32 or 64 | integer |\n\n## RowMajorInterleaved (ColumnMajorInterleaved)\n\n```c++\n#include \"cutlass/layout/matrix.h\"\ntemplate<int Interleave> struct cutlass::layout::RowMajorInterleaved<Interleave>;\n```\n\nRowMajorInterleaved is a layout which confused me. I didn't know the meaning of Interleaved.So I create an example to figure it out.\n\n```c++\n\n#include <iostream>\n#include <cstdio>\n\n// Defines cutlass::layout::RowMajorInterleave\n#include \"cutlass/layout/matrix.h\"\n\n// Defines cutlass::HostTensor<>\n#include \"cutlass/util/host_tensor.h\"\n\n// Defines cutlass::MatrixCoord\n#include \"cutlass/matrix_coord.h\"\n\n#define M 4\n#define N 4\n\nint main(){\n    cutlass::HostTensor<int,cutlass::layout::RowMajorInterleaved<2> > A(cutlass::MatrixCoord(M,N));\n    \n    int num = 0;\n    for(int i=0;i<M;i++)\n    for(int j=0;j<N;j++){\n        A.at({i,j}) = ++num; \n    }\n\n    int *A_ = A.host_data();\n    for(int i=0;i<A.capacity();i++){\n        printf(\"%3d \",A_[i]);\n        // if((i+1)%N==0)printf(\"\\n\");\n    }\n    /**\n     *  output:\n     *  1 5 2 6 3 7 4 8 9 13 10 14 11 15 12 16\n     *  \n    */\n}\n```\n\nIf tensor A is a simple RowMajor, the output should be this\n\n```\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\n```\nIn my opinion, `Interleaved` means it will iterate in shape(1) with size `Interleave` and then iterate in shape(0). \nOther things need to mind is `Interleaved` may cause padding of a matrix, like\n\n```c++\n#define M 3\n#define N 3\ncutlass::HostTensor<int,cutlass::layout::RowMajorInterleaved<2> > A(cutlass::MatrixCoord(M,N));\nint num = 0;\nfor(int i=0;i<M;i++)\nfor(int j=0;j<N;j++){\n    A.at({i,j}) = ++num; \n}\n/**\n * the element in A should be \n * 1 4 2 5 3 6 7 0 8 0 9 0\n```\n\n## typename in C++ \n\nIn cutlass, you will see `typename` everywhere. Obviously, you can use `typename` when building template. But it has other usage, such as \n\n> The following is generated by chatgpt\n\n```c++\ntemplate<typename T>\nvoid foo() {\n    typename T::iterator it;\n    // ...\n}\n``` \n\nIn C++, the typename keyword is used to specify that a dependent name is a type. A dependent name is a name that depends on a template parameter, and the compiler cannot determine whether the name refers to a type or some other entity until the template is instantiated with concrete types.\n\nConsider the above example.\n\nHere, T::iterator is a dependent name because it depends on the template parameter T. Without the typename keyword, the compiler would not be able to determine whether T::iterator is a type or a member variable until T is instantiated with a concrete type. The typename keyword tells the compiler that T::iterator is a type.\n\nNote that typename is only needed when using a dependent name as a type. If the name does not depend on a template parameter, or if it is not used as a type, the typename keyword should not be used.\n\n## ThreadblockSwizzle\n\n> The following is generated by chatgpt\n\nIn Cutlass, ThreadblockSwizzle is a feature that allows for different threadblock configurations to be used when performing matrix-multiplication operations. ThreadblockSwizzle can be used to optimize the performance of GEMM (General Matrix Multiply) operations on GPUs, by mapping the threadblocks to the data in a way that maximizes data reuse and minimizes thread divergence.\n\nThreadblockSwizzle functions in Cutlass can be used to map the blockIdx (block index) of the CUDA grid to GEMM problems. These functions implement different strategies for organizing the threads within a threadblock, such as tiling, padding, and transposing, in order to improve the efficiency of memory access patterns and to maximize the utilization of GPU resources.\n\nOverall, ThreadblockSwizzle in Cutlass is a powerful feature that can significantly improve the performance of matrix-multiplication operations on GPUs, particularly for large-scale problems.\n\n## Comparison between typedef and using\n\n> The following is generated by chatgpt\n\nIn C++, both typedef and using can be used to create aliases for types. Here are some key differences between the two:\n\n### Syntax\n\n```c++\ntypedef <existing_type> <new_type_alias>;\nusing <new_type_alias> = <existing_type>;\n```\n\n### Scope\n\ntypedef: A type alias created using typedef has the same scope as the original type. This means that it is accessible anywhere that the original type is accessible.\n\nusing: A type alias created using using has a narrower scope than the original type. It is only accessible within the block in which it is defined.\n\n### Functionality\ntypedef: typedef can only be used to create aliases for types. It cannot be used to create template aliases.\n\nusing: using can be used to create both type aliases and template aliases. This makes it more versatile than typedef.\n\n### Readability\n\ntypedef: typedef can only be used to create aliases for types. It cannot be used to create template aliases.\n\nusing: using can be used to create both type aliases and template aliases. This makes it more versatile than typedef.\n\n## 05_batched_gemm\n\nBatched gemm can be illustrated as follows\n\n<!-- ![](/img/batched_gemm.jpg) -->\n<img src=\"/img/batched_gemm.jpg\"  width=\"60%\">\n\nIn the example, it simply calls two APIs which is `cutlass::gemm::device::GemmArray` and `cutlass::gemm::device::GemmBatched`. So I think it is time to read the source of cutlass.\n\n### GemmArray\n\nLet's take GemmArray as an example. \n\n```c++\n// GemmArray is defined in following file\n#include \"cutlass/gemm/device/gemm_array.h\"\n\n// simplified defination of GemmArray\ntemlate<\n    typename ElementA_,\n    typename LayoutA_,\n    typename ElementB_,\n    typename LayoutB_,\n    typename ElementC_,\n    typename LayoutC_\n    //...\n>\nclass GemmArray{\n  public:\n    // ignore some detailed attribute and functions\n    using GemmKernel = kernel::GemmArray<typename DefaultGemmKernel::Mma, typename DefaultGemmKernel::Epilogue, ThreadblockSwizzle>;\n\n    Status run(cudaStream_t stream = nullptr) {\n        // ignore some detailed codes\n        cutlass::Kernel<GemmKernel><<<grid, block, smem_size, stream>>>(params_);\n    }\n\n    // overload operator () for calling gemm_op(...)\n    Status operator()(cudaStream_t stream = nullptr) {\n        return run(stream);\n    }\n\n};\n```\nSee, it is not very complicated. The class `GemmArray` is just built with many templates(the context of a class) and overloads operator `()` to call `cutlass::Kernel`. Then the question is coming. What is `cutlass:Kernal`?\n\n```c++\n#include \"cutlass/device_kernel.h\"\n\n/// Generic CUTLASS kernel template.\ntemplate <typename Operator>\n__global__\nvoid Kernel(typename Operator::Params params) {\n  // Dynamic shared memory base pointer\n  extern __shared__ int SharedStorageBase[];\n\n  // Declare pointer to dynamic shared memory.\n  typename Operator::SharedStorage *shared_storage =\n      reinterpret_cast<typename Operator::SharedStorage *>(SharedStorageBase);\n\n  Operator op;\n\n  op(params, *shared_storage);\n};\n```\n\nIt is just a kernel template. So the important is `Opearator` of `cutlass::Kernal` which stands for `cutlass::gemm::kernel::GemmArray`.\n\n```c++\n#include \"cutlass/gemm/kernel/gemm_array.h\"\n\ntemplate <\n  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate \n  typename Epilogue_,             ///! Epilogue\n  typename ThreadblockSwizzle_    ///! Threadblock swizzling function\n>\nstruct GemmArray{\n    // ignore some detailed attribute and functions\n    CUTLASS_DEVICE\n    void operator()(Params const &params, SharedStorage &shared_storage) {\n        // codes run on device\n    }\n};\n\n```\n\nSo `operator()` is the core of class/struct in cutlass. And all the others are the context of that class/struct.\n\n\n\n\n\n","slug":"learn-cutlass-1","published":1,"updated":"2023-03-23T08:05:10.189Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clfm5kvrs000i717v0u8r88yw","content":"<p>In cutlass 3.0, it introduces a new library, Cute, to describe and manipulate tensors of threads and data. I think the core of cutlass is GEMM(or other computations) and data movement.</p>\n<span id=\"more\"></span>\n\n<h2 id=\"Different-types-of-GEMM\"><a href=\"#Different-types-of-GEMM\" class=\"headerlink\" title=\"Different types of GEMM\"></a>Different types of GEMM</h2><table>\n<thead>\n<tr>\n<th>TYPE of GEMM</th>\n<th>BITS of DATA</th>\n<th>TYPE of DATA</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HGEMM</td>\n<td>16</td>\n<td>floating-point number</td>\n</tr>\n<tr>\n<td>SGEMM</td>\n<td>32</td>\n<td>floating-point number</td>\n</tr>\n<tr>\n<td>DGEMM</td>\n<td>64</td>\n<td>floating-point number</td>\n</tr>\n<tr>\n<td>IGEMM</td>\n<td>8 or 16 or 32 or 64</td>\n<td>integer</td>\n</tr>\n</tbody></table>\n<h2 id=\"RowMajorInterleaved-ColumnMajorInterleaved\"><a href=\"#RowMajorInterleaved-ColumnMajorInterleaved\" class=\"headerlink\" title=\"RowMajorInterleaved (ColumnMajorInterleaved)\"></a>RowMajorInterleaved (ColumnMajorInterleaved)</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/layout/matrix.h&quot;</span></span></span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"type\">int</span> Interleave&gt; <span class=\"keyword\">struct</span> <span class=\"title class_\">cutlass</span>::layout::RowMajorInterleaved&lt;Interleave&gt;;</span><br></pre></td></tr></table></figure>\n\n<p>RowMajorInterleaved is a layout which confused me. I didn’t know the meaning of Interleaved.So I create an example to figure it out.</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;cstdio&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::layout::RowMajorInterleave</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/layout/matrix.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::HostTensor&lt;&gt;</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/host_tensor.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::MatrixCoord</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/matrix_coord.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> M 4</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> N 4</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">    cutlass::HostTensor&lt;<span class=\"type\">int</span>,cutlass::layout::RowMajorInterleaved&lt;<span class=\"number\">2</span>&gt; &gt; <span class=\"built_in\">A</span>(cutlass::<span class=\"built_in\">MatrixCoord</span>(M,N));</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"type\">int</span> num = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;M;i++)</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> j=<span class=\"number\">0</span>;j&lt;N;j++)&#123;</span><br><span class=\"line\">        A.<span class=\"built_in\">at</span>(&#123;i,j&#125;) = ++num; </span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> *A_ = A.<span class=\"built_in\">host_data</span>();</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;A.<span class=\"built_in\">capacity</span>();i++)&#123;</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%3d &quot;</span>,A_[i]);</span><br><span class=\"line\">        <span class=\"comment\">// if((i+1)%N==0)printf(&quot;\\n&quot;);</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     *  output:</span></span><br><span class=\"line\"><span class=\"comment\">     *  1 5 2 6 3 7 4 8 9 13 10 14 11 15 12 16</span></span><br><span class=\"line\"><span class=\"comment\">     *  </span></span><br><span class=\"line\"><span class=\"comment\">    */</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>If tensor A is a simple RowMajor, the output should be this</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16</span><br></pre></td></tr></table></figure>\n<p>In my opinion, <code>Interleaved</code> means it will iterate in shape(1) with size <code>Interleave</code> and then iterate in shape(0).<br>Other things need to mind is <code>Interleaved</code> may cause padding of a matrix, like</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> M 3</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> N 3</span></span><br><span class=\"line\">cutlass::HostTensor&lt;<span class=\"type\">int</span>,cutlass::layout::RowMajorInterleaved&lt;<span class=\"number\">2</span>&gt; &gt; <span class=\"built_in\">A</span>(cutlass::<span class=\"built_in\">MatrixCoord</span>(M,N));</span><br><span class=\"line\"><span class=\"type\">int</span> num = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;M;i++)</span><br><span class=\"line\"><span class=\"keyword\">for</span>(<span class=\"type\">int</span> j=<span class=\"number\">0</span>;j&lt;N;j++)&#123;</span><br><span class=\"line\">    A.<span class=\"built_in\">at</span>(&#123;i,j&#125;) = ++num; </span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * the element in A should be </span></span><br><span class=\"line\"><span class=\"comment\"> * 1 4 2 5 3 6 7 0 8 0 9 0</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"typename-in-C\"><a href=\"#typename-in-C\" class=\"headerlink\" title=\"typename in C++\"></a>typename in C++</h2><p>In cutlass, you will see <code>typename</code> everywhere. Obviously, you can use <code>typename</code> when building template. But it has other usage, such as </p>\n<blockquote>\n<p>The following is generated by chatgpt</p>\n</blockquote>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> T&gt;</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">foo</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">typename</span> T::iterator it;</span><br><span class=\"line\">    <span class=\"comment\">// ...</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>In C++, the typename keyword is used to specify that a dependent name is a type. A dependent name is a name that depends on a template parameter, and the compiler cannot determine whether the name refers to a type or some other entity until the template is instantiated with concrete types.</p>\n<p>Consider the above example.</p>\n<p>Here, T::iterator is a dependent name because it depends on the template parameter T. Without the typename keyword, the compiler would not be able to determine whether T::iterator is a type or a member variable until T is instantiated with a concrete type. The typename keyword tells the compiler that T::iterator is a type.</p>\n<p>Note that typename is only needed when using a dependent name as a type. If the name does not depend on a template parameter, or if it is not used as a type, the typename keyword should not be used.</p>\n<h2 id=\"ThreadblockSwizzle\"><a href=\"#ThreadblockSwizzle\" class=\"headerlink\" title=\"ThreadblockSwizzle\"></a>ThreadblockSwizzle</h2><blockquote>\n<p>The following is generated by chatgpt</p>\n</blockquote>\n<p>In Cutlass, ThreadblockSwizzle is a feature that allows for different threadblock configurations to be used when performing matrix-multiplication operations. ThreadblockSwizzle can be used to optimize the performance of GEMM (General Matrix Multiply) operations on GPUs, by mapping the threadblocks to the data in a way that maximizes data reuse and minimizes thread divergence.</p>\n<p>ThreadblockSwizzle functions in Cutlass can be used to map the blockIdx (block index) of the CUDA grid to GEMM problems. These functions implement different strategies for organizing the threads within a threadblock, such as tiling, padding, and transposing, in order to improve the efficiency of memory access patterns and to maximize the utilization of GPU resources.</p>\n<p>Overall, ThreadblockSwizzle in Cutlass is a powerful feature that can significantly improve the performance of matrix-multiplication operations on GPUs, particularly for large-scale problems.</p>\n<h2 id=\"Comparison-between-typedef-and-using\"><a href=\"#Comparison-between-typedef-and-using\" class=\"headerlink\" title=\"Comparison between typedef and using\"></a>Comparison between typedef and using</h2><blockquote>\n<p>The following is generated by chatgpt</p>\n</blockquote>\n<p>In C++, both typedef and using can be used to create aliases for types. Here are some key differences between the two:</p>\n<h3 id=\"Syntax\"><a href=\"#Syntax\" class=\"headerlink\" title=\"Syntax\"></a>Syntax</h3><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> &lt;existing_type&gt; &lt;new_type_alias&gt;;</span><br><span class=\"line\"><span class=\"keyword\">using</span> &lt;new_type_alias&gt; = &lt;existing_type&gt;;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Scope\"><a href=\"#Scope\" class=\"headerlink\" title=\"Scope\"></a>Scope</h3><p>typedef: A type alias created using typedef has the same scope as the original type. This means that it is accessible anywhere that the original type is accessible.</p>\n<p>using: A type alias created using using has a narrower scope than the original type. It is only accessible within the block in which it is defined.</p>\n<h3 id=\"Functionality\"><a href=\"#Functionality\" class=\"headerlink\" title=\"Functionality\"></a>Functionality</h3><p>typedef: typedef can only be used to create aliases for types. It cannot be used to create template aliases.</p>\n<p>using: using can be used to create both type aliases and template aliases. This makes it more versatile than typedef.</p>\n<h3 id=\"Readability\"><a href=\"#Readability\" class=\"headerlink\" title=\"Readability\"></a>Readability</h3><p>typedef: typedef can only be used to create aliases for types. It cannot be used to create template aliases.</p>\n<p>using: using can be used to create both type aliases and template aliases. This makes it more versatile than typedef.</p>\n<h2 id=\"05-batched-gemm\"><a href=\"#05-batched-gemm\" class=\"headerlink\" title=\"05_batched_gemm\"></a>05_batched_gemm</h2><p>Batched gemm can be illustrated as follows</p>\n<!-- ![](/img/batched_gemm.jpg) -->\n<img src=\"/img/batched_gemm.jpg\"  width=\"60%\">\n\n<p>In the example, it simply calls two APIs which is <code>cutlass::gemm::device::GemmArray</code> and <code>cutlass::gemm::device::GemmBatched</code>. So I think it is time to read the source of cutlass.</p>\n<h3 id=\"GemmArray\"><a href=\"#GemmArray\" class=\"headerlink\" title=\"GemmArray\"></a>GemmArray</h3><p>Let’s take GemmArray as an example. </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// GemmArray is defined in following file</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/gemm/device/gemm_array.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// simplified defination of GemmArray</span></span><br><span class=\"line\">temlate&lt;</span><br><span class=\"line\">    <span class=\"keyword\">typename</span> ElementA_,</span><br><span class=\"line\">    <span class=\"keyword\">typename</span> LayoutA_,</span><br><span class=\"line\">    <span class=\"keyword\">typename</span> ElementB_,</span><br><span class=\"line\">    <span class=\"keyword\">typename</span> LayoutB_,</span><br><span class=\"line\">    <span class=\"keyword\">typename</span> ElementC_,</span><br><span class=\"line\">    <span class=\"keyword\">typename</span> LayoutC_</span><br><span class=\"line\">    <span class=\"comment\">//...</span></span><br><span class=\"line\">&gt;</span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">GemmArray</span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"comment\">// ignore some detailed attribute and functions</span></span><br><span class=\"line\">    <span class=\"keyword\">using</span> GemmKernel = kernel::GemmArray&lt;<span class=\"keyword\">typename</span> DefaultGemmKernel::Mma, <span class=\"keyword\">typename</span> DefaultGemmKernel::Epilogue, ThreadblockSwizzle&gt;;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\">Status <span class=\"title\">run</span><span class=\"params\">(cudaStream_t stream = <span class=\"literal\">nullptr</span>)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// ignore some detailed codes</span></span><br><span class=\"line\">        cutlass::Kernel&lt;GemmKernel&gt;&lt;&lt;&lt;grid, block, smem_size, stream&gt;&gt;&gt;(params_);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// overload operator () for calling gemm_op(...)</span></span><br><span class=\"line\">    <span class=\"function\">Status <span class=\"title\">operator</span><span class=\"params\">()</span><span class=\"params\">(cudaStream_t stream = <span class=\"literal\">nullptr</span>)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">run</span>(stream);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>See, it is not very complicated. The class <code>GemmArray</code> is just built with many templates(the context of a class) and overloads operator <code>()</code> to call <code>cutlass::Kernel</code>. Then the question is coming. What is <code>cutlass:Kernal</code>?</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/device_kernel.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/// Generic CUTLASS kernel template.</span></span><br><span class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Operator&gt;</span><br><span class=\"line\"><span class=\"function\">__global__</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">Kernel</span><span class=\"params\">(<span class=\"keyword\">typename</span> Operator::Params params)</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"comment\">// Dynamic shared memory base pointer</span></span><br><span class=\"line\">  <span class=\"keyword\">extern</span> __shared__ <span class=\"type\">int</span> SharedStorageBase[];</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// Declare pointer to dynamic shared memory.</span></span><br><span class=\"line\">  <span class=\"keyword\">typename</span> Operator::SharedStorage *shared_storage =</span><br><span class=\"line\">      <span class=\"built_in\">reinterpret_cast</span>&lt;<span class=\"keyword\">typename</span> Operator::SharedStorage *&gt;(SharedStorageBase);</span><br><span class=\"line\"></span><br><span class=\"line\">  Operator op;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"built_in\">op</span>(params, *shared_storage);</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>It is just a kernel template. So the important is <code>Opearator</code> of <code>cutlass::Kernal</code> which stands for <code>cutlass::gemm::kernel::GemmArray</code>.</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/gemm/kernel/gemm_array.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">template</span> &lt;</span><br><span class=\"line\">  <span class=\"keyword\">typename</span> Mma_,                  <span class=\"comment\">///! Threadblock-scoped matrix multiply-accumulate </span></span><br><span class=\"line\">  <span class=\"keyword\">typename</span> Epilogue_,             <span class=\"comment\">///! Epilogue</span></span><br><span class=\"line\">  <span class=\"keyword\">typename</span> ThreadblockSwizzle_    <span class=\"comment\">///! Threadblock swizzling function</span></span><br><span class=\"line\">&gt;</span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">GemmArray</span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// ignore some detailed attribute and functions</span></span><br><span class=\"line\">    <span class=\"function\">CUTLASS_DEVICE</span></span><br><span class=\"line\"><span class=\"function\">    <span class=\"type\">void</span> <span class=\"title\">operator</span><span class=\"params\">()</span><span class=\"params\">(Params <span class=\"type\">const</span> &amp;params, SharedStorage &amp;shared_storage)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// codes run on device</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>So <code>operator()</code> is the core of class&#x2F;struct in cutlass. And all the others are the context of that class&#x2F;struct.</p>\n","site":{"data":{}},"excerpt":"<p>In cutlass 3.0, it introduces a new library, Cute, to describe and manipulate tensors of threads and data. I think the core of cutlass is GEMM(or other computations) and data movement.</p>","more":"<h2 id=\"Different-types-of-GEMM\"><a href=\"#Different-types-of-GEMM\" class=\"headerlink\" title=\"Different types of GEMM\"></a>Different types of GEMM</h2><table>\n<thead>\n<tr>\n<th>TYPE of GEMM</th>\n<th>BITS of DATA</th>\n<th>TYPE of DATA</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HGEMM</td>\n<td>16</td>\n<td>floating-point number</td>\n</tr>\n<tr>\n<td>SGEMM</td>\n<td>32</td>\n<td>floating-point number</td>\n</tr>\n<tr>\n<td>DGEMM</td>\n<td>64</td>\n<td>floating-point number</td>\n</tr>\n<tr>\n<td>IGEMM</td>\n<td>8 or 16 or 32 or 64</td>\n<td>integer</td>\n</tr>\n</tbody></table>\n<h2 id=\"RowMajorInterleaved-ColumnMajorInterleaved\"><a href=\"#RowMajorInterleaved-ColumnMajorInterleaved\" class=\"headerlink\" title=\"RowMajorInterleaved (ColumnMajorInterleaved)\"></a>RowMajorInterleaved (ColumnMajorInterleaved)</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/layout/matrix.h&quot;</span></span></span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"type\">int</span> Interleave&gt; <span class=\"keyword\">struct</span> <span class=\"title class_\">cutlass</span>::layout::RowMajorInterleaved&lt;Interleave&gt;;</span><br></pre></td></tr></table></figure>\n\n<p>RowMajorInterleaved is a layout which confused me. I didn’t know the meaning of Interleaved.So I create an example to figure it out.</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;cstdio&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::layout::RowMajorInterleave</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/layout/matrix.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::HostTensor&lt;&gt;</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/util/host_tensor.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Defines cutlass::MatrixCoord</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/matrix_coord.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> M 4</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> N 4</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">    cutlass::HostTensor&lt;<span class=\"type\">int</span>,cutlass::layout::RowMajorInterleaved&lt;<span class=\"number\">2</span>&gt; &gt; <span class=\"built_in\">A</span>(cutlass::<span class=\"built_in\">MatrixCoord</span>(M,N));</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"type\">int</span> num = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;M;i++)</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> j=<span class=\"number\">0</span>;j&lt;N;j++)&#123;</span><br><span class=\"line\">        A.<span class=\"built_in\">at</span>(&#123;i,j&#125;) = ++num; </span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> *A_ = A.<span class=\"built_in\">host_data</span>();</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;A.<span class=\"built_in\">capacity</span>();i++)&#123;</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%3d &quot;</span>,A_[i]);</span><br><span class=\"line\">        <span class=\"comment\">// if((i+1)%N==0)printf(&quot;\\n&quot;);</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     *  output:</span></span><br><span class=\"line\"><span class=\"comment\">     *  1 5 2 6 3 7 4 8 9 13 10 14 11 15 12 16</span></span><br><span class=\"line\"><span class=\"comment\">     *  </span></span><br><span class=\"line\"><span class=\"comment\">    */</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>If tensor A is a simple RowMajor, the output should be this</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16</span><br></pre></td></tr></table></figure>\n<p>In my opinion, <code>Interleaved</code> means it will iterate in shape(1) with size <code>Interleave</code> and then iterate in shape(0).<br>Other things need to mind is <code>Interleaved</code> may cause padding of a matrix, like</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> M 3</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> N 3</span></span><br><span class=\"line\">cutlass::HostTensor&lt;<span class=\"type\">int</span>,cutlass::layout::RowMajorInterleaved&lt;<span class=\"number\">2</span>&gt; &gt; <span class=\"built_in\">A</span>(cutlass::<span class=\"built_in\">MatrixCoord</span>(M,N));</span><br><span class=\"line\"><span class=\"type\">int</span> num = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;M;i++)</span><br><span class=\"line\"><span class=\"keyword\">for</span>(<span class=\"type\">int</span> j=<span class=\"number\">0</span>;j&lt;N;j++)&#123;</span><br><span class=\"line\">    A.<span class=\"built_in\">at</span>(&#123;i,j&#125;) = ++num; </span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * the element in A should be </span></span><br><span class=\"line\"><span class=\"comment\"> * 1 4 2 5 3 6 7 0 8 0 9 0</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"typename-in-C\"><a href=\"#typename-in-C\" class=\"headerlink\" title=\"typename in C++\"></a>typename in C++</h2><p>In cutlass, you will see <code>typename</code> everywhere. Obviously, you can use <code>typename</code> when building template. But it has other usage, such as </p>\n<blockquote>\n<p>The following is generated by chatgpt</p>\n</blockquote>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> T&gt;</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">foo</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">typename</span> T::iterator it;</span><br><span class=\"line\">    <span class=\"comment\">// ...</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>In C++, the typename keyword is used to specify that a dependent name is a type. A dependent name is a name that depends on a template parameter, and the compiler cannot determine whether the name refers to a type or some other entity until the template is instantiated with concrete types.</p>\n<p>Consider the above example.</p>\n<p>Here, T::iterator is a dependent name because it depends on the template parameter T. Without the typename keyword, the compiler would not be able to determine whether T::iterator is a type or a member variable until T is instantiated with a concrete type. The typename keyword tells the compiler that T::iterator is a type.</p>\n<p>Note that typename is only needed when using a dependent name as a type. If the name does not depend on a template parameter, or if it is not used as a type, the typename keyword should not be used.</p>\n<h2 id=\"ThreadblockSwizzle\"><a href=\"#ThreadblockSwizzle\" class=\"headerlink\" title=\"ThreadblockSwizzle\"></a>ThreadblockSwizzle</h2><blockquote>\n<p>The following is generated by chatgpt</p>\n</blockquote>\n<p>In Cutlass, ThreadblockSwizzle is a feature that allows for different threadblock configurations to be used when performing matrix-multiplication operations. ThreadblockSwizzle can be used to optimize the performance of GEMM (General Matrix Multiply) operations on GPUs, by mapping the threadblocks to the data in a way that maximizes data reuse and minimizes thread divergence.</p>\n<p>ThreadblockSwizzle functions in Cutlass can be used to map the blockIdx (block index) of the CUDA grid to GEMM problems. These functions implement different strategies for organizing the threads within a threadblock, such as tiling, padding, and transposing, in order to improve the efficiency of memory access patterns and to maximize the utilization of GPU resources.</p>\n<p>Overall, ThreadblockSwizzle in Cutlass is a powerful feature that can significantly improve the performance of matrix-multiplication operations on GPUs, particularly for large-scale problems.</p>\n<h2 id=\"Comparison-between-typedef-and-using\"><a href=\"#Comparison-between-typedef-and-using\" class=\"headerlink\" title=\"Comparison between typedef and using\"></a>Comparison between typedef and using</h2><blockquote>\n<p>The following is generated by chatgpt</p>\n</blockquote>\n<p>In C++, both typedef and using can be used to create aliases for types. Here are some key differences between the two:</p>\n<h3 id=\"Syntax\"><a href=\"#Syntax\" class=\"headerlink\" title=\"Syntax\"></a>Syntax</h3><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> &lt;existing_type&gt; &lt;new_type_alias&gt;;</span><br><span class=\"line\"><span class=\"keyword\">using</span> &lt;new_type_alias&gt; = &lt;existing_type&gt;;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Scope\"><a href=\"#Scope\" class=\"headerlink\" title=\"Scope\"></a>Scope</h3><p>typedef: A type alias created using typedef has the same scope as the original type. This means that it is accessible anywhere that the original type is accessible.</p>\n<p>using: A type alias created using using has a narrower scope than the original type. It is only accessible within the block in which it is defined.</p>\n<h3 id=\"Functionality\"><a href=\"#Functionality\" class=\"headerlink\" title=\"Functionality\"></a>Functionality</h3><p>typedef: typedef can only be used to create aliases for types. It cannot be used to create template aliases.</p>\n<p>using: using can be used to create both type aliases and template aliases. This makes it more versatile than typedef.</p>\n<h3 id=\"Readability\"><a href=\"#Readability\" class=\"headerlink\" title=\"Readability\"></a>Readability</h3><p>typedef: typedef can only be used to create aliases for types. It cannot be used to create template aliases.</p>\n<p>using: using can be used to create both type aliases and template aliases. This makes it more versatile than typedef.</p>\n<h2 id=\"05-batched-gemm\"><a href=\"#05-batched-gemm\" class=\"headerlink\" title=\"05_batched_gemm\"></a>05_batched_gemm</h2><p>Batched gemm can be illustrated as follows</p>\n<!-- ![](/img/batched_gemm.jpg) -->\n<img src=\"/img/batched_gemm.jpg\"  width=\"60%\">\n\n<p>In the example, it simply calls two APIs which is <code>cutlass::gemm::device::GemmArray</code> and <code>cutlass::gemm::device::GemmBatched</code>. So I think it is time to read the source of cutlass.</p>\n<h3 id=\"GemmArray\"><a href=\"#GemmArray\" class=\"headerlink\" title=\"GemmArray\"></a>GemmArray</h3><p>Let’s take GemmArray as an example. </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// GemmArray is defined in following file</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/gemm/device/gemm_array.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// simplified defination of GemmArray</span></span><br><span class=\"line\">temlate&lt;</span><br><span class=\"line\">    <span class=\"keyword\">typename</span> ElementA_,</span><br><span class=\"line\">    <span class=\"keyword\">typename</span> LayoutA_,</span><br><span class=\"line\">    <span class=\"keyword\">typename</span> ElementB_,</span><br><span class=\"line\">    <span class=\"keyword\">typename</span> LayoutB_,</span><br><span class=\"line\">    <span class=\"keyword\">typename</span> ElementC_,</span><br><span class=\"line\">    <span class=\"keyword\">typename</span> LayoutC_</span><br><span class=\"line\">    <span class=\"comment\">//...</span></span><br><span class=\"line\">&gt;</span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">GemmArray</span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"comment\">// ignore some detailed attribute and functions</span></span><br><span class=\"line\">    <span class=\"keyword\">using</span> GemmKernel = kernel::GemmArray&lt;<span class=\"keyword\">typename</span> DefaultGemmKernel::Mma, <span class=\"keyword\">typename</span> DefaultGemmKernel::Epilogue, ThreadblockSwizzle&gt;;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\">Status <span class=\"title\">run</span><span class=\"params\">(cudaStream_t stream = <span class=\"literal\">nullptr</span>)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// ignore some detailed codes</span></span><br><span class=\"line\">        cutlass::Kernel&lt;GemmKernel&gt;&lt;&lt;&lt;grid, block, smem_size, stream&gt;&gt;&gt;(params_);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// overload operator () for calling gemm_op(...)</span></span><br><span class=\"line\">    <span class=\"function\">Status <span class=\"title\">operator</span><span class=\"params\">()</span><span class=\"params\">(cudaStream_t stream = <span class=\"literal\">nullptr</span>)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">run</span>(stream);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>See, it is not very complicated. The class <code>GemmArray</code> is just built with many templates(the context of a class) and overloads operator <code>()</code> to call <code>cutlass::Kernel</code>. Then the question is coming. What is <code>cutlass:Kernal</code>?</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/device_kernel.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/// Generic CUTLASS kernel template.</span></span><br><span class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Operator&gt;</span><br><span class=\"line\"><span class=\"function\">__global__</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">Kernel</span><span class=\"params\">(<span class=\"keyword\">typename</span> Operator::Params params)</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"comment\">// Dynamic shared memory base pointer</span></span><br><span class=\"line\">  <span class=\"keyword\">extern</span> __shared__ <span class=\"type\">int</span> SharedStorageBase[];</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// Declare pointer to dynamic shared memory.</span></span><br><span class=\"line\">  <span class=\"keyword\">typename</span> Operator::SharedStorage *shared_storage =</span><br><span class=\"line\">      <span class=\"built_in\">reinterpret_cast</span>&lt;<span class=\"keyword\">typename</span> Operator::SharedStorage *&gt;(SharedStorageBase);</span><br><span class=\"line\"></span><br><span class=\"line\">  Operator op;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"built_in\">op</span>(params, *shared_storage);</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>It is just a kernel template. So the important is <code>Opearator</code> of <code>cutlass::Kernal</code> which stands for <code>cutlass::gemm::kernel::GemmArray</code>.</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cutlass/gemm/kernel/gemm_array.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">template</span> &lt;</span><br><span class=\"line\">  <span class=\"keyword\">typename</span> Mma_,                  <span class=\"comment\">///! Threadblock-scoped matrix multiply-accumulate </span></span><br><span class=\"line\">  <span class=\"keyword\">typename</span> Epilogue_,             <span class=\"comment\">///! Epilogue</span></span><br><span class=\"line\">  <span class=\"keyword\">typename</span> ThreadblockSwizzle_    <span class=\"comment\">///! Threadblock swizzling function</span></span><br><span class=\"line\">&gt;</span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">GemmArray</span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// ignore some detailed attribute and functions</span></span><br><span class=\"line\">    <span class=\"function\">CUTLASS_DEVICE</span></span><br><span class=\"line\"><span class=\"function\">    <span class=\"type\">void</span> <span class=\"title\">operator</span><span class=\"params\">()</span><span class=\"params\">(Params <span class=\"type\">const</span> &amp;params, SharedStorage &amp;shared_storage)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// codes run on device</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>So <code>operator()</code> is the core of class&#x2F;struct in cutlass. And all the others are the context of that class&#x2F;struct.</p>"},{"title":"冰雹","toc":true,"date":"2023-03-24T06:13:05.000Z","_content":"\n中午在微博上看到广州下冰雹了，但是自己却没有遇见，今天下午刚到超算，听见外面有几声巨大的雷响，就发现外面在下冰雹，于是想出去感受下被冰雹砸中的感觉，刚到楼下就转为雨点了，这冰雹持续时间也太短了吧。记得上一次遇见冰雹还是在高中时候快高考在教室中模考，当时还特意去窗户边看了下。\n\n\n","source":"_posts/冰雹.md","raw":"---\ntitle: 冰雹\ntoc: true\ndate: 2023-03-24 14:13:05\ncategories:\n- log\ntags:\n- 冰雹\n---\n\n中午在微博上看到广州下冰雹了，但是自己却没有遇见，今天下午刚到超算，听见外面有几声巨大的雷响，就发现外面在下冰雹，于是想出去感受下被冰雹砸中的感觉，刚到楼下就转为雨点了，这冰雹持续时间也太短了吧。记得上一次遇见冰雹还是在高中时候快高考在教室中模考，当时还特意去窗户边看了下。\n\n\n","slug":"冰雹","published":1,"updated":"2023-03-24T06:21:18.595Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clfm5mhsc0000gx7v5xtghyjz","content":"<p>中午在微博上看到广州下冰雹了，但是自己却没有遇见，今天下午刚到超算，听见外面有几声巨大的雷响，就发现外面在下冰雹，于是想出去感受下被冰雹砸中的感觉，刚到楼下就转为雨点了，这冰雹持续时间也太短了吧。记得上一次遇见冰雹还是在高中时候快高考在教室中模考，当时还特意去窗户边看了下。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>中午在微博上看到广州下冰雹了，但是自己却没有遇见，今天下午刚到超算，听见外面有几声巨大的雷响，就发现外面在下冰雹，于是想出去感受下被冰雹砸中的感觉，刚到楼下就转为雨点了，这冰雹持续时间也太短了吧。记得上一次遇见冰雹还是在高中时候快高考在教室中模考，当时还特意去窗户边看了下。</p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"clfm5kvrg0001717vgwbmft8i","category_id":"clfm5kvrj0003717v22bb8u64","_id":"clfm5kvrn0009717vdfpwdnny"},{"post_id":"clfm5kvri0002717vdfbs9lkr","category_id":"clfm5kvrm0006717v0gyg4qt1","_id":"clfm5kvrq000c717v91jr16xr"},{"post_id":"clfm5kvrl0005717v7lch1xqa","category_id":"clfm5kvrm0006717v0gyg4qt1","_id":"clfm5kvrq000f717v4tzm49ez"},{"post_id":"clfm5kvrs000i717v0u8r88yw","category_id":"clfm5kvrm0006717v0gyg4qt1","_id":"clfm5kvrs000k717vhdukdl8b"},{"post_id":"clfm5mhsc0000gx7v5xtghyjz","category_id":"clfm5mhsi0001gx7v3v9gend3","_id":"clfm5mhss0004gx7v3cc15ryw"}],"PostTag":[{"post_id":"clfm5kvrg0001717vgwbmft8i","tag_id":"clfm5kvrk0004717v2nfkh400","_id":"clfm5kvrm0008717vhm6e1pnn"},{"post_id":"clfm5kvri0002717vdfbs9lkr","tag_id":"clfm5kvrm0007717vf4akgudu","_id":"clfm5kvrq000e717vb6i5h4zv"},{"post_id":"clfm5kvri0002717vdfbs9lkr","tag_id":"clfm5kvrn000b717vcugu43ma","_id":"clfm5kvrq000g717vfnuq4ixp"},{"post_id":"clfm5kvrl0005717v7lch1xqa","tag_id":"clfm5kvrq000d717vgj4oed9l","_id":"clfm5kvrq000h717v1snxeo2s"},{"post_id":"clfm5kvrs000i717v0u8r88yw","tag_id":"clfm5kvrq000d717vgj4oed9l","_id":"clfm5kvrs000j717v1nd33klx"},{"post_id":"clfm5mhsc0000gx7v5xtghyjz","tag_id":"clfm5mhsq0002gx7v83pr9bnx","_id":"clfm5mhss0003gx7vgxsr0f12"}],"Tag":[{"name":"一生一芯","_id":"clfm5kvrk0004717v2nfkh400"},{"name":"Hexo","_id":"clfm5kvrm0007717vf4akgudu"},{"name":"Icarus","_id":"clfm5kvrn000b717vcugu43ma"},{"name":"cutlass","_id":"clfm5kvrq000d717vgj4oed9l"},{"name":"冰雹","_id":"clfm5mhsq0002gx7v83pr9bnx"}]}}